{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edithzeng/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from util import *\n",
    "%matplotlib inline\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1/31/2019\n",
    "\n",
    "Below are my findings about questions in VizWiz & VQA datasets based on the training sets from Nilavra. \n",
    "Assumptions - \n",
    "- VizWiz consists of primarily questions from blind users, whereas VQA's questions come from visually-abled crowd workers (Agrawal et al. 2015, p4); \n",
    "- skills labelled are indicative of users' information needs; and \n",
    "- human opinions are limited, expensive and more reliable whereas automated solutions are cheaper but less accurate, and there's a trade-off when we try to combine the two.\n",
    "\n",
    "0) Where did the skills categories come from (not from VQA?)\n",
    "\n",
    "1) Skills -- class distributions:\n",
    "- Both datasets are heavily focused on object recognition task (98% in VizWiz and 99% in VQA)\n",
    "- Differences between VizWiz and VQA's class distributions suggest:\n",
    " - blind users had significantly higher demands for text recognition (VQA training set is much smaller though)\n",
    " - blind users appear less interested in counting tasks. Perhaps it's because they can count the number of coin or the number of shirts by touching the objects?\n",
    " \n",
    "2) Although both datasets consist of primarily object recognition questions, questions are expressed in different ways. We can further look into this to highlight the unique needs of blind users.\n",
    "- VizWiz: most questions are variants of the question \"what is this?\" A trivial classifier can confidently predict \"object recognition\" based on the first couple words and, as I've mentioned in the course project report, in most cases the skill can be predicted without the corresponding image.\n",
    "- VQA: most questions gives away what object is in the picture. eg. question \"are these trains moving?\" is labelled as object recognition, but the relevant object (trains) is already mentioned in the question. It's a priori for the respondent to be able to identify the object so that they can answer the question. \n",
    "\n",
    "3) Knowing the skills can help when we don't have the ability to fully automate object recognition task with a high level of granularity. For example, as opposed to generating the label \"bottle,\" the description \"a white bottle of 36 oz Dove shampoo\" contains text recognition, color recognition and counting while performing object recognition, and a blind user could find it useful to know such granular information about the scene that would be available to a visually-abled person (i.e. seeing the shampoo bottle with all the texts and colors, as opposed to just parts that are relevant to the question). The fact that crowd workers labelled almost all of VizWiz and VQA tasks as object recognition suggests that most of these pairs are inherently context-based recognition tasks.\n",
    "\n",
    "4) Therefore, these predetermined skill categories are important when we come to consider improving the status quo. It also suggests that down the road we need some form of user research with qualitative components to better understand blind users' needs.\n",
    "\n",
    "5) If we're interested in developing assitive tools with a focus on visually impaired users, VizWiz will be more appropriate than VQA (which is smaller and contains more descriptive questions). \n",
    "\n",
    "Some ideas for improving the status quo for object recognition subcategory: \n",
    " - Estimate/quantify the difficulty of object recognition task. Automate tasks with high confidence level and route the difficult/uncertain ones to humans. This could be done by automatically analyzing the quality of question (ie how clearly does the user express their information needs) and photo (ie if the image contains sufficient relevant information for a human to answer the question). One example is to include an automated quality check to extract keyword in the question, and check if image contains such object, before sending it out for community answering. \n",
    " - Train on domain-specific data set to improve object recognition. Compared to VQA, most objects in VizWiz questions focus on domestic objects.\n",
    " \n",
    "Ideas for text recognition tasks (30% of VizWiz):\n",
    " - Automatically detect text recognition taks based on the question and route the task to ML algorithms (reduce human efforts)\n",
    " - For the task above, detect if texts in the image are recognizable/answerable. \n",
    "\n",
    "Color recognition tasks (37% of VizWiz and 35% of VQA):\n",
    " - Many color recognition questions contains the word \"color\" in its question. We can perhaps focus on the image part and add a step of auto-correction or otherwise determine image quality when trying to label the task?\n",
    " \n",
    "Analysis of task/user:\n",
    " - Understand the correlations between the skills (eg. under what circumstances does an object recognition task require color recognition? Can we use one as a proxy to improve performance?)\n",
    " - Qualitative user study on their information need\n",
    " - Determine what type of questions likely require more skills as opposed to just one (eg. poorly lit image for object recognition may require color/text recognition--examples at the end of this file)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "High frequency words (log n scale, removing stop words)\n",
    "- VizWiz: daily objects & identification questions \n",
    "- VQA: more diverse contents; more descriptive nouns and verbs\n",
    "\n",
    "VizWiz\n",
    "![VizWiz](./vizwiz_wordcloud.svg)\n",
    "## [VizWiz Word Cloud](https://github.com/edithzeng/VQA_skills/blob/master/vizwiz_wordcloud.svg)\n",
    "\n",
    "VQA\n",
    "![VQA](./vqa_wordcloud.svg)\n",
    "## [VQA Word Cloud](https://github.com/edithzeng/VQA_skills/blob/master/vqa_wordcloud.svg)\n",
    "\n",
    "Images\n",
    "- VizWiz: inconsistent image quality\n",
    "- VQA: labelled image with MSCOCO captions\n",
    "\n",
    "Text + Image\n",
    "- For most entries in both datasets, skills are dependent on questions (text) only. Images are often irrelevant in determining the skills required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# traning sets only\n",
    "vizwiz = pd.read_csv('../dataset_skill_typ/vizwiz_skill_typ_train.csv', skipinitialspace=True, engine='python')\n",
    "vqa = pd.read_csv('../dataset_skill_typ/vqa_skill_typ_train.csv', skipinitialspace=True, engine='python')\n",
    "targets = np.array(['TXT', 'OBJ', 'COL', 'CNT', 'OTH'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14269"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vizwiz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3230"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vqa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check class distributions\n",
    "def skills_class_distribution(df):\n",
    "    class_dist = []\n",
    "    for skill in targets:\n",
    "        percent = len(df.loc[df[skill]==1])/len(df)\n",
    "        temp = {'skill':skill, '%':percent}\n",
    "        class_dist.append(temp)\n",
    "    return pd.DataFrame(class_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vizwiz_skill_dist = skills_class_distribution(vizwiz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>%</th>\n",
       "      <th>skill</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.561847</td>\n",
       "      <td>TXT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.984652</td>\n",
       "      <td>OBJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.379564</td>\n",
       "      <td>COL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.036513</td>\n",
       "      <td>CNT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001542</td>\n",
       "      <td>OTH</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          % skill\n",
       "0  0.561847   TXT\n",
       "1  0.984652   OBJ\n",
       "2  0.379564   COL\n",
       "3  0.036513   CNT\n",
       "4  0.001542   OTH"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vizwiz_skill_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vqa_skill_dist = skills_class_distribution(vqa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>%</th>\n",
       "      <th>skill</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.110836</td>\n",
       "      <td>TXT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.995975</td>\n",
       "      <td>OBJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.355418</td>\n",
       "      <td>COL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.204644</td>\n",
       "      <td>CNT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.018576</td>\n",
       "      <td>OTH</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          % skill\n",
       "0  0.110836   TXT\n",
       "1  0.995975   OBJ\n",
       "2  0.355418   COL\n",
       "3  0.204644   CNT\n",
       "4  0.018576   OTH"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vqa_skill_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(targets, vizwiz_skill_dist['%'].values, color='r', label='VizWiz')\n",
    "rects2 = ax.bar(targets, vqa_skill_dist['%'].values, color='b', label='VQA')\n",
    "plt.grid(alpha=.2)\n",
    "plt.legend()\n",
    "\n",
    "plt.show()  # note the difference in text recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = vizwiz.copy()\n",
    "df['obj_keyword'] = 0\n",
    "df['color_keyword'] = 0\n",
    "df['text_keyword'] = 0\n",
    "df['count_keyword'] = 0\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    question = row[2].lower()\n",
    "    if \"what is this\" in question:\n",
    "        df.iloc[i,19] = 1\n",
    "    if \"color\" in question:\n",
    "        df.iloc[i,20] = 1\n",
    "    if \"say\" in question:\n",
    "        df.iloc[i,21] = 1\n",
    "    if \"many\" in question or \"much\" in question:\n",
    "        df.iloc[i,22] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TXT</th>\n",
       "      <th>OBJ</th>\n",
       "      <th>COL</th>\n",
       "      <th>CNT</th>\n",
       "      <th>OTH</th>\n",
       "      <th>obj_keyword</th>\n",
       "      <th>color_keyword</th>\n",
       "      <th>text_keyword</th>\n",
       "      <th>count_keyword</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>TXT</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.031074</td>\n",
       "      <td>-0.479960</td>\n",
       "      <td>0.094344</td>\n",
       "      <td>-0.019298</td>\n",
       "      <td>0.006768</td>\n",
       "      <td>-0.432996</td>\n",
       "      <td>0.150167</td>\n",
       "      <td>0.019031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OBJ</th>\n",
       "      <td>0.031074</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.035096</td>\n",
       "      <td>0.006068</td>\n",
       "      <td>0.004906</td>\n",
       "      <td>0.060392</td>\n",
       "      <td>-0.127140</td>\n",
       "      <td>-0.001179</td>\n",
       "      <td>0.009684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>COL</th>\n",
       "      <td>-0.479960</td>\n",
       "      <td>-0.035096</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.106065</td>\n",
       "      <td>-0.016013</td>\n",
       "      <td>-0.130548</td>\n",
       "      <td>0.524348</td>\n",
       "      <td>-0.072632</td>\n",
       "      <td>-0.037827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CNT</th>\n",
       "      <td>0.094344</td>\n",
       "      <td>0.006068</td>\n",
       "      <td>-0.106065</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.007650</td>\n",
       "      <td>-0.082836</td>\n",
       "      <td>-0.071459</td>\n",
       "      <td>0.020494</td>\n",
       "      <td>0.297442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OTH</th>\n",
       "      <td>-0.019298</td>\n",
       "      <td>0.004906</td>\n",
       "      <td>-0.016013</td>\n",
       "      <td>-0.007650</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.019836</td>\n",
       "      <td>-0.016141</td>\n",
       "      <td>-0.008403</td>\n",
       "      <td>0.026325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>obj_keyword</th>\n",
       "      <td>0.006768</td>\n",
       "      <td>0.060392</td>\n",
       "      <td>-0.130548</td>\n",
       "      <td>-0.082836</td>\n",
       "      <td>-0.019836</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.239541</td>\n",
       "      <td>-0.123675</td>\n",
       "      <td>-0.061531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color_keyword</th>\n",
       "      <td>-0.432996</td>\n",
       "      <td>-0.127140</td>\n",
       "      <td>0.524348</td>\n",
       "      <td>-0.071459</td>\n",
       "      <td>-0.016141</td>\n",
       "      <td>-0.239541</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.069315</td>\n",
       "      <td>-0.030836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text_keyword</th>\n",
       "      <td>0.150167</td>\n",
       "      <td>-0.001179</td>\n",
       "      <td>-0.072632</td>\n",
       "      <td>0.020494</td>\n",
       "      <td>-0.008403</td>\n",
       "      <td>-0.123675</td>\n",
       "      <td>-0.069315</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.013173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>count_keyword</th>\n",
       "      <td>0.019031</td>\n",
       "      <td>0.009684</td>\n",
       "      <td>-0.037827</td>\n",
       "      <td>0.297442</td>\n",
       "      <td>0.026325</td>\n",
       "      <td>-0.061531</td>\n",
       "      <td>-0.030836</td>\n",
       "      <td>-0.013173</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    TXT       OBJ       COL       CNT       OTH  obj_keyword  \\\n",
       "TXT            1.000000  0.031074 -0.479960  0.094344 -0.019298     0.006768   \n",
       "OBJ            0.031074  1.000000 -0.035096  0.006068  0.004906     0.060392   \n",
       "COL           -0.479960 -0.035096  1.000000 -0.106065 -0.016013    -0.130548   \n",
       "CNT            0.094344  0.006068 -0.106065  1.000000 -0.007650    -0.082836   \n",
       "OTH           -0.019298  0.004906 -0.016013 -0.007650  1.000000    -0.019836   \n",
       "obj_keyword    0.006768  0.060392 -0.130548 -0.082836 -0.019836     1.000000   \n",
       "color_keyword -0.432996 -0.127140  0.524348 -0.071459 -0.016141    -0.239541   \n",
       "text_keyword   0.150167 -0.001179 -0.072632  0.020494 -0.008403    -0.123675   \n",
       "count_keyword  0.019031  0.009684 -0.037827  0.297442  0.026325    -0.061531   \n",
       "\n",
       "               color_keyword  text_keyword  count_keyword  \n",
       "TXT                -0.432996      0.150167       0.019031  \n",
       "OBJ                -0.127140     -0.001179       0.009684  \n",
       "COL                 0.524348     -0.072632      -0.037827  \n",
       "CNT                -0.071459      0.020494       0.297442  \n",
       "OTH                -0.016141     -0.008403       0.026325  \n",
       "obj_keyword        -0.239541     -0.123675      -0.061531  \n",
       "color_keyword       1.000000     -0.069315      -0.030836  \n",
       "text_keyword       -0.069315      1.000000      -0.013173  \n",
       "count_keyword      -0.030836     -0.013173       1.000000  "
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['TXT', 'OBJ', 'COL', 'CNT', 'OTH', 'obj_keyword', 'color_keyword', 'text_keyword','count_keyword']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7                                             This item.\n",
       "8                              What color do these look?\n",
       "10                                              Is this.\n",
       "11                                         what is this?\n",
       "12                           What kind of drink is this?\n",
       "13               What color... what color is this skirt?\n",
       "17     Can I ask about anything or any information wi...\n",
       "18                                    What is this game?\n",
       "19                                  What is in this can?\n",
       "22                            What is this a picture of?\n",
       "26     What color furniture is that?  And is that nic...\n",
       "30                              What is on the bar code?\n",
       "32                                   What is this spice?\n",
       "40     Please describe the pattern and color of this ...\n",
       "44                          What type of chips are this?\n",
       "54                               Is my - is my light on?\n",
       "56                                 What am I looking at?\n",
       "57                                          What is this\n",
       "58                               what color is his shirt\n",
       "59     Can you please tell me if you see the headphon...\n",
       "60                            What type of tree is this?\n",
       "67                                         What is this?\n",
       "71               What does the sky look like? Thank you.\n",
       "74                          What kind of coffee is this?\n",
       "76                      What colors are in this sweater?\n",
       "79                             What color is this shirt?\n",
       "80                              What is in this bottle? \n",
       "81                             What color is this shirt?\n",
       "83                                 Which shirt is green?\n",
       "84                                    What is this item?\n",
       "86                            What is this a picture of?\n",
       "93                             What kind of tea is this?\n",
       "98                                         What is this?\n",
       "99     Can you please tell me what kind of coffee or ...\n",
       "100                                 What is this bottle?\n",
       "104                             What color are my shoes?\n",
       "105                           What this student planner?\n",
       "106                           What color is this sweater\n",
       "110    Can you please tell me what color this shirt i...\n",
       "111                                  What? What is this?\n",
       "122                           What color are these jeans\n",
       "127                                     Oh, what's this?\n",
       "133                                        What is this?\n",
       "135                        What color is this cartridge?\n",
       "136                             Is this black or purple?\n",
       "137                            What color is this shirt?\n",
       "140                                        What is this?\n",
       "143                       What's this? Can you describe?\n",
       "148                                  what color is this?\n",
       "149    Are you able to see the screen? What does it say?\n",
       "Name: QSN, dtype: object"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# correlations between\n",
    "# word \"color\" in question and color recognition skill\n",
    "# word \"color\" in question and text recogintion skill\n",
    "\n",
    "# due to class imbalance (98% true), the Pearson correlation coef\n",
    "# between \"what is this\" and \"object recognition\" flag is very small\n",
    "\n",
    "# For questions that are labelled as \"color recognition\" besides \"object recognition\"\n",
    "# but does not contain the word \"color\" in the question,\n",
    "# many are due to poor photos. eg. \"What is this spice?\" with strange lighting.\n",
    "# Therefore, working on determine the image quality could be another way.\n",
    "df.loc[df['COL'] == 1, 'QSN'].head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
