{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skill label prediction with image-based features (text and color with descriptive tags)\n",
    "### Text recognition category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/edithzeng/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/edithzeng/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/edithzeng/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 17755839873414014374\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 271777792\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 1954441188433325375\n",
      "physical_device_desc: \"device: 0, name: Tesla K80, pci bus id: 365b:00:00.0, compute capability: 3.7\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import random\n",
    "import numpy as np\n",
    "import copy\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "kfold=KFold(n_splits=10)\n",
    "\n",
    "# for LSTM (keras with tf backend)\n",
    "import gzip\n",
    "import os\n",
    "import pickle\n",
    "import requests\n",
    "import time\n",
    "import re\n",
    "# os.environ['KERAS_BACKEND']='cntk'\n",
    "os.environ['KERAS_BACKEND']='tensorflow'\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.models import Sequential, load_model\n",
    "from keras import regularizers\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.initializers import he_normal\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional\n",
    "from keras.callbacks import History, CSVLogger\n",
    "from keras.utils import to_categorical\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import time \n",
    "import keras\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto( device_count = {'GPU': 1 , 'CPU': 56} ) \n",
    "sess = tf.Session(config=config) \n",
    "keras.backend.set_session(sess)\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "\n",
    "MAX_DOC_LEN = 40\n",
    "VOCAB_SIZE = 3000\n",
    "EMBEDDING_DIM = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/job:localhost/replica:0/task:0/device:GPU:0']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vizwiz_features_train_color = pd.read_csv('azure_features_images/data/vizwiz_train_color_recognition.csv',\n",
    "                                    delimiter=';', engine='python', \n",
    "                                    dtype={'qid':str, 'question':str, 'descriptions':list,\n",
    "                                          'tags':list, 'dominant_colors':list},\n",
    "                                    quotechar='\"', error_bad_lines=False)\n",
    "vizwiz_features_train_text = pd.read_csv('azure_features_images/data/vizwiz_train_text_recognition.csv',\n",
    "                                    delimiter=';', engine='python', \n",
    "                                    dtype={'qid':str, 'question':str, 'descriptions':list,\n",
    "                                          'ocr_text':list, 'handwritten_text':list},\n",
    "                                    quotechar='\"', error_bad_lines=False)\n",
    "\n",
    "\n",
    "vizwiz_features_val_color = pd.read_csv('azure_features_images/data/vizwiz_val_color_recognition.csv',\n",
    "                                  delimiter=';', engine='python',\n",
    "                                  dtype={'qid':str, 'question':str, 'descriptions':list,\n",
    "                                          'tags':list, 'dominant_colors':list},\n",
    "                                  quotechar='\"', error_bad_lines=False)\n",
    "vizwiz_features_val_text = pd.read_csv('azure_features_images/data/vizwiz_val_text_recognition.csv',\n",
    "                                    delimiter=';', engine='python', \n",
    "                                    dtype={'qid':str, 'question':str, 'descriptions':list,\n",
    "                                          'ocr_text':list, 'handwritten_text':list},\n",
    "                                    quotechar='\"', error_bad_lines=False)\n",
    "\n",
    "\n",
    "vqa_features_train_color = pd.read_csv('azure_features_images/data/vqa_train_color_recognition.csv',\n",
    "                                 delimiter=';', engine='python', \n",
    "                                 dtype={'qid':str, 'question':str, 'descriptions':list,\n",
    "                                          'tags':list, 'dominant_colors':list},\n",
    "                                 quotechar='\"', error_bad_lines=False)\n",
    "vqa_features_train_text = pd.read_csv('azure_features_images/data/vqa_train_text_recognition.csv',\n",
    "                                    delimiter=';', engine='python', \n",
    "                                    dtype={'qid':str, 'question':str, 'descriptions':list,\n",
    "                                          'ocr_text':list, 'handwritten_text':list},\n",
    "                                    quotechar='\"', error_bad_lines=False)\n",
    "vqa_features_val_color = pd.read_csv('azure_features_images/data/vqa_val_color_recognition.csv',\n",
    "                               delimiter=';', engine='python',\n",
    "                               dtype={'qid':str, 'question':str, 'descriptions':list,\n",
    "                                          'tags':list, 'dominant_colors':list},\n",
    "                               quotechar='\"', error_bad_lines=False)\n",
    "vqa_features_val_text = pd.read_csv('azure_features_images/data/vqa_val_text_recognition.csv',\n",
    "                                    delimiter=';', engine='python', \n",
    "                                    dtype={'qid':str, 'question':str, 'descriptions':list,\n",
    "                                          'ocr_text':list, 'handwritten_text':list},\n",
    "                                    quotechar='\"', error_bad_lines=False)\n",
    "\n",
    "\n",
    "vizwiz_targets_train = pd.read_csv('../vizwiz_skill_typ_train.csv', dtype={'QID':str},\n",
    "                                   delimiter=',', quotechar='\"',\n",
    "                                   engine='python', error_bad_lines=False)\n",
    "vizwiz_targets_val = pd.read_csv('../vizwiz_skill_typ_val.csv', dtype={'QID':str},\n",
    "                                 delimiter=',', quotechar='\"', engine='python', error_bad_lines=False)\n",
    "vqa_targets_train = pd.read_csv('../vqa_skill_typ_train.csv', dtype={'QID':str},\n",
    "                               engine='python', quotechar='\"', error_bad_lines=False)\n",
    "vqa_targets_val = pd.read_csv('../vqa_skill_typ_val.csv', dtype={'QID':str},\n",
    "                               engine='python', quotechar='\"', error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>question</th>\n",
       "      <th>ocr_text</th>\n",
       "      <th>handwritten_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>VizWiz_train_000000000000.jpg</td>\n",
       "      <td>What's the name of this product?</td>\n",
       "      <td>['b', 'sil', 'leaves', '0.62', 'oz', '(170)']</td>\n",
       "      <td>['NET WT O. 62 02 ( 179)']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>VizWiz_train_000000000001.jpg</td>\n",
       "      <td>Can you tell me what is in this can please?</td>\n",
       "      <td>[]</td>\n",
       "      <td>['^TAKE Three 1^']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             qid                                     question  \\\n",
       "0  VizWiz_train_000000000000.jpg             What's the name of this product?   \n",
       "1  VizWiz_train_000000000001.jpg  Can you tell me what is in this can please?   \n",
       "\n",
       "                                        ocr_text            handwritten_text  \n",
       "0  ['b', 'sil', 'leaves', '0.62', 'oz', '(170)']  ['NET WT O. 62 02 ( 179)']  \n",
       "1                                             []          ['^TAKE Three 1^']  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vizwiz_features_train_text.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_feature_target(feature_df_text, feature_df_color, target_df):\n",
    "    feature_text = copy.deepcopy(feature_df_text)\n",
    "    feature_color = copy.deepcopy(feature_df_color)\n",
    "    target = copy.deepcopy(target_df)\n",
    "    # text features \n",
    "    feature_text.rename({'qid': 'QID'}, axis=1, inplace=True)\n",
    "    feature_text.set_index('QID', inplace=True)\n",
    "    # color features\n",
    "    feature_color.rename({'qid': 'QID'}, axis=1, inplace=True)\n",
    "    feature_color.set_index('QID', inplace=True)\n",
    "    # join features\n",
    "    features = feature_text.join(feature_color[['descriptions','tags','dominant_colors']],\n",
    "                                 on='QID',\n",
    "                                 how='outer')\n",
    "    # join features with target\n",
    "    target = target[['QID', 'IMG', 'QSN', 'TXT', 'OBJ', 'COL', 'CNT', 'OTH']]\n",
    "    target.set_index('QID', inplace=True)\n",
    "    target = target.astype(dtype=str)\n",
    "    df = target.join(features, on='QID', how='inner')\n",
    "    df['descriptions'].astype(list)\n",
    "    return df\n",
    "\n",
    "def lem(s):\n",
    "    arr = s.split(\" \")\n",
    "    lem = WordNetLemmatizer()\n",
    "    op = \"\"\n",
    "    for w in arr:\n",
    "        word = lem.lemmatize(w) + ' '\n",
    "        op += word\n",
    "    return op\n",
    "\n",
    "def preprocess_text(feature_columns):\n",
    "    \"\"\" output an nparray with single document per data point \"\"\"\n",
    "    ip = copy.deepcopy(feature_columns).values\n",
    "    op = []\n",
    "    for i in range(ip.shape[0]):\n",
    "        doc      =  \"\"\n",
    "        for j in range(ip.shape[1]):\n",
    "            # clean up chars\n",
    "            s    =  str(ip[i][j])\n",
    "            s    =  s.translate({ord(c): \"\" for c in \"!@#$%^&*()[]{};:,./<>?\\|`~-=_+'\"}).lower() + \" \"\n",
    "            if j == 1:             # clean descriptions\n",
    "                s = re.sub(r'confidence\\s+\\d+', '', s)\n",
    "                s = re.sub(r'text', '', s)\n",
    "            # lexicon normalize\n",
    "            s    = lem(s)\n",
    "            doc  += s\n",
    "        op.append(doc)\n",
    "    op = np.asarray(op)\n",
    "    return op\n",
    "\n",
    "#def remove_stop_words(features_array):\n",
    "#    stop_words = set(stopwords.words(features_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14257, 13) (3230, 13) 17487\n",
      "(2247, 13) (513, 13) 2760\n"
     ]
    }
   ],
   "source": [
    "vizwiz_train   = join_feature_target(vizwiz_features_train_text, \n",
    "                                   vizwiz_features_train_color, \n",
    "                                   vizwiz_targets_train)\n",
    "vizwiz_val     = join_feature_target(vizwiz_features_val_text, \n",
    "                                 vizwiz_features_val_color, \n",
    "                                 vizwiz_targets_val)\n",
    "vqa_train      = join_feature_target(vqa_features_train_text, \n",
    "                                   vqa_features_train_color, \n",
    "                                   vqa_targets_train)\n",
    "vqa_val        = join_feature_target(vizwiz_features_val_text, \n",
    "                                 vqa_features_val_color, \n",
    "                                 vqa_targets_val)\n",
    "print(vizwiz_train.shape, vqa_train.shape, vizwiz_train.shape[0] + vqa_train.shape[0])\n",
    "print(vizwiz_val.shape, vqa_val.shape, vizwiz_val.shape[0] + vqa_val.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: (14257, 13)\n",
      "Validation: (2247, 13)\n"
     ]
    }
   ],
   "source": [
    "# VizWiz only\n",
    "train = vizwiz_train\n",
    "val = vizwiz_val\n",
    "print(\"Training: {}\\nValidation: {}\".format(train.shape, val.shape))\n",
    "\n",
    "features_train = preprocess_text(train[['QSN','descriptions', 'tags', 'dominant_colors', \n",
    "                                        'handwritten_text', 'ocr_text']])\n",
    "txt_train      = train['TXT'].values\n",
    "col_train      = train['COL'].values\n",
    "cnt_train      = train['CNT'].values\n",
    "\n",
    "features_val   = preprocess_text(val[['QSN', 'descriptions', 'tags', 'dominant_colors',\n",
    "                                      'handwritten_text', 'ocr_text']])\n",
    "txt_val        = val['TXT'].values.astype('float32')\n",
    "col_val        = val['COL'].values.astype('float32')\n",
    "cnt_val        = val['CNT'].values.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: (3230, 13)\n",
      "Validation: (513, 13)\n"
     ]
    }
   ],
   "source": [
    "# VQA only\n",
    "train = vqa_train\n",
    "val = vqa_val\n",
    "print(\"Training: {}\\nValidation: {}\".format(train.shape, val.shape))\n",
    "\n",
    "features_train = preprocess_text(train[['QSN','descriptions', 'tags', 'dominant_colors', \n",
    "                                        'handwritten_text', 'ocr_text']])\n",
    "txt_train      = train['TXT'].values\n",
    "col_train      = train['COL'].values\n",
    "cnt_train      = train['CNT'].values\n",
    "\n",
    "features_val   = preprocess_text(val[['QSN', 'descriptions', 'tags', 'dominant_colors',\n",
    "                                      'handwritten_text', 'ocr_text']])\n",
    "txt_val        = val['TXT'].values.astype('float32')\n",
    "col_val        = val['COL'].values.astype('float32')\n",
    "cnt_val        = val['CNT'].values.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: (17487, 13)\n",
      "Validation: (2760, 13)\n"
     ]
    }
   ],
   "source": [
    "# Combining Vizwiz and VQA (create X and Y)\n",
    "\n",
    "train = pd.concat([vizwiz_train, vqa_train], axis=0)\n",
    "val   = pd.concat([vizwiz_val, vqa_val], axis=0)\n",
    "print(\"Training: {}\\nValidation: {}\".format(train.shape, val.shape))\n",
    "\n",
    "features_train = preprocess_text(train[['QSN','descriptions', 'tags', 'dominant_colors', \n",
    "                                        'handwritten_text', 'ocr_text']])\n",
    "txt_train      = train['TXT'].values\n",
    "col_train      = train['COL'].values\n",
    "cnt_train      = train['CNT'].values\n",
    "\n",
    "features_val   = preprocess_text(val[['QSN', 'descriptions', 'tags', 'dominant_colors',\n",
    "                                      'handwritten_text', 'ocr_text']])\n",
    "txt_val        = val['TXT'].values.astype('float32')\n",
    "col_val        = val['COL'].values.astype('float32')\n",
    "cnt_val        = val['CNT'].values.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'what kind of pudding is this   a person holding a remote control   person indoor remote sitting holding man orange control red television boy girl young bed playing food table room video game white standing shirt  black      '"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_train[random.randint(0,len(features_val))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i know this is healthy choice but what dinner is it   a close up of food on a table   indoor table food sitting green plate restaurant wooden sandwich white  black brown  healthy choice complete meall with dessert solwin healthy  complete meal healthy with dessert •hoice 9g fiber 310 calorie homestyle bury salis steak salisbury steak in sautéed onion red apple ca crisp multigrain  '"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_val[random.randint(0,len(features_val))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# remove stop words\n",
    "stop_words     = set(stopwords.words('english'))\n",
    "features_train = [w for w in features_train if not w in stop_words]\n",
    "features_val   = [w for w in features_val if not w in stop_words]\n",
    "\n",
    "# tokenize\n",
    "tok        = Tokenizer(num_words=VOCAB_SIZE, \n",
    "                       filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "                       lower=True,\n",
    "                       split=\" \")\n",
    "tok.fit_on_texts(features_train)\n",
    "\n",
    "# create sequences & pad\n",
    "train_seq  = tok.texts_to_sequences(features_train)\n",
    "train_seq  = sequence.pad_sequences(train_seq, maxlen=MAX_DOC_LEN)\n",
    "val_seq    = tok.texts_to_sequences(features_val)\n",
    "val_seq    = sequence.pad_sequences(val_seq, maxlen=MAX_DOC_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples each class: \n",
      "Text recognition [6247. 8010.]\n",
      "Color recognition [8844. 5413.]\n"
     ]
    }
   ],
   "source": [
    "# check class distribution\n",
    "text_recognition_labels = to_categorical(np.asarray(txt_train)).astype('float32')\n",
    "color_recognition_labels = to_categorical(np.asarray(col_train)).astype('float32')\n",
    "print('Number of samples each class: ')\n",
    "print('Text recognition', text_recognition_labels.sum(axis=0))\n",
    "print('Color recognition', color_recognition_labels.sum(axis=0))\n",
    "n_classes = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM model (skip-gram word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import requests\n",
    "import time\n",
    "import gzip\n",
    "\n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.models import Sequential, load_model\n",
    "from keras import regularizers\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional\n",
    "from keras.callbacks import History, CSVLogger\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import nltk \n",
    "import gensim\n",
    "import logging\n",
    "\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve\n",
    "\n",
    "MAX_DOC_LEN = 40\n",
    "VOCAB_SIZE = 3000\n",
    "EMBEDDING_DIM = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# punkt sentence level tokenizer\n",
    "sent_lst = []\n",
    "for doc in features_train:\n",
    "    sentences = nltk.tokenize.sent_tokenize(doc)\n",
    "    for sent in sentences:\n",
    "        word_lst = [w for w in nltk.tokenize.word_tokenize(sent) if w.isalnum()]\n",
    "        sent_lst.append(word_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-05 22:25:52,060 : INFO : collecting all words and their counts\n",
      "2019-03-05 22:25:52,061 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-03-05 22:25:52,124 : INFO : PROGRESS: at sentence #10000, processed 383294 words, keeping 24880 word types\n",
      "2019-03-05 22:25:52,151 : INFO : collected 31762 word types from a corpus of 541792 raw words and 14257 sentences\n",
      "2019-03-05 22:25:52,152 : INFO : Loading a fresh vocabulary\n",
      "2019-03-05 22:25:52,172 : INFO : effective_min_count=6 retains 3565 unique words (11% of original 31762, drops 28197)\n",
      "2019-03-05 22:25:52,172 : INFO : effective_min_count=6 leaves 501856 word corpus (92% of original 541792, drops 39936)\n",
      "2019-03-05 22:25:52,182 : INFO : deleting the raw counts dictionary of 31762 items\n",
      "2019-03-05 22:25:52,183 : INFO : sample=0.001 downsamples 67 most-common words\n",
      "2019-03-05 22:25:52,184 : INFO : downsampling leaves estimated 318802 word corpus (63.5% of prior 501856)\n",
      "2019-03-05 22:25:52,191 : INFO : estimated required memory for 3565 words and 100 dimensions: 4634500 bytes\n",
      "2019-03-05 22:25:52,192 : INFO : resetting layer weights\n",
      "2019-03-05 22:25:52,246 : INFO : training model with 6 workers on 3565 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-03-05 22:25:52,651 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-03-05 22:25:52,652 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-03-05 22:25:52,656 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-05 22:25:52,656 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-05 22:25:52,657 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-05 22:25:52,660 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-05 22:25:52,661 : INFO : EPOCH - 1 : training on 541792 raw words (319157 effective words) took 0.4s, 781883 effective words/s\n",
      "2019-03-05 22:25:53,055 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-03-05 22:25:53,059 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-03-05 22:25:53,060 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-05 22:25:53,062 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-05 22:25:53,076 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-05 22:25:53,077 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-05 22:25:53,078 : INFO : EPOCH - 2 : training on 541792 raw words (318833 effective words) took 0.4s, 778566 effective words/s\n",
      "2019-03-05 22:25:53,452 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-03-05 22:25:53,457 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-03-05 22:25:53,464 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-05 22:25:53,468 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-05 22:25:53,475 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-05 22:25:53,476 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-05 22:25:53,476 : INFO : EPOCH - 3 : training on 541792 raw words (318910 effective words) took 0.4s, 811577 effective words/s\n",
      "2019-03-05 22:25:53,854 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-03-05 22:25:53,860 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-03-05 22:25:53,862 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-05 22:25:53,868 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-05 22:25:53,871 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-05 22:25:53,876 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-05 22:25:53,877 : INFO : EPOCH - 4 : training on 541792 raw words (318641 effective words) took 0.4s, 808435 effective words/s\n",
      "2019-03-05 22:25:54,250 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-03-05 22:25:54,261 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-03-05 22:25:54,262 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-05 22:25:54,272 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-05 22:25:54,277 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-05 22:25:54,279 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-05 22:25:54,280 : INFO : EPOCH - 5 : training on 541792 raw words (319015 effective words) took 0.4s, 803039 effective words/s\n",
      "2019-03-05 22:25:54,280 : INFO : training on a 2708960 raw words (1594556 effective words) took 2.0s, 784181 effective words/s\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "word2vec_model = gensim.models.Word2Vec(sentences=sent_lst,\n",
    "                                        min_count=6,\n",
    "                                        size=EMBEDDING_DIM,\n",
    "                                        sg=1,\n",
    "                                        workers=os.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 3565 word vectors\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "\n",
    "for word in word2vec_model.wv.vocab:\n",
    "    coefs = np.asarray(word2vec_model.wv[word], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "\n",
    "print('Total %s word vectors' % len(embeddings_index))\n",
    "\n",
    "# Initial word embedding\n",
    "embedding_matrix = np.zeros((VOCAB_SIZE, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in tok.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None and i < VOCAB_SIZE:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_create_train(labels, learning_rate, lstm_dim, batch_size, num_epochs, optimizer_param, regularization=1e-7):\n",
    "    \n",
    "    l2_reg = regularizers.l2(regularization)\n",
    "    \n",
    "    # init model\n",
    "    embedding_layer = Embedding(VOCAB_SIZE,\n",
    "                                EMBEDDING_DIM,\n",
    "                                input_length=MAX_DOC_LEN,\n",
    "                                trainable=True,\n",
    "                                mask_zero=False,\n",
    "                                embeddings_regularizer=l2_reg,\n",
    "                                weights=[embedding_matrix])\n",
    "    lstm_layer = LSTM(units=lstm_dim, kernel_regularizer=l2_reg)\n",
    "    dense_layer = Dense(n_classes,\n",
    "                        activation='softmax', \n",
    "                        kernel_regularizer=l2_reg)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(embedding_layer)\n",
    "    model.add(Bidirectional(lstm_layer))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(dense_layer)\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizer_param,\n",
    "                  metrics=['acc'])\n",
    "    history = History()\n",
    "    csv_logger = CSVLogger('./LSTM/text/{}_{}_{}_{}.log'.format(learning_rate, regularization, batch_size, num_epochs),\n",
    "                           separator=',',\n",
    "                           append=True)\n",
    "    t1 = time.time()\n",
    "    # model fit\n",
    "    model.fit(train_seq,\n",
    "              labels.astype('float32'),\n",
    "              batch_size=batch_size,\n",
    "              epochs=num_epochs,\n",
    "              callbacks=[history, csv_logger],\n",
    "              verbose=2)\n",
    "    t2 = time.time()\n",
    "    # save hdf5\n",
    "    model.save('./LSTM/text/{}_{}_{}_{}_model.h5'.format(learning_rate, regularization, batch_size, num_epochs))\n",
    "    np.savetxt('./LSTM/text/{}_{}_{}_{}_time.txt'.format(learning_rate, regularization, batch_size, num_epochs), \n",
    "               [regularization, (t2-t1) / 3600])\n",
    "    with open('./LSTM/text/{}_{}_{}_{}_history.txt'.format(learning_rate, regularization, batch_size, num_epochs), \"w\") as res_file:\n",
    "        res_file.write(str(history.history))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Vizwiz + VQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      " - 31s - loss: 0.4646 - acc: 0.7892\n",
      "Epoch 2/30\n",
      " - 31s - loss: 0.4124 - acc: 0.8254\n",
      "Epoch 3/30\n",
      " - 31s - loss: 0.3873 - acc: 0.8391\n",
      "Epoch 4/30\n",
      " - 31s - loss: 0.3728 - acc: 0.8457\n",
      "Epoch 5/30\n",
      " - 31s - loss: 0.3622 - acc: 0.8525\n",
      "Epoch 6/30\n",
      " - 31s - loss: 0.3563 - acc: 0.8542\n",
      "Epoch 7/30\n",
      " - 31s - loss: 0.3511 - acc: 0.8585\n",
      "Epoch 8/30\n",
      " - 31s - loss: 0.3448 - acc: 0.8593\n",
      "Epoch 9/30\n",
      " - 31s - loss: 0.3416 - acc: 0.8619\n",
      "Epoch 10/30\n",
      " - 31s - loss: 0.3355 - acc: 0.8674\n",
      "Epoch 11/30\n",
      " - 31s - loss: 0.3305 - acc: 0.8671\n",
      "Epoch 12/30\n",
      " - 31s - loss: 0.3310 - acc: 0.8672\n",
      "Epoch 13/30\n",
      " - 31s - loss: 0.3264 - acc: 0.8707\n",
      "Epoch 14/30\n",
      " - 31s - loss: 0.3200 - acc: 0.8730\n",
      "Epoch 15/30\n",
      " - 31s - loss: 0.3205 - acc: 0.8727\n",
      "Epoch 16/30\n",
      " - 31s - loss: 0.3173 - acc: 0.8753\n",
      "Epoch 17/30\n",
      " - 31s - loss: 0.3141 - acc: 0.8760\n",
      "Epoch 18/30\n",
      " - 31s - loss: 0.3098 - acc: 0.8788\n",
      "Epoch 19/30\n",
      " - 31s - loss: 0.3080 - acc: 0.8775\n",
      "Epoch 20/30\n",
      " - 31s - loss: 0.3061 - acc: 0.8792\n",
      "Epoch 21/30\n",
      " - 31s - loss: 0.2993 - acc: 0.8809\n",
      "Epoch 22/30\n",
      " - 31s - loss: 0.2998 - acc: 0.8821\n",
      "Epoch 23/30\n",
      " - 31s - loss: 0.2954 - acc: 0.8843\n",
      "Epoch 24/30\n",
      " - 31s - loss: 0.2935 - acc: 0.8838\n",
      "Epoch 25/30\n",
      " - 31s - loss: 0.2904 - acc: 0.8865\n",
      "Epoch 26/30\n",
      " - 31s - loss: 0.2881 - acc: 0.8876\n",
      "Epoch 27/30\n",
      " - 31s - loss: 0.2821 - acc: 0.8902\n",
      "Epoch 28/30\n",
      " - 31s - loss: 0.2780 - acc: 0.8932\n",
      "Epoch 29/30\n",
      " - 31s - loss: 0.2758 - acc: 0.8904\n",
      "Epoch 30/30\n",
      " - 31s - loss: 0.2710 - acc: 0.8934\n",
      "Learning rate: 0.2 Regularization: 1e-14 Batch size: 32 Epoch: 30\n",
      "Accuracy = 0.8496376811594203 \t AUC = 0.9012556082540055\n"
     ]
    }
   ],
   "source": [
    "L = 2e-1\n",
    "R = 1e-14\n",
    "B = 32\n",
    "E = 30\n",
    "lstm_create_train(labels=text_recognition_labels,\n",
    "                                  learning_rate=L,\n",
    "                                  lstm_dim=150, \n",
    "                                  batch_size=B,\n",
    "                                  num_epochs=E, \n",
    "                                  optimizer_param=SGD(lr=L, nesterov=True),\n",
    "                                  regularization=R)\n",
    "model = load_model('./LSTM/text/{}_{}_{}_{}_model.h5'.format(L,R,B,E))\n",
    "preds = model.predict(val_seq, verbose=0)\n",
    "print(\"Learning rate: {} Regularization: {} Batch size: {} Epoch: {}\".format(L,R,B,E))\n",
    "print((\"Accuracy = {0} \\t AUC = {1}\".format(accuracy_score(txt_val, preds.argmax(axis=1)), \n",
    "                                                            roc_auc_score(txt_val, preds[:,1]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      " - 57s - loss: 0.4622 - acc: 0.7857\n",
      "Epoch 2/30\n",
      " - 56s - loss: 0.4101 - acc: 0.8246\n",
      "Epoch 3/30\n",
      " - 55s - loss: 0.3879 - acc: 0.8401\n",
      "Epoch 4/30\n",
      " - 55s - loss: 0.3718 - acc: 0.8455\n",
      "Epoch 5/30\n",
      " - 55s - loss: 0.3643 - acc: 0.8506\n",
      "Epoch 6/30\n",
      " - 56s - loss: 0.3537 - acc: 0.8573\n",
      "Epoch 7/30\n",
      " - 55s - loss: 0.3517 - acc: 0.8567\n",
      "Epoch 8/30\n",
      " - 55s - loss: 0.3442 - acc: 0.8612\n",
      "Epoch 9/30\n",
      " - 55s - loss: 0.3401 - acc: 0.8641\n",
      "Epoch 10/30\n",
      " - 55s - loss: 0.3363 - acc: 0.8658\n",
      "Epoch 11/30\n",
      " - 55s - loss: 0.3346 - acc: 0.8656\n",
      "Epoch 12/30\n",
      " - 55s - loss: 0.3307 - acc: 0.8669\n",
      "Epoch 13/30\n",
      " - 55s - loss: 0.3275 - acc: 0.8704\n",
      "Epoch 14/30\n",
      " - 55s - loss: 0.3216 - acc: 0.8717\n",
      "Epoch 15/30\n",
      " - 55s - loss: 0.3189 - acc: 0.8728\n",
      "Epoch 16/30\n",
      " - 55s - loss: 0.3161 - acc: 0.8744\n",
      "Epoch 17/30\n",
      " - 55s - loss: 0.3161 - acc: 0.8721\n",
      "Epoch 18/30\n",
      " - 55s - loss: 0.3116 - acc: 0.8768\n",
      "Epoch 19/30\n",
      " - 55s - loss: 0.3083 - acc: 0.8791\n",
      "Epoch 20/30\n",
      " - 55s - loss: 0.3056 - acc: 0.8802\n",
      "Epoch 21/30\n",
      " - 57s - loss: 0.3023 - acc: 0.8831\n",
      "Epoch 22/30\n",
      " - 57s - loss: 0.3002 - acc: 0.8794\n",
      "Epoch 23/30\n",
      " - 56s - loss: 0.2948 - acc: 0.8845\n",
      "Epoch 24/30\n",
      " - 56s - loss: 0.2941 - acc: 0.8859\n",
      "Epoch 25/30\n",
      " - 56s - loss: 0.2927 - acc: 0.8844\n",
      "Epoch 26/30\n",
      " - 56s - loss: 0.2875 - acc: 0.8875\n",
      "Epoch 27/30\n",
      " - 56s - loss: 0.2817 - acc: 0.8894\n",
      "Epoch 28/30\n",
      " - 56s - loss: 0.2819 - acc: 0.8893\n",
      "Epoch 29/30\n",
      " - 56s - loss: 0.2767 - acc: 0.8902\n",
      "Epoch 30/30\n",
      " - 55s - loss: 0.2716 - acc: 0.8931\n",
      "Learning rate: 0.2 Regularization: 0 Batch size: 32 Epoch: 30\n",
      "Accuracy = 0.8467391304347827 \t AUC = 0.9062736302837314\n"
     ]
    }
   ],
   "source": [
    "# todo\n",
    "L = 2e-1\n",
    "R = 0\n",
    "B = 32\n",
    "E = 30\n",
    "lstm_create_train(labels=text_recognition_labels,\n",
    "                                  learning_rate=L,\n",
    "                                  lstm_dim=200, \n",
    "                                  batch_size=B,\n",
    "                                  num_epochs=E, \n",
    "                                  optimizer_param=SGD(lr=L, nesterov=True),\n",
    "                                  regularization=R)\n",
    "model = load_model('./LSTM/text/{}_{}_{}_{}_model.h5'.format(L,R,B,E))\n",
    "preds = model.predict(val_seq, verbose=0)\n",
    "print(\"Learning rate: {} Regularization: {} Batch size: {} Epoch: {}\".format(L,R,B,E))\n",
    "print((\"Accuracy = {0} \\t AUC = {1}\".format(accuracy_score(txt_val, preds.argmax(axis=1)), \n",
    "                                                            roc_auc_score(txt_val, preds[:,1]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and validation on VizWiz only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VizWiz only - varying learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      " - 46s - loss: 0.7091 - acc: 0.4469\n",
      "Epoch 2/30\n",
      " - 45s - loss: 0.7090 - acc: 0.4468\n",
      "Epoch 3/30\n",
      " - 45s - loss: 0.7102 - acc: 0.4399\n",
      "Epoch 4/30\n",
      " - 45s - loss: 0.7096 - acc: 0.4419\n",
      "Epoch 5/30\n",
      " - 45s - loss: 0.7090 - acc: 0.4497\n"
     ]
    }
   ],
   "source": [
    "# GPU \n",
    "L = 1e-8\n",
    "R = 0\n",
    "B = 32\n",
    "E = 30\n",
    "start = time.time()\n",
    "\n",
    "lstm_create_train(labels=text_recognition_labels,\n",
    "                                  learning_rate=L,\n",
    "                                  lstm_dim=150, \n",
    "                                  batch_size=B,\n",
    "                                  num_epochs=E, \n",
    "                                  optimizer_param=SGD(lr=L, nesterov=True),\n",
    "                                  regularization=R)\n",
    "end = time.time()\n",
    "model = load_model('./LSTM/text/{}_{}_{}_{}_model.h5'.format(L,R,B,E))\n",
    "preds = model.predict(val_seq, verbose=0)\n",
    "print(\"Learning rate: {} Regularization: {} Batch size: {} Epoch: {}\".format(L,R,B,E))\n",
    "print((\"Accuracy = {0} \\t AUC = {1}\".format(accuracy_score(txt_val, preds.argmax(axis=1)), \n",
    "                                                            roc_auc_score(txt_val, preds[:,1]))))\n",
    "print(\"----- Total training: {} seconds -----\".format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU \n",
    "L = 1e-5\n",
    "R = 0\n",
    "B = 32\n",
    "E = 30\n",
    "start = time.time()\n",
    "\n",
    "lstm_create_train(labels=text_recognition_labels,\n",
    "                                  learning_rate=L,\n",
    "                                  lstm_dim=150, \n",
    "                                  batch_size=B,\n",
    "                                  num_epochs=E, \n",
    "                                  optimizer_param=SGD(lr=L, nesterov=True),\n",
    "                                  regularization=R)\n",
    "end = time.time()\n",
    "model = load_model('./LSTM/text/{}_{}_{}_{}_model.h5'.format(L,R,B,E))\n",
    "preds = model.predict(val_seq, verbose=0)\n",
    "print(\"Learning rate: {} Regularization: {} Batch size: {} Epoch: {}\".format(L,R,B,E))\n",
    "print((\"Accuracy = {0} \\t AUC = {1}\".format(accuracy_score(txt_val, preds.argmax(axis=1)), \n",
    "                                                            roc_auc_score(txt_val, preds[:,1]))))\n",
    "print(\"----- Total training: {} seconds -----\".format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU \n",
    "L = 1e-3\n",
    "R = 0\n",
    "B = 32\n",
    "E = 30\n",
    "start = time.time()\n",
    "\n",
    "lstm_create_train(labels=text_recognition_labels,\n",
    "                                  learning_rate=L,\n",
    "                                  lstm_dim=150, \n",
    "                                  batch_size=B,\n",
    "                                  num_epochs=E, \n",
    "                                  optimizer_param=SGD(lr=L, nesterov=True),\n",
    "                                  regularization=R)\n",
    "end = time.time()\n",
    "model = load_model('./LSTM/text/{}_{}_{}_{}_model.h5'.format(L,R,B,E))\n",
    "preds = model.predict(val_seq, verbose=0)\n",
    "print(\"Learning rate: {} Regularization: {} Batch size: {} Epoch: {}\".format(L,R,B,E))\n",
    "print((\"Accuracy = {0} \\t AUC = {1}\".format(accuracy_score(txt_val, preds.argmax(axis=1)), \n",
    "                                                            roc_auc_score(txt_val, preds[:,1]))))\n",
    "print(\"----- Total training: {} seconds -----\".format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# GPU \n",
    "L = 1e-1\n",
    "R = 0\n",
    "B = 32\n",
    "E = 30\n",
    "start = time.time()\n",
    "\n",
    "lstm_create_train(labels=text_recognition_labels,\n",
    "                                  learning_rate=L,\n",
    "                                  lstm_dim=150, \n",
    "                                  batch_size=B,\n",
    "                                  num_epochs=E, \n",
    "                                  optimizer_param=SGD(lr=L, nesterov=True),\n",
    "                                  regularization=R)\n",
    "end = time.time()\n",
    "model = load_model('./LSTM/text/{}_{}_{}_{}_model.h5'.format(L,R,B,E))\n",
    "preds = model.predict(val_seq, verbose=0)\n",
    "print(\"Learning rate: {} Regularization: {} Batch size: {} Epoch: {}\".format(L,R,B,E))\n",
    "print((\"Accuracy = {0} \\t AUC = {1}\".format(accuracy_score(txt_val, preds.argmax(axis=1)), \n",
    "                                                            roc_auc_score(txt_val, preds[:,1]))))\n",
    "print(\"----- Total training: {} seconds -----\".format(end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VQA only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      " - 24s - loss: 0.3398 - acc: 0.8830\n",
      "Epoch 2/30\n",
      " - 21s - loss: 0.3161 - acc: 0.8879\n",
      "Epoch 3/30\n",
      " - 20s - loss: 0.3074 - acc: 0.8904\n",
      "Epoch 4/30\n",
      " - 21s - loss: 0.2982 - acc: 0.8895\n",
      "Epoch 5/30\n",
      " - 21s - loss: 0.2935 - acc: 0.8938\n",
      "Epoch 6/30\n",
      " - 20s - loss: 0.2887 - acc: 0.8910\n",
      "Epoch 7/30\n",
      " - 21s - loss: 0.2899 - acc: 0.8938\n",
      "Epoch 8/30\n",
      " - 21s - loss: 0.2824 - acc: 0.8929\n",
      "Epoch 9/30\n",
      " - 21s - loss: 0.2793 - acc: 0.8966\n",
      "Epoch 10/30\n",
      " - 21s - loss: 0.2823 - acc: 0.9000\n",
      "Epoch 11/30\n",
      " - 21s - loss: 0.2806 - acc: 0.8972\n",
      "Epoch 12/30\n",
      " - 21s - loss: 0.2748 - acc: 0.9009\n",
      "Epoch 13/30\n",
      " - 21s - loss: 0.2723 - acc: 0.8985\n",
      "Epoch 14/30\n",
      " - 21s - loss: 0.2731 - acc: 0.9000\n",
      "Epoch 15/30\n",
      " - 21s - loss: 0.2683 - acc: 0.9034\n",
      "Epoch 16/30\n",
      " - 20s - loss: 0.2660 - acc: 0.9031\n",
      "Epoch 17/30\n",
      " - 20s - loss: 0.2660 - acc: 0.9037\n",
      "Epoch 18/30\n",
      " - 21s - loss: 0.2584 - acc: 0.9074\n",
      "Epoch 19/30\n",
      " - 21s - loss: 0.2643 - acc: 0.9025\n",
      "Epoch 20/30\n",
      " - 21s - loss: 0.2643 - acc: 0.9102\n",
      "Epoch 21/30\n",
      " - 21s - loss: 0.2597 - acc: 0.9077\n",
      "Epoch 22/30\n",
      " - 21s - loss: 0.2546 - acc: 0.9099\n",
      "Epoch 23/30\n",
      " - 21s - loss: 0.2541 - acc: 0.9059\n",
      "Epoch 24/30\n",
      " - 21s - loss: 0.2550 - acc: 0.9090\n",
      "Epoch 25/30\n",
      " - 21s - loss: 0.2478 - acc: 0.9099\n",
      "Epoch 26/30\n",
      " - 21s - loss: 0.2521 - acc: 0.9127\n",
      "Epoch 27/30\n",
      " - 21s - loss: 0.2522 - acc: 0.9087\n",
      "Epoch 28/30\n",
      " - 21s - loss: 0.2470 - acc: 0.9121\n",
      "Epoch 29/30\n",
      " - 21s - loss: 0.2490 - acc: 0.9090\n",
      "Epoch 30/30\n",
      " - 21s - loss: 0.2463 - acc: 0.9093\n",
      "Learning rate: 0.1 Regularization: 0 Batch size: 32 Epoch: 30\n",
      "Accuracy = 0.884990253411306 \t AUC = 0.770405643738977\n",
      "----- Total training: 629.4630818367004 seconds -----\n"
     ]
    }
   ],
   "source": [
    "# GPU \n",
    "L = 1e-1\n",
    "R = 0\n",
    "B = 32\n",
    "E = 30\n",
    "start = time.time()\n",
    "\n",
    "lstm_create_train(labels=text_recognition_labels,\n",
    "                                  learning_rate=L,\n",
    "                                  lstm_dim=100, \n",
    "                                  batch_size=B,\n",
    "                                  num_epochs=E, \n",
    "                                  optimizer_param=SGD(lr=L, nesterov=True),\n",
    "                                  regularization=R)\n",
    "end = time.time()\n",
    "model = load_model('./LSTM/text/{}_{}_{}_{}_model.h5'.format(L,R,B,E))\n",
    "preds = model.predict(val_seq, verbose=0)\n",
    "print(\"Learning rate: {} Regularization: {} Batch size: {} Epoch: {}\".format(L,R,B,E))\n",
    "print((\"Accuracy = {0} \\t AUC = {1}\".format(accuracy_score(txt_val, preds.argmax(axis=1)), \n",
    "                                                            roc_auc_score(txt_val, preds[:,1]))))\n",
    "print(\"----- Total training: {} seconds -----\".format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      " - 25s - loss: 0.4166 - acc: 0.8703\n",
      "Epoch 2/30\n",
      " - 21s - loss: 0.3435 - acc: 0.8892\n",
      "Epoch 3/30\n",
      " - 21s - loss: 0.3371 - acc: 0.8892\n",
      "Epoch 4/30\n",
      " - 21s - loss: 0.3350 - acc: 0.8892\n",
      "Epoch 5/30\n",
      " - 21s - loss: 0.3348 - acc: 0.8892\n",
      "Epoch 6/30\n",
      " - 21s - loss: 0.3298 - acc: 0.8892\n",
      "Epoch 7/30\n",
      " - 21s - loss: 0.3309 - acc: 0.8892\n",
      "Epoch 8/30\n",
      " - 21s - loss: 0.3281 - acc: 0.8892\n",
      "Epoch 9/30\n",
      " - 21s - loss: 0.3237 - acc: 0.8892\n",
      "Epoch 10/30\n",
      " - 21s - loss: 0.3200 - acc: 0.8898\n",
      "Epoch 11/30\n",
      " - 21s - loss: 0.3226 - acc: 0.8895\n",
      "Epoch 12/30\n",
      " - 21s - loss: 0.3167 - acc: 0.8901\n",
      "Epoch 13/30\n",
      " - 21s - loss: 0.3181 - acc: 0.8904\n",
      "Epoch 14/30\n",
      " - 21s - loss: 0.3163 - acc: 0.8901\n",
      "Epoch 15/30\n",
      " - 21s - loss: 0.3130 - acc: 0.8895\n",
      "Epoch 16/30\n",
      " - 21s - loss: 0.3127 - acc: 0.8898\n",
      "Epoch 17/30\n",
      " - 21s - loss: 0.3116 - acc: 0.8901\n",
      "Epoch 18/30\n",
      " - 21s - loss: 0.3063 - acc: 0.8907\n",
      "Epoch 19/30\n",
      " - 21s - loss: 0.3059 - acc: 0.8889\n",
      "Epoch 20/30\n",
      " - 21s - loss: 0.3064 - acc: 0.8898\n",
      "Epoch 21/30\n",
      " - 21s - loss: 0.3034 - acc: 0.8913\n",
      "Epoch 22/30\n",
      " - 21s - loss: 0.3027 - acc: 0.8901\n",
      "Epoch 23/30\n",
      " - 21s - loss: 0.3038 - acc: 0.8913\n",
      "Epoch 24/30\n",
      " - 21s - loss: 0.2996 - acc: 0.8901\n",
      "Epoch 25/30\n",
      " - 21s - loss: 0.2963 - acc: 0.8929\n",
      "Epoch 26/30\n",
      " - 21s - loss: 0.2965 - acc: 0.8923\n",
      "Epoch 27/30\n",
      " - 21s - loss: 0.2969 - acc: 0.8923\n",
      "Epoch 28/30\n",
      " - 21s - loss: 0.2950 - acc: 0.8892\n",
      "Epoch 29/30\n",
      " - 21s - loss: 0.2933 - acc: 0.8923\n",
      "Epoch 30/30\n",
      " - 21s - loss: 0.2940 - acc: 0.8929\n",
      "Learning rate: 0.01 Regularization: 0 Batch size: 32 Epoch: 30\n",
      "Accuracy = 0.8791423001949318 \t AUC = 0.7391887125220459\n",
      "----- Total training: 631.8761060237885 seconds -----\n"
     ]
    }
   ],
   "source": [
    "# GPU \n",
    "L = 1e-2\n",
    "R = 0\n",
    "B = 32\n",
    "E = 30\n",
    "start = time.time()\n",
    "\n",
    "lstm_create_train(labels=text_recognition_labels,\n",
    "                                  learning_rate=L,\n",
    "                                  lstm_dim=100, \n",
    "                                  batch_size=B,\n",
    "                                  num_epochs=E, \n",
    "                                  optimizer_param=SGD(lr=L, nesterov=True),\n",
    "                                  regularization=R)\n",
    "end = time.time()\n",
    "model = load_model('./LSTM/text/{}_{}_{}_{}_model.h5'.format(L,R,B,E))\n",
    "preds = model.predict(val_seq, verbose=0)\n",
    "print(\"Learning rate: {} Regularization: {} Batch size: {} Epoch: {}\".format(L,R,B,E))\n",
    "print((\"Accuracy = {0} \\t AUC = {1}\".format(accuracy_score(txt_val, preds.argmax(axis=1)), \n",
    "                                                            roc_auc_score(txt_val, preds[:,1]))))\n",
    "print(\"----- Total training: {} seconds -----\".format(end-start))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
