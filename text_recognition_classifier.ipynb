{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py35/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/edithzeng/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/edithzeng/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/edithzeng/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 763477183725581923\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 11285289370\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 14559069804883178800\n",
      "physical_device_desc: \"device: 0, name: Tesla K80, pci bus id: 1d78:00:00.0, compute capability: 3.7\"\n",
      "]\n",
      "['/job:localhost/replica:0/task:0/device:GPU:0']\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "import tensorflow as tf\n",
    "from skill_label_classifier import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = SkillClassifier()\n",
    "experiment.import_data()\n",
    "experiment.create_df()\n",
    "\n",
    "experiment.choose_dataset('vizwiz')\n",
    "# experiment.set_features(['QSN', 'descriptions', 'tags', 'dominant_colors','handwritten_text', 'ocr_text'])\n",
    "experiment.set_features(['descriptions'])\n",
    "experiment.set_targets()\n",
    "\n",
    "features_train = experiment.features_train\n",
    "features_val   = experiment.features_val\n",
    "\n",
    "# check training class distribution\n",
    "text_recognition_y_train = to_categorical(np.asarray(experiment.txt_train)).astype('float32')\n",
    "color_recognition_y_train = to_categorical(np.asarray(experiment.col_train)).astype('float32')\n",
    "print('Number of training samples each class: ')\n",
    "print('Text recognition', text_recognition_y_train.sum(axis=0))\n",
    "print('Color recognition', color_recognition_y_train.sum(axis=0))\n",
    "\n",
    "n_classes = 2\n",
    "\n",
    "# check validation class distribution\n",
    "text_recognition_y_val = to_categorical(np.asarray(experiment.txt_val)).astype('float32')\n",
    "color_recognition_y_val = to_categorical(np.asarray(experiment.col_val)).astype('float32')\n",
    "print('Number of validation samples each class: ')\n",
    "print('Text recognition', text_recognition_y_val.sum(axis=0))\n",
    "print('Color recognition', color_recognition_y_val.sum(axis=0))\n",
    "\n",
    "# get targets\n",
    "txt_train      = experiment.txt_train\n",
    "col_train      = experiment.col_train\n",
    "txt_val        = experiment.txt_val\n",
    "col_val        = experiment.col_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-09 16:41:57,269 : INFO : collecting all words and their counts\n",
      "2019-03-09 16:41:57,269 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-03-09 16:41:57,287 : INFO : PROGRESS: at sentence #10000, processed 95361 words, keeping 10420 word types\n",
      "2019-03-09 16:41:57,291 : INFO : collected 12648 word types from a corpus of 116936 raw words and 12259 sentences\n",
      "2019-03-09 16:41:57,292 : INFO : Loading a fresh vocabulary\n",
      "2019-03-09 16:41:57,297 : INFO : effective_min_count=6 retains 302 unique words (2% of original 12648, drops 12346)\n",
      "2019-03-09 16:41:57,298 : INFO : effective_min_count=6 leaves 104035 word corpus (88% of original 116936, drops 12901)\n",
      "2019-03-09 16:41:57,299 : INFO : deleting the raw counts dictionary of 12648 items\n",
      "2019-03-09 16:41:57,300 : INFO : sample=0.001 downsamples 39 most-common words\n",
      "2019-03-09 16:41:57,300 : INFO : downsampling leaves estimated 32650 word corpus (31.4% of prior 104035)\n",
      "2019-03-09 16:41:57,301 : INFO : estimated required memory for 302 words and 100 dimensions: 392600 bytes\n",
      "2019-03-09 16:41:57,302 : INFO : resetting layer weights\n",
      "2019-03-09 16:41:57,311 : INFO : training model with 6 workers on 302 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-03-09 16:41:57,345 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-03-09 16:41:57,353 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-03-09 16:41:57,354 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-09 16:41:57,355 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-09 16:41:57,357 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-09 16:41:57,358 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-09 16:41:57,358 : INFO : EPOCH - 1 : training on 116936 raw words (32644 effective words) took 0.0s, 826664 effective words/s\n",
      "2019-03-09 16:41:57,393 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-03-09 16:41:57,395 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-03-09 16:41:57,398 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-09 16:41:57,399 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-09 16:41:57,399 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-09 16:41:57,400 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-09 16:41:57,401 : INFO : EPOCH - 2 : training on 116936 raw words (32501 effective words) took 0.0s, 1027697 effective words/s\n",
      "2019-03-09 16:41:57,433 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-03-09 16:41:57,438 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-03-09 16:41:57,440 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-09 16:41:57,443 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-09 16:41:57,444 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-09 16:41:57,446 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-09 16:41:57,446 : INFO : EPOCH - 3 : training on 116936 raw words (32599 effective words) took 0.0s, 893780 effective words/s\n",
      "2019-03-09 16:41:57,480 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-03-09 16:41:57,482 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-03-09 16:41:57,484 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-09 16:41:57,488 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-09 16:41:57,488 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-09 16:41:57,489 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-09 16:41:57,489 : INFO : EPOCH - 4 : training on 116936 raw words (32768 effective words) took 0.0s, 939062 effective words/s\n",
      "2019-03-09 16:41:57,525 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-03-09 16:41:57,528 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-03-09 16:41:57,529 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-09 16:41:57,530 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-09 16:41:57,530 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-09 16:41:57,531 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-09 16:41:57,531 : INFO : EPOCH - 5 : training on 116936 raw words (32808 effective words) took 0.0s, 985622 effective words/s\n",
      "2019-03-09 16:41:57,532 : INFO : training on a 584680 raw words (163320 effective words) took 0.2s, 740723 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 302 word vectors\n"
     ]
    }
   ],
   "source": [
    "# tokenize\n",
    "tok        = Tokenizer(num_words=VOCAB_SIZE, \n",
    "                       filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "                       lower=True,\n",
    "                       split=\" \")\n",
    "tok.fit_on_texts(features_train)\n",
    "\n",
    "# create sequences & pad\n",
    "train_seq  = tok.texts_to_sequences(features_train)\n",
    "train_seq  = sequence.pad_sequences(train_seq, maxlen=MAX_DOC_LEN)\n",
    "val_seq    = tok.texts_to_sequences(features_val)\n",
    "val_seq    = sequence.pad_sequences(val_seq, maxlen=MAX_DOC_LEN)\n",
    "\n",
    "# punkt sentence level tokenizer\n",
    "sent_lst = [] \n",
    "for doc in features_train:\n",
    "    sentences = nltk.tokenize.sent_tokenize(doc)\n",
    "    for sent in sentences:\n",
    "        word_lst = [w for w in nltk.tokenize.word_tokenize(sent) if w.isalnum()]\n",
    "        sent_lst.append(word_lst)\n",
    "\n",
    "# word2vec\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "word2vec_model = gensim.models.Word2Vec(sentences=sent_lst,\n",
    "                                        min_count=6,\n",
    "                                        size=EMBEDDING_DIM,\n",
    "                                        sg=1,\n",
    "                                        workers=os.cpu_count())\n",
    "\n",
    "embeddings_index = {}\n",
    "for word in word2vec_model.wv.vocab:\n",
    "    coefs = np.asarray(word2vec_model.wv[word], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "print('Total %s word vectors' % len(embeddings_index))\n",
    "\n",
    "# Initial word embedding\n",
    "embedding_matrix = np.zeros((VOCAB_SIZE, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in tok.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None and i < VOCAB_SIZE:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.01 Regularization: 0 Batch size: 32 Epoch: 30\n",
      "Accuracy = 0.7578994214508233 \t AUC = 0.8303036876355747\n",
      "----- Total training: 1442.251312971115 seconds -----\n"
     ]
    }
   ],
   "source": [
    "L = 1e-2\n",
    "R = 0\n",
    "B = 32\n",
    "E = 30\n",
    "val_data = (val_seq, text_recognition_y_val)\n",
    "start = time.time()\n",
    "result = lstm_create_train(train_seq, embedding_matrix,\n",
    "                 train_labels=text_recognition_y_train, skill='text',\n",
    "                 val_data=val_data,\n",
    "                 learning_rate=L,\n",
    "                 lstm_dim=100,\n",
    "                 batch_size=B,\n",
    "                 num_epochs=E,\n",
    "                 optimizer_param=SGD(lr=L, nesterov=True),\n",
    "                 regularization=R)\n",
    "end = time.time()\n",
    "model = load_model('./LSTM/text/{}_{}_{}_{}_model.h5'.format(L,R,B,E))\n",
    "preds = model.predict(val_seq, verbose=0)\n",
    "print(\"Learning rate: {} Regularization: {} Batch size: {} Epoch: {}\".format(L,R,B,E))\n",
    "print((\"Accuracy = {0} \\t AUC = {1}\".format(accuracy_score(txt_val, preds.argmax(axis=1)), \n",
    "                                                            roc_auc_score(txt_val, preds[:,1]))))\n",
    "print(\"----- Total training: {} seconds -----\".format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_acc', 'val_loss', 'loss', 'acc'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training accuracy\n",
    "result.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.680017801539655,\n",
       " 0.7138406764840252,\n",
       " 0.7254116600441519,\n",
       " 0.7418780596881216,\n",
       " 0.7441032488291986,\n",
       " 0.7449933244856294,\n",
       " 0.7472185136267064,\n",
       " 0.7458834001420602,\n",
       " 0.7476635514283955,\n",
       " 0.7507788162524296,\n",
       " 0.7525589675387648,\n",
       " 0.7521139297370758,\n",
       " 0.7561192701910143,\n",
       " 0.7552291945345836,\n",
       " 0.7565643080192298,\n",
       " 0.7574543836756605,\n",
       " 0.7570093458474452,\n",
       " 0.7561192701644881,\n",
       " 0.7583444593320914,\n",
       " 0.7587894971603067,\n",
       " 0.7592345349885221,\n",
       " 0.7596795728167376,\n",
       " 0.7565643079927035,\n",
       " 0.7592345349885221,\n",
       " 0.7605696484731683,\n",
       " 0.7601246106184266,\n",
       " 0.7623497997860299,\n",
       " 0.7592345349619959,\n",
       " 0.7485536271378789,\n",
       " 0.7659101024117532]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 30 epoch\n",
    "training_accuracy   = result.history['acc']\n",
    "validation_accuracy = result.history['val_acc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 1e-2\n",
    "R = 0\n",
    "B = 32\n",
    "E = 1000\n",
    "result = lstm_create_train(train_seq, embedding_matrix,\n",
    "                 train_labels=text_recognition_y_train, skill='text',\n",
    "                 val_data=val_data,\n",
    "                 learning_rate=L,\n",
    "                 lstm_dim=100,\n",
    "                 batch_size=B,\n",
    "                 num_epochs=E,\n",
    "                 optimizer_param=SGD(lr=L, nesterov=True),\n",
    "                 regularization=R)\n",
    "model = load_model('./LSTM/text/{}_{}_{}_{}_model.h5'.format(L,R,B,E))\n",
    "preds = model.predict(val_seq, verbose=0)\n",
    "print(\"Learning rate: {} Regularization: {} Batch size: {} Epoch: {}\".format(L,R,B,E))\n",
    "print((\"Accuracy = {0} \\t AUC = {1}\".format(accuracy_score(txt_val, preds.argmax(axis=1)), \n",
    "                                                            roc_auc_score(txt_val, preds[:,1]))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
