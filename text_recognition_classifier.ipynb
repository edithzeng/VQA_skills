{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "import tensorflow as tf\n",
    "from skill_label_classifier import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = SkillClassifier()\n",
    "experiment.import_data()\n",
    "experiment.create_df()\n",
    "\n",
    "experiment.choose_dataset('vizwiz')\n",
    "# experiment.set_features(['QSN', 'descriptions', 'tags', 'dominant_colors','handwritten_text', 'ocr_text'])\n",
    "experiment.set_features(['descriptions', 'tags', 'dominant_colors', 'handwritten_text', 'ocr_text'])\n",
    "experiment.set_targets()\n",
    "\n",
    "features_train = experiment.features_train\n",
    "features_val   = experiment.features_val\n",
    "\n",
    "# check class distribution\n",
    "text_recognition_labels = to_categorical(np.asarray(experiment.txt_train)).astype('float32')\n",
    "color_recognition_labels = to_categorical(np.asarray(experiment.col_train)).astype('float32')\n",
    "print('Number of samples each class: ')\n",
    "print('Text recognition', text_recognition_labels.sum(axis=0))\n",
    "print('Color recognition', color_recognition_labels.sum(axis=0))\n",
    "n_classes = 2\n",
    "\n",
    "# get targets\n",
    "txt_train      = experiment.txt_train\n",
    "col_train      = experiment.col_train\n",
    "txt_val        = experiment.txt_val\n",
    "col_val        = experiment.col_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-07 00:43:30,214 : INFO : collecting all words and their counts\n",
      "2019-03-07 00:43:30,215 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-03-07 00:43:30,274 : INFO : PROGRESS: at sentence #10000, processed 346668 words, keeping 32892 word types\n",
      "2019-03-07 00:43:30,299 : INFO : collected 43239 word types from a corpus of 489472 raw words and 14257 sentences\n",
      "2019-03-07 00:43:30,299 : INFO : Loading a fresh vocabulary\n",
      "2019-03-07 00:43:30,324 : INFO : effective_min_count=6 retains 3312 unique words (7% of original 43239, drops 39927)\n",
      "2019-03-07 00:43:30,324 : INFO : effective_min_count=6 leaves 437841 word corpus (89% of original 489472, drops 51631)\n",
      "2019-03-07 00:43:30,334 : INFO : deleting the raw counts dictionary of 43239 items\n",
      "2019-03-07 00:43:30,335 : INFO : sample=0.001 downsamples 63 most-common words\n",
      "2019-03-07 00:43:30,336 : INFO : downsampling leaves estimated 265959 word corpus (60.7% of prior 437841)\n",
      "2019-03-07 00:43:30,343 : INFO : estimated required memory for 3312 words and 100 dimensions: 4305600 bytes\n",
      "2019-03-07 00:43:30,344 : INFO : resetting layer weights\n",
      "2019-03-07 00:43:30,390 : INFO : training model with 6 workers on 3312 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-03-07 00:43:30,700 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-03-07 00:43:30,702 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-03-07 00:43:30,707 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-07 00:43:30,711 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-07 00:43:30,716 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-07 00:43:30,717 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-07 00:43:30,717 : INFO : EPOCH - 1 : training on 489472 raw words (266095 effective words) took 0.3s, 830121 effective words/s\n",
      "2019-03-07 00:43:31,038 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-03-07 00:43:31,042 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-03-07 00:43:31,049 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-07 00:43:31,050 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-07 00:43:31,061 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-07 00:43:31,067 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-07 00:43:31,068 : INFO : EPOCH - 2 : training on 489472 raw words (265740 effective words) took 0.3s, 773355 effective words/s\n",
      "2019-03-07 00:43:31,379 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-03-07 00:43:31,382 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-03-07 00:43:31,385 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-07 00:43:31,397 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-07 00:43:31,401 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-07 00:43:31,403 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-07 00:43:31,404 : INFO : EPOCH - 3 : training on 489472 raw words (266213 effective words) took 0.3s, 806542 effective words/s\n",
      "2019-03-07 00:43:31,717 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-03-07 00:43:31,719 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-03-07 00:43:31,720 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-07 00:43:31,726 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-07 00:43:31,729 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-07 00:43:31,731 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-07 00:43:31,732 : INFO : EPOCH - 4 : training on 489472 raw words (266164 effective words) took 0.3s, 828704 effective words/s\n",
      "2019-03-07 00:43:32,047 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-03-07 00:43:32,048 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-03-07 00:43:32,053 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-07 00:43:32,056 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-07 00:43:32,061 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-07 00:43:32,062 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-07 00:43:32,062 : INFO : EPOCH - 5 : training on 489472 raw words (265673 effective words) took 0.3s, 818928 effective words/s\n",
      "2019-03-07 00:43:32,063 : INFO : training on a 2447360 raw words (1329885 effective words) took 1.7s, 795399 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 3312 word vectors\n"
     ]
    }
   ],
   "source": [
    "# tokenize\n",
    "tok        = Tokenizer(num_words=VOCAB_SIZE, \n",
    "                       filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "                       lower=True,\n",
    "                       split=\" \")\n",
    "tok.fit_on_texts(features_train)\n",
    "\n",
    "# create sequences & pad\n",
    "train_seq  = tok.texts_to_sequences(features_train)\n",
    "train_seq  = sequence.pad_sequences(train_seq, maxlen=MAX_DOC_LEN)\n",
    "val_seq    = tok.texts_to_sequences(features_val)\n",
    "val_seq    = sequence.pad_sequences(val_seq, maxlen=MAX_DOC_LEN)\n",
    "\n",
    "# punkt sentence level tokenizer\n",
    "sent_lst = [] \n",
    "for doc in features_train:\n",
    "    sentences = nltk.tokenize.sent_tokenize(doc)\n",
    "    for sent in sentences:\n",
    "        word_lst = [w for w in nltk.tokenize.word_tokenize(sent) if w.isalnum()]\n",
    "        sent_lst.append(word_lst)\n",
    "\n",
    "# word2vec\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "word2vec_model = gensim.models.Word2Vec(sentences=sent_lst,\n",
    "                                        min_count=6,\n",
    "                                        size=EMBEDDING_DIM,\n",
    "                                        sg=1,\n",
    "                                        workers=os.cpu_count())\n",
    "\n",
    "embeddings_index = {}\n",
    "for word in word2vec_model.wv.vocab:\n",
    "    coefs = np.asarray(word2vec_model.wv[word], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "print('Total %s word vectors' % len(embeddings_index))\n",
    "\n",
    "# Initial word embedding\n",
    "embedding_matrix = np.zeros((VOCAB_SIZE, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in tok.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None and i < VOCAB_SIZE:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      " - 46s - loss: 0.6098 - acc: 0.6770\n",
      "Epoch 2/30\n",
      " - 44s - loss: 0.5028 - acc: 0.7667\n",
      "Epoch 3/30\n",
      " - 43s - loss: 0.4723 - acc: 0.7823\n",
      "Epoch 4/30\n",
      " - 43s - loss: 0.4584 - acc: 0.7933\n",
      "Epoch 5/30\n",
      " - 43s - loss: 0.4517 - acc: 0.7956\n",
      "Epoch 6/30\n",
      " - 43s - loss: 0.4457 - acc: 0.7978\n",
      "Epoch 7/30\n",
      " - 44s - loss: 0.4401 - acc: 0.8052\n",
      "Epoch 8/30\n",
      " - 44s - loss: 0.4343 - acc: 0.8063\n",
      "Epoch 9/30\n",
      " - 43s - loss: 0.4338 - acc: 0.8074\n",
      "Epoch 10/30\n",
      " - 43s - loss: 0.4305 - acc: 0.8120\n",
      "Epoch 11/30\n",
      " - 43s - loss: 0.4278 - acc: 0.8093\n",
      "Epoch 12/30\n",
      " - 43s - loss: 0.4250 - acc: 0.8152\n",
      "Epoch 13/30\n",
      " - 44s - loss: 0.4227 - acc: 0.8153\n",
      "Epoch 14/30\n",
      " - 43s - loss: 0.4191 - acc: 0.8180\n",
      "Epoch 15/30\n",
      " - 43s - loss: 0.4189 - acc: 0.8171\n",
      "Epoch 16/30\n",
      " - 43s - loss: 0.4179 - acc: 0.8167\n",
      "Epoch 17/30\n",
      " - 43s - loss: 0.4154 - acc: 0.8188\n",
      "Epoch 18/30\n",
      " - 43s - loss: 0.4112 - acc: 0.8236\n",
      "Epoch 19/30\n",
      " - 44s - loss: 0.4098 - acc: 0.8241\n",
      "Epoch 20/30\n",
      " - 44s - loss: 0.4093 - acc: 0.8207\n",
      "Epoch 21/30\n"
     ]
    }
   ],
   "source": [
    "L = 1e-2\n",
    "R = 0\n",
    "B = 32\n",
    "E = 30\n",
    "start = time.time()\n",
    "lstm_create_train(train_seq, embedding_matrix,\n",
    "                 labels=text_recognition_labels, skill='text',\n",
    "                 learning_rate=L,\n",
    "                 lstm_dim=100,\n",
    "                 batch_size=B,\n",
    "                 num_epochs=E,\n",
    "                 optimizer_param=SGD(lr=L, nesterov=True),\n",
    "                  regularization=R)\n",
    "end = time.time()\n",
    "model = load_model('./LSTM/text/{}_{}_{}_{}_model.h5'.format(L,R,B,E))\n",
    "preds = model.predict(val_seq, verbose=0)\n",
    "print(\"Learning rate: {} Regularization: {} Batch size: {} Epoch: {}\".format(L,R,B,E))\n",
    "print((\"Accuracy = {0} \\t AUC = {1}\".format(accuracy_score(txt_val, preds.argmax(axis=1)), \n",
    "                                                            roc_auc_score(txt_val, preds[:,1]))))\n",
    "print(\"----- Total training: {} seconds -----\".format(end-start))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
