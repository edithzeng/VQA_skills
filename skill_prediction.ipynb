{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skill label prediction with image-based features (text and color with descriptive tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/edithzeng/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import random\n",
    "import numpy as np\n",
    "import copy\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "kfold=KFold(n_splits=10)\n",
    "\n",
    "# for LSTM (keras with tf backend)\n",
    "import gzip\n",
    "import os\n",
    "import pickle\n",
    "import requests\n",
    "import time\n",
    "import re\n",
    "os.environ['KERAS_BACKEND']='cntk'\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.models import Sequential, load_model\n",
    "from keras import regularizers\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional\n",
    "from keras.callbacks import History, CSVLogger\n",
    "from keras.utils import to_categorical\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "MAX_DOC_LEN = 40\n",
    "VOCAB_SIZE = 3000\n",
    "EMBEDDING_DIM = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vizwiz_features_train_color = pd.read_csv('azure_features_images/data/vizwiz_train_color_recognition.csv',\n",
    "                                    delimiter=';', engine='python', \n",
    "                                    dtype={'qid':str, 'question':str, 'descriptions':list,\n",
    "                                          'tags':list, 'dominant_colors':list},\n",
    "                                    quotechar='\"', error_bad_lines=False)\n",
    "vizwiz_features_train_text = pd.read_csv('azure_features_images/data/vizwiz_train_text_recognition.csv',\n",
    "                                    delimiter=';', engine='python', \n",
    "                                    dtype={'qid':str, 'question':str, 'descriptions':list,\n",
    "                                          'ocr_text':list, 'handwritten_text':list},\n",
    "                                    quotechar='\"', error_bad_lines=False)\n",
    "\n",
    "\n",
    "vizwiz_features_val_color = pd.read_csv('azure_features_images/data/vizwiz_val_color_recognition.csv',\n",
    "                                  delimiter=';', engine='python',\n",
    "                                  dtype={'qid':str, 'question':str, 'descriptions':list,\n",
    "                                          'tags':list, 'dominant_colors':list},\n",
    "                                  quotechar='\"', error_bad_lines=False)\n",
    "vizwiz_features_val_text = pd.read_csv('azure_features_images/data/vizwiz_val_text_recognition.csv',\n",
    "                                    delimiter=';', engine='python', \n",
    "                                    dtype={'qid':str, 'question':str, 'descriptions':list,\n",
    "                                          'ocr_text':list, 'handwritten_text':list},\n",
    "                                    quotechar='\"', error_bad_lines=False)\n",
    "\n",
    "\n",
    "vqa_features_train_color = pd.read_csv('azure_features_images/data/vqa_train_color_recognition.csv',\n",
    "                                 delimiter=';', engine='python', \n",
    "                                 dtype={'qid':str, 'question':str, 'descriptions':list,\n",
    "                                          'tags':list, 'dominant_colors':list},\n",
    "                                 quotechar='\"', error_bad_lines=False)\n",
    "vqa_features_train_text = pd.read_csv('azure_features_images/data/vqa_train_text_recognition.csv',\n",
    "                                    delimiter=';', engine='python', \n",
    "                                    dtype={'qid':str, 'question':str, 'descriptions':list,\n",
    "                                          'ocr_text':list, 'handwritten_text':list},\n",
    "                                    quotechar='\"', error_bad_lines=False)\n",
    "vqa_features_val_color = pd.read_csv('azure_features_images/data/vqa_val_color_recognition.csv',\n",
    "                               delimiter=';', engine='python',\n",
    "                               dtype={'qid':str, 'question':str, 'descriptions':list,\n",
    "                                          'tags':list, 'dominant_colors':list},\n",
    "                               quotechar='\"', error_bad_lines=False)\n",
    "vqa_features_val_text = pd.read_csv('azure_features_images/data/vqa_val_text_recognition.csv',\n",
    "                                    delimiter=';', engine='python', \n",
    "                                    dtype={'qid':str, 'question':str, 'descriptions':list,\n",
    "                                          'ocr_text':list, 'handwritten_text':list},\n",
    "                                    quotechar='\"', error_bad_lines=False)\n",
    "\n",
    "\n",
    "vizwiz_targets_train = pd.read_csv('../vizwiz_skill_typ_train.csv',\n",
    "                                   delimiter=',', quotechar='\"',\n",
    "                                   engine='python', error_bad_lines=False)\n",
    "vizwiz_targets_val = pd.read_csv('../vizwiz_skill_typ_val.csv',\n",
    "                                 delimiter=',', quotechar='\"', engine='python', error_bad_lines=False)\n",
    "vqa_targets_train = pd.read_csv('../vqa_skill_typ_train.csv',\n",
    "                               engine='python', quotechar='\"', error_bad_lines=False)\n",
    "vqa_targets_val = pd.read_csv('../vqa_skill_typ_val.csv',\n",
    "                               engine='python', quotechar='\"', error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>question</th>\n",
       "      <th>ocr_text</th>\n",
       "      <th>handwritten_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>VizWiz_train_000000000000.jpg</td>\n",
       "      <td>What's the name of this product?</td>\n",
       "      <td>['b', 'sil', 'leaves', '0.62', 'oz', '(170)']</td>\n",
       "      <td>['NET WT O. 62 02 ( 179)']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>VizWiz_train_000000000001.jpg</td>\n",
       "      <td>Can you tell me what is in this can please?</td>\n",
       "      <td>[]</td>\n",
       "      <td>['^TAKE Three 1^']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             qid                                     question  \\\n",
       "0  VizWiz_train_000000000000.jpg             What's the name of this product?   \n",
       "1  VizWiz_train_000000000001.jpg  Can you tell me what is in this can please?   \n",
       "\n",
       "                                        ocr_text            handwritten_text  \n",
       "0  ['b', 'sil', 'leaves', '0.62', 'oz', '(170)']  ['NET WT O. 62 02 ( 179)']  \n",
       "1                                             []          ['^TAKE Three 1^']  "
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vizwiz_features_train_text.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IMG</th>\n",
       "      <th>QSN</th>\n",
       "      <th>TXT</th>\n",
       "      <th>OBJ</th>\n",
       "      <th>COL</th>\n",
       "      <th>CNT</th>\n",
       "      <th>OTH</th>\n",
       "      <th>question</th>\n",
       "      <th>ocr_text</th>\n",
       "      <th>handwritten_text</th>\n",
       "      <th>descriptions</th>\n",
       "      <th>tags</th>\n",
       "      <th>dominant_colors</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>VizWiz_train_000000009759.jpg</th>\n",
       "      <td>VizWiz_train_000000009759.jpg</td>\n",
       "      <td>what's this?</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>what's this?</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{'text': 'a close up of a computer', 'confide...</td>\n",
       "      <td>['sitting', 'man', 'computer', 'black', 'stand...</td>\n",
       "      <td>['Grey', 'White']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                         IMG           QSN  \\\n",
       "QID                                                                          \n",
       "VizWiz_train_000000009759.jpg  VizWiz_train_000000009759.jpg  what's this?   \n",
       "\n",
       "                              TXT OBJ COL CNT OTH      question ocr_text  \\\n",
       "QID                                                                        \n",
       "VizWiz_train_000000009759.jpg   0   1   0   0   0  what's this?       []   \n",
       "\n",
       "                              handwritten_text  \\\n",
       "QID                                              \n",
       "VizWiz_train_000000009759.jpg               []   \n",
       "\n",
       "                                                                    descriptions  \\\n",
       "QID                                                                                \n",
       "VizWiz_train_000000009759.jpg  [{'text': 'a close up of a computer', 'confide...   \n",
       "\n",
       "                                                                            tags  \\\n",
       "QID                                                                                \n",
       "VizWiz_train_000000009759.jpg  ['sitting', 'man', 'computer', 'black', 'stand...   \n",
       "\n",
       "                                 dominant_colors  \n",
       "QID                                               \n",
       "VizWiz_train_000000009759.jpg  ['Grey', 'White']  "
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vizwiz_train.sample(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_feature_target(feature_df_text, feature_df_color, target_df):\n",
    "    feature_text = copy.deepcopy(feature_df_text)\n",
    "    feature_color = copy.deepcopy(feature_df_color)\n",
    "    target = copy.deepcopy(target_df)\n",
    "    # text features \n",
    "    feature_text.rename({'qid': 'QID'}, axis=1, inplace=True)\n",
    "    feature_text.set_index('QID', inplace=True)\n",
    "    # color features\n",
    "    feature_color.rename({'qid': 'QID'}, axis=1, inplace=True)\n",
    "    feature_color.set_index('QID', inplace=True)\n",
    "    # join features\n",
    "    features = feature_text.join(feature_color[['descriptions','tags','dominant_colors']],\n",
    "                                 on='QID',\n",
    "                                 how='outer')\n",
    "    # join features with target\n",
    "    target = target[['QID', 'IMG', 'QSN', 'TXT', 'OBJ', 'COL', 'CNT', 'OTH']]\n",
    "    target.set_index('QID', inplace=True)\n",
    "    target = target.astype(dtype=str)\n",
    "    df = target.join(features, on='QID', how='inner')\n",
    "    df['descriptions'].astype(list)\n",
    "    return df\n",
    "\n",
    "def lem(s):\n",
    "    arr = s.split(\" \")\n",
    "    lem = WordNetLemmatizer()\n",
    "    op = \"\"\n",
    "    for w in arr:\n",
    "        word = lem.lemmatize(w) + ' '\n",
    "        op += word\n",
    "    return op\n",
    "\n",
    "def preprocess_text(feature_columns):\n",
    "    \"\"\" output an nparray with single document per data point \"\"\"\n",
    "    ip = copy.deepcopy(feature_columns).values\n",
    "    op = []\n",
    "    for i in range(ip.shape[0]):\n",
    "        doc      =  \"\"\n",
    "        for j in range(ip.shape[1]):\n",
    "            # clean up chars\n",
    "            s    =  str(ip[i][j])\n",
    "            s    =  s.translate({ord(c): \"\" for c in \"!@#$%^&*()[]{};:,./<>?\\|`~-=_+'\"}).lower() + \" \"\n",
    "            if j == 1:             # clean descriptions\n",
    "                s = re.sub(r'confidence\\s+\\d+', '', s)\n",
    "                s = re.sub(r'text', '', s)\n",
    "            # lexicon normalize\n",
    "            s    = lem(s)\n",
    "            doc  += s\n",
    "        op.append(doc)\n",
    "    op = np.asarray(op)\n",
    "    return op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vizwiz_train   = join_feature_target(vizwiz_features_train_text, \n",
    "                                   vizwiz_features_train_color, \n",
    "                                   vizwiz_targets_train)\n",
    "vizwiz_val     = join_feature_target(vizwiz_features_val_text, \n",
    "                                 vizwiz_features_val_color, \n",
    "                                 vizwiz_targets_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create X and Y\n",
    "features_train = preprocess_text(vizwiz_train[['QSN', \n",
    "                                              'descriptions', 'tags', 'dominant_colors', \n",
    "                                              'handwritten_text', 'ocr_text']])\n",
    "txt_train      = vizwiz_train[\"TXT\"].values\n",
    "obj_train      = vizwiz_train[\"OBJ\"].values\n",
    "col_train      = vizwiz_train[\"COL\"].values\n",
    "cnt_train      = vizwiz_train[\"CNT\"].values\n",
    "\n",
    "features_val   = preprocess_text(vizwiz_val[['QSN', \n",
    "                                             'descriptions', 'tags', 'dominant_colors',\n",
    "                                             'handwritten_text', 'ocr_text']])\n",
    "txt_val        = vizwiz_val[\"TXT\"].values\n",
    "obj_val        = vizwiz_val[\"OBJ\"].values\n",
    "col_val        = vizwiz_val[\"COL\"].values\n",
    "cnt_val        = vizwiz_val[\"CNT\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'is it possible to tell the ingredient of this pizza and the brand  thanks   a pizza sitting on top of a table   indoor table sitting food pizza plate red phone eating cat laying  brown red  stone tombsto original    '"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_train[random.randint(0,len(features_val))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'what is this medicine text a close up of a bottle  indoor sitting bottle green table small food black counter white lotion fruit laying phone brush grey active ingredient  new cetirizine hot 10 ndc 30142  458  95 uses temporary relievesto  runny nose  sneezing 1 warnings do not use in 24 hour ingredients or to an artnozz have liver or kidney deese wey all day doctor or pharmacistbe this product \" drowsiness allergy tranquilizers may not machinery stop use and medical help right away  cetirizine hydrochloride tablets  recommended  i prays 10 ing  antihistamine children  in case downdue directions adults and 45 tablets active i gredie•ltp\" hoc 3014245895 24 hour allday allergy ceirizåne hydrochloride tablets 10 mgantihistamine 45 tablets '"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_val[random.randint(0,len(features_val))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tokenize\n",
    "tok        = Tokenizer(num_words=VOCAB_SIZE, \n",
    "                       filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "                       lower=True,\n",
    "                       split=\" \")\n",
    "tok.fit_on_texts(features_train)\n",
    "\n",
    "# create sequences & pad\n",
    "train_seq  = tok.texts_to_sequences(features_train)\n",
    "train_seq  = sequence.pad_sequences(train_seq, maxlen=MAX_DOC_LEN)\n",
    "val_seq    = tok.texts_to_sequences(features_val)\n",
    "val_seq    = sequence.pad_sequences(val_seq, maxlen=MAX_DOC_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples each class - Vizwiz - train\n",
      "Text recognition [6247. 8010.]\n",
      "Color recognition [8844. 5413.]\n"
     ]
    }
   ],
   "source": [
    "# check class distribution\n",
    "text_recognition_labels = to_categorical(np.asarray(txt_train)).astype('float32')\n",
    "color_recognition_labels = to_categorical(np.asarray(col_train)).astype('float32')\n",
    "print('Number of samples each class - Vizwiz - train')\n",
    "print('Text recognition', text_recognition_labels.sum(axis=0))\n",
    "print('Color recognition', color_recognition_labels.sum(axis=0))\n",
    "n_classes = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM model (skip-gram word2vec)\n",
    "config for cpu only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import requests\n",
    "import time\n",
    "import gzip\n",
    "\n",
    "os.environ['KERAS_BACKEND'] = 'cntk'\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.models import Sequential, load_model\n",
    "from keras import regularizers\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional\n",
    "from keras.callbacks import History, CSVLogger\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import nltk \n",
    "import gensim\n",
    "import logging\n",
    "\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve\n",
    "\n",
    "MAX_DOC_LEN = 40\n",
    "VOCAB_SIZE = 3000\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "#MAX_DOC_LEN = 40\n",
    "#VOCAB_SIZE = 7000\n",
    "#EMBEDDING_DIM = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# punkt sentence level tokenizer\n",
    "sent_lst = []\n",
    "for doc in features_train:\n",
    "    sentences = nltk.tokenize.sent_tokenize(doc)\n",
    "    for sent in sentences:\n",
    "        word_lst = [w for w in nltk.tokenize.word_tokenize(sent) if w.isalnum()]\n",
    "        sent_lst.append(word_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-02-26 22:45:30,009 : INFO : collecting all words and their counts\n",
      "2019-02-26 22:45:30,009 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-02-26 22:45:30,072 : INFO : PROGRESS: at sentence #10000, processed 383294 words, keeping 24880 word types\n",
      "2019-02-26 22:45:30,099 : INFO : collected 31762 word types from a corpus of 541792 raw words and 14257 sentences\n",
      "2019-02-26 22:45:30,100 : INFO : Loading a fresh vocabulary\n",
      "2019-02-26 22:45:30,126 : INFO : effective_min_count=6 retains 3565 unique words (11% of original 31762, drops 28197)\n",
      "2019-02-26 22:45:30,126 : INFO : effective_min_count=6 leaves 501856 word corpus (92% of original 541792, drops 39936)\n",
      "2019-02-26 22:45:30,135 : INFO : deleting the raw counts dictionary of 31762 items\n",
      "2019-02-26 22:45:30,136 : INFO : sample=0.001 downsamples 67 most-common words\n",
      "2019-02-26 22:45:30,136 : INFO : downsampling leaves estimated 318802 word corpus (63.5% of prior 501856)\n",
      "2019-02-26 22:45:30,143 : INFO : estimated required memory for 3565 words and 100 dimensions: 4634500 bytes\n",
      "2019-02-26 22:45:30,144 : INFO : resetting layer weights\n",
      "2019-02-26 22:45:30,200 : INFO : training model with 6 workers on 3565 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-02-26 22:45:30,577 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-02-26 22:45:30,579 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-02-26 22:45:30,586 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-02-26 22:45:30,590 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-02-26 22:45:30,592 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-02-26 22:45:30,595 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-02-26 22:45:30,595 : INFO : EPOCH - 1 : training on 541792 raw words (318977 effective words) took 0.4s, 818158 effective words/s\n",
      "2019-02-26 22:45:30,965 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-02-26 22:45:30,977 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-02-26 22:45:30,984 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-02-26 22:45:30,988 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-02-26 22:45:30,991 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-02-26 22:45:30,994 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-02-26 22:45:30,995 : INFO : EPOCH - 2 : training on 541792 raw words (319339 effective words) took 0.4s, 812179 effective words/s\n",
      "2019-02-26 22:45:31,360 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-02-26 22:45:31,369 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-02-26 22:45:31,371 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-02-26 22:45:31,372 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-02-26 22:45:31,378 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-02-26 22:45:31,385 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-02-26 22:45:31,386 : INFO : EPOCH - 3 : training on 541792 raw words (318669 effective words) took 0.4s, 826956 effective words/s\n",
      "2019-02-26 22:45:31,788 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-02-26 22:45:31,792 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-02-26 22:45:31,794 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-02-26 22:45:31,795 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-02-26 22:45:31,811 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-02-26 22:45:31,811 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-02-26 22:45:31,812 : INFO : EPOCH - 4 : training on 541792 raw words (318709 effective words) took 0.4s, 758747 effective words/s\n",
      "2019-02-26 22:45:32,184 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-02-26 22:45:32,192 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-02-26 22:45:32,196 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-02-26 22:45:32,202 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-02-26 22:45:32,207 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-02-26 22:45:32,209 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-02-26 22:45:32,210 : INFO : EPOCH - 5 : training on 541792 raw words (318769 effective words) took 0.4s, 815570 effective words/s\n",
      "2019-02-26 22:45:32,210 : INFO : training on a 2708960 raw words (1594463 effective words) took 2.0s, 793298 effective words/s\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "word2vec_model = gensim.models.Word2Vec(sentences=sent_lst,\n",
    "                                        min_count=6,\n",
    "                                        size=EMBEDDING_DIM,\n",
    "                                        sg=1,\n",
    "                                        workers=os.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 3565 word vectors\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "\n",
    "for word in word2vec_model.wv.vocab:\n",
    "    coefs = np.asarray(word2vec_model.wv[word], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "\n",
    "print('Total %s word vectors' % len(embeddings_index))\n",
    "\n",
    "# Initial word embedding\n",
    "embedding_matrix = np.zeros((VOCAB_SIZE, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in tok.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None and i < VOCAB_SIZE:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_create_train(labels, lstm_dim, batch_size, num_epochs, optimizer_param, regularization=1e-7):\n",
    "    \n",
    "    l2_reg = regularizers.l2(regularization)\n",
    "    \n",
    "    # init model\n",
    "    embedding_layer = Embedding(VOCAB_SIZE,\n",
    "                                EMBEDDING_DIM,\n",
    "                                input_length=MAX_DOC_LEN,\n",
    "                                trainable=True,\n",
    "                                mask_zero=False,\n",
    "                                embeddings_regularizer=l2_reg,\n",
    "                                weights=[embedding_matrix])\n",
    "    lstm_layer = LSTM(units=lstm_dim, kernel_regularizer=l2_reg)\n",
    "    dense_layer = Dense(n_classes, activation='softmax', kernel_regularizer=l2_reg)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(embedding_layer)\n",
    "    model.add(Bidirectional(lstm_layer))\n",
    "    model.add(Dropout(0.2))  # todo\n",
    "    model.add(dense_layer)\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizer_param,\n",
    "                  metrics=['acc'])\n",
    "    fname = \"lstm\"\n",
    "    history = History()\n",
    "    csv_logger = CSVLogger('./LSTM/{0}_{1}.log'.format(fname, regularization),\n",
    "                           separator=',',\n",
    "                           append=True)\n",
    "    t1 = time.time()\n",
    "    # model fit\n",
    "    model.fit(train_seq,\n",
    "              labels.astype('float32'),\n",
    "              batch_size=batch_size,\n",
    "              epochs=num_epochs,\n",
    "              callbacks=[history, csv_logger],\n",
    "              verbose=2)\n",
    "    t2 = time.time()\n",
    "    # save h5\n",
    "    model.save('./LSTM/{0}_{1}_model.h5'.format(fname, regularization))\n",
    "    np.savetxt('./LSTM/{0}_{1}_time.txt'.format(fname, regularization), \n",
    "               [regularization, (t2-t1) / 3600])\n",
    "    with open('./LSTM/{0}_{1}_history.txt'.format(fname, regularization), \"w\") as res_file:\n",
    "        res_file.write(str(history.history))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n",
      " - 14s - loss: 0.5271 - acc: 0.7457\n",
      "Epoch 2/80\n",
      " - 14s - loss: 0.4520 - acc: 0.7934\n",
      "Epoch 3/80\n",
      " - 14s - loss: 0.4299 - acc: 0.8062\n",
      "Epoch 4/80\n",
      " - 15s - loss: 0.4132 - acc: 0.8157\n",
      "Epoch 5/80\n",
      " - 15s - loss: 0.4032 - acc: 0.8249\n",
      "Epoch 6/80\n",
      " - 15s - loss: 0.3910 - acc: 0.8317\n",
      "Epoch 7/80\n",
      " - 15s - loss: 0.3867 - acc: 0.8316\n",
      "Epoch 8/80\n",
      " - 15s - loss: 0.3776 - acc: 0.8374\n",
      "Epoch 9/80\n",
      " - 15s - loss: 0.3740 - acc: 0.8410\n",
      "Epoch 10/80\n",
      " - 15s - loss: 0.3706 - acc: 0.8419\n",
      "Epoch 11/80\n",
      " - 15s - loss: 0.3627 - acc: 0.8504\n",
      "Epoch 12/80\n",
      " - 15s - loss: 0.3628 - acc: 0.8470\n",
      "Epoch 13/80\n",
      " - 15s - loss: 0.3565 - acc: 0.8502\n",
      "Epoch 14/80\n",
      " - 15s - loss: 0.3538 - acc: 0.8538\n",
      "Epoch 15/80\n",
      " - 16s - loss: 0.3519 - acc: 0.8551\n",
      "Epoch 16/80\n",
      " - 15s - loss: 0.3514 - acc: 0.8548\n",
      "Epoch 17/80\n",
      " - 15s - loss: 0.3483 - acc: 0.8564\n",
      "Epoch 18/80\n",
      " - 15s - loss: 0.3461 - acc: 0.8583\n",
      "Epoch 19/80\n",
      " - 15s - loss: 0.3451 - acc: 0.8599\n",
      "Epoch 20/80\n",
      " - 16s - loss: 0.3425 - acc: 0.8616\n",
      "Epoch 21/80\n",
      " - 15s - loss: 0.3415 - acc: 0.8612\n",
      "Epoch 22/80\n",
      " - 15s - loss: 0.3407 - acc: 0.8616\n",
      "Epoch 23/80\n",
      " - 15s - loss: 0.3355 - acc: 0.8641\n",
      "Epoch 24/80\n",
      " - 15s - loss: 0.3348 - acc: 0.8644\n",
      "Epoch 25/80\n",
      " - 15s - loss: 0.3334 - acc: 0.8655\n",
      "Epoch 26/80\n",
      " - 15s - loss: 0.3336 - acc: 0.8651\n",
      "Epoch 27/80\n",
      " - 15s - loss: 0.3308 - acc: 0.8672\n",
      "Epoch 28/80\n",
      " - 15s - loss: 0.3304 - acc: 0.8657\n",
      "Epoch 29/80\n",
      " - 16s - loss: 0.3286 - acc: 0.8668\n",
      "Epoch 30/80\n",
      " - 16s - loss: 0.3277 - acc: 0.8667\n",
      "Epoch 31/80\n",
      " - 15s - loss: 0.3239 - acc: 0.8686\n",
      "Epoch 32/80\n",
      " - 15s - loss: 0.3251 - acc: 0.8683\n",
      "Epoch 33/80\n",
      " - 16s - loss: 0.3262 - acc: 0.8674\n",
      "Epoch 34/80\n",
      " - 15s - loss: 0.3239 - acc: 0.8695\n",
      "Epoch 35/80\n",
      " - 16s - loss: 0.3219 - acc: 0.8704\n",
      "Epoch 36/80\n",
      " - 15s - loss: 0.3192 - acc: 0.8686\n",
      "Epoch 37/80\n",
      " - 15s - loss: 0.3213 - acc: 0.8716\n",
      "Epoch 38/80\n",
      " - 15s - loss: 0.3200 - acc: 0.8728\n",
      "Epoch 39/80\n",
      " - 16s - loss: 0.3188 - acc: 0.8721\n",
      "Epoch 40/80\n",
      " - 15s - loss: 0.3179 - acc: 0.8711\n",
      "Epoch 41/80\n",
      " - 16s - loss: 0.3169 - acc: 0.8729\n",
      "Epoch 42/80\n",
      " - 15s - loss: 0.3159 - acc: 0.8728\n",
      "Epoch 43/80\n",
      " - 15s - loss: 0.3142 - acc: 0.8751\n",
      "Epoch 44/80\n",
      " - 15s - loss: 0.3142 - acc: 0.8739\n",
      "Epoch 45/80\n",
      " - 15s - loss: 0.3108 - acc: 0.8738\n",
      "Epoch 46/80\n"
     ]
    }
   ],
   "source": [
    "lstm_create_train(labels=text_recognition_labels,\n",
    "                  lstm_dim=100, \n",
    "                  batch_size=60,\n",
    "                  num_epochs=80, \n",
    "                  optimizer_param=SGD(lr=0.06, nesterov=True),\n",
    "                  regularization=1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n",
      " - 14s - loss: 0.5209 - acc: 0.7502\n",
      "Epoch 2/80\n",
      " - 14s - loss: 0.4539 - acc: 0.7946\n",
      "Epoch 3/80\n",
      " - 14s - loss: 0.4314 - acc: 0.8098\n",
      "Epoch 4/80\n",
      " - 14s - loss: 0.4201 - acc: 0.8127\n",
      "Epoch 5/80\n",
      " - 14s - loss: 0.4067 - acc: 0.8211\n",
      "Epoch 6/80\n",
      " - 14s - loss: 0.3972 - acc: 0.8267\n",
      "Epoch 7/80\n",
      " - 14s - loss: 0.3925 - acc: 0.8312\n",
      "Epoch 8/80\n",
      " - 14s - loss: 0.3863 - acc: 0.8345\n",
      "Epoch 9/80\n",
      " - 14s - loss: 0.3792 - acc: 0.8366\n",
      "Epoch 10/80\n",
      " - 14s - loss: 0.3787 - acc: 0.8401\n",
      "Epoch 11/80\n",
      " - 14s - loss: 0.3725 - acc: 0.8436\n",
      "Epoch 12/80\n",
      " - 14s - loss: 0.3658 - acc: 0.8456\n",
      "Epoch 13/80\n",
      " - 14s - loss: 0.3628 - acc: 0.8458\n",
      "Epoch 14/80\n",
      " - 14s - loss: 0.3584 - acc: 0.8502\n",
      "Epoch 15/80\n",
      " - 14s - loss: 0.3623 - acc: 0.8490\n",
      "Epoch 16/80\n",
      " - 14s - loss: 0.3549 - acc: 0.8530\n",
      "Epoch 17/80\n",
      " - 14s - loss: 0.3542 - acc: 0.8540\n",
      "Epoch 18/80\n",
      " - 14s - loss: 0.3510 - acc: 0.8554\n",
      "Epoch 19/80\n",
      " - 14s - loss: 0.3489 - acc: 0.8572\n",
      "Epoch 20/80\n",
      " - 14s - loss: 0.3464 - acc: 0.8557\n",
      "Epoch 21/80\n",
      " - 14s - loss: 0.3435 - acc: 0.8576\n",
      "Epoch 22/80\n",
      " - 14s - loss: 0.3436 - acc: 0.8575\n",
      "Epoch 23/80\n",
      " - 14s - loss: 0.3413 - acc: 0.8601\n",
      "Epoch 24/80\n",
      " - 14s - loss: 0.3407 - acc: 0.8592\n",
      "Epoch 25/80\n",
      " - 14s - loss: 0.3374 - acc: 0.8622\n",
      "Epoch 26/80\n",
      " - 14s - loss: 0.3377 - acc: 0.8611\n",
      "Epoch 27/80\n",
      " - 14s - loss: 0.3387 - acc: 0.8604\n",
      "Epoch 28/80\n",
      " - 14s - loss: 0.3380 - acc: 0.8614\n",
      "Epoch 29/80\n",
      " - 14s - loss: 0.3347 - acc: 0.8613\n",
      "Epoch 30/80\n",
      " - 14s - loss: 0.3328 - acc: 0.8646\n",
      "Epoch 31/80\n",
      " - 14s - loss: 0.3318 - acc: 0.8654\n",
      "Epoch 32/80\n",
      " - 14s - loss: 0.3283 - acc: 0.8669\n",
      "Epoch 33/80\n",
      " - 14s - loss: 0.3283 - acc: 0.8667\n",
      "Epoch 34/80\n",
      " - 14s - loss: 0.3300 - acc: 0.8676\n",
      "Epoch 35/80\n",
      " - 14s - loss: 0.3259 - acc: 0.8682\n",
      "Epoch 36/80\n",
      " - 14s - loss: 0.3271 - acc: 0.8660\n",
      "Epoch 37/80\n",
      " - 14s - loss: 0.3249 - acc: 0.8672\n",
      "Epoch 38/80\n",
      " - 14s - loss: 0.3224 - acc: 0.8696\n",
      "Epoch 39/80\n",
      " - 14s - loss: 0.3213 - acc: 0.8696\n",
      "Epoch 40/80\n",
      " - 14s - loss: 0.3202 - acc: 0.8709\n",
      "Epoch 41/80\n",
      " - 14s - loss: 0.3192 - acc: 0.8709\n",
      "Epoch 42/80\n",
      " - 14s - loss: 0.3193 - acc: 0.8732\n",
      "Epoch 43/80\n",
      " - 14s - loss: 0.3171 - acc: 0.8723\n",
      "Epoch 44/80\n",
      " - 14s - loss: 0.3171 - acc: 0.8724\n",
      "Epoch 45/80\n",
      " - 14s - loss: 0.3178 - acc: 0.8723\n",
      "Epoch 46/80\n",
      " - 14s - loss: 0.3156 - acc: 0.8735\n",
      "Epoch 47/80\n",
      " - 14s - loss: 0.3128 - acc: 0.8729\n",
      "Epoch 48/80\n",
      " - 14s - loss: 0.3126 - acc: 0.8753\n",
      "Epoch 49/80\n",
      " - 14s - loss: 0.3126 - acc: 0.8739\n",
      "Epoch 50/80\n",
      " - 14s - loss: 0.3094 - acc: 0.8755\n",
      "Epoch 51/80\n",
      " - 14s - loss: 0.3100 - acc: 0.8749\n",
      "Epoch 52/80\n",
      " - 14s - loss: 0.3109 - acc: 0.8744\n",
      "Epoch 53/80\n",
      " - 14s - loss: 0.3071 - acc: 0.8780\n",
      "Epoch 54/80\n",
      " - 14s - loss: 0.3068 - acc: 0.8766\n",
      "Epoch 55/80\n",
      " - 14s - loss: 0.3055 - acc: 0.8780\n",
      "Epoch 56/80\n",
      " - 14s - loss: 0.3044 - acc: 0.8781\n",
      "Epoch 57/80\n",
      " - 14s - loss: 0.3040 - acc: 0.8804\n",
      "Epoch 58/80\n",
      " - 14s - loss: 0.3034 - acc: 0.8773\n",
      "Epoch 59/80\n",
      " - 14s - loss: 0.3002 - acc: 0.8781\n",
      "Epoch 60/80\n",
      " - 14s - loss: 0.2994 - acc: 0.8800\n",
      "Epoch 61/80\n",
      " - 14s - loss: 0.3075 - acc: 0.8756\n",
      "Epoch 62/80\n",
      " - 14s - loss: 0.2997 - acc: 0.8793\n",
      "Epoch 63/80\n",
      " - 14s - loss: 0.3001 - acc: 0.8820\n",
      "Epoch 64/80\n",
      " - 14s - loss: 0.2978 - acc: 0.8817\n",
      "Epoch 65/80\n",
      " - 14s - loss: 0.2976 - acc: 0.8807\n",
      "Epoch 66/80\n",
      " - 14s - loss: 0.2963 - acc: 0.8822\n",
      "Epoch 67/80\n",
      " - 14s - loss: 0.2936 - acc: 0.8844\n",
      "Epoch 68/80\n",
      " - 14s - loss: 0.2951 - acc: 0.8827\n",
      "Epoch 69/80\n",
      " - 14s - loss: 0.2915 - acc: 0.8837\n",
      "Epoch 70/80\n",
      " - 14s - loss: 0.2896 - acc: 0.8848\n",
      "Epoch 71/80\n",
      " - 14s - loss: 0.2887 - acc: 0.8865\n",
      "Epoch 72/80\n",
      " - 14s - loss: 0.2877 - acc: 0.8838\n",
      "Epoch 73/80\n",
      " - 14s - loss: 0.2942 - acc: 0.8838\n",
      "Epoch 74/80\n",
      " - 14s - loss: 0.2875 - acc: 0.8871\n",
      "Epoch 75/80\n",
      " - 14s - loss: 0.2851 - acc: 0.8878\n",
      "Epoch 76/80\n",
      " - 14s - loss: 0.2853 - acc: 0.8860\n",
      "Epoch 77/80\n",
      " - 14s - loss: 0.2854 - acc: 0.8883\n",
      "Epoch 78/80\n",
      " - 14s - loss: 0.2812 - acc: 0.8875\n",
      "Epoch 79/80\n",
      " - 14s - loss: 0.2809 - acc: 0.8883\n",
      "Epoch 80/80\n",
      " - 14s - loss: 0.2793 - acc: 0.8912\n"
     ]
    }
   ],
   "source": [
    "# lemmatization\n",
    "lstm_create_train(labels=text_recognition_labels,\n",
    "                  lstm_dim=100, \n",
    "                  batch_size=60,\n",
    "                  num_epochs=80, \n",
    "                  optimizer_param=SGD(lr=0.06, nesterov=True),\n",
    "                  regularization=1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = load_model('./LSTM/lstm_{0}_model.h5'.format(1e-3))\n",
    "preds = model.predict(val_seq, verbose=0)\n",
    "roc_auc_score(txt_val, preds[:,1])\n",
    "#print((\"Accuracy = {0} \\t AUC = {1}\".format(accuracy_score(txt_val, preds.argmax(axis=1)), \n",
    "#       roc_auc_score(txt_val, preds[:,1]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Color recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_create_train(labels=color_recognition_labels,\n",
    "                  lstm_dim=100, \n",
    "                  batch_size=50,\n",
    "                  num_epochs=30, \n",
    "                  optimizer_param=SGD(lr=0.05, nesterov=True),\n",
    "                  regularization=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_create_train(labels=color_recognition_labels,\n",
    "                  lstm_dim=100, \n",
    "                  batch_size=100,\n",
    "                  num_epochs=30, \n",
    "                  optimizer_param=SGD(lr=0.05, nesterov=True),\n",
    "                  regularization=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_create_train(labels=color_recognition_labels,\n",
    "                  lstm_dim=100, \n",
    "                  batch_size=100,\n",
    "                  num_epochs=50, \n",
    "                  optimizer_param=SGD(lr=0.03, nesterov=True),\n",
    "                  regularization=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_create_train(labels=color_recognition_labels,\n",
    "                  lstm_dim=100, \n",
    "                  batch_size=200,\n",
    "                  num_epochs=30, \n",
    "                  optimizer_param=SGD(lr=0.02, nesterov=True),\n",
    "                  regularization=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
