{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skill label prediction with image-based features (text and color with descriptive tags)\n",
    "## Text recognition only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py35/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using CNTK backend\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/edithzeng/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/edithzeng/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import random\n",
    "import numpy as np\n",
    "import copy\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "kfold=KFold(n_splits=10)\n",
    "\n",
    "# for LSTM (keras with tf backend)\n",
    "import gzip\n",
    "import os\n",
    "import pickle\n",
    "import requests\n",
    "import time\n",
    "import re\n",
    "os.environ['KERAS_BACKEND']='cntk'\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.models import Sequential, load_model\n",
    "from keras import regularizers\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.initializers import he_normal\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional\n",
    "from keras.callbacks import History, CSVLogger\n",
    "from keras.utils import to_categorical\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "MAX_DOC_LEN = 40\n",
    "VOCAB_SIZE = 3000\n",
    "EMBEDDING_DIM = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vizwiz_features_train_color = pd.read_csv('azure_features_images/data/vizwiz_train_color_recognition.csv',\n",
    "                                    delimiter=';', engine='python', \n",
    "                                    dtype={'qid':str, 'question':str, 'descriptions':list,\n",
    "                                          'tags':list, 'dominant_colors':list},\n",
    "                                    quotechar='\"', error_bad_lines=False)\n",
    "vizwiz_features_train_text = pd.read_csv('azure_features_images/data/vizwiz_train_text_recognition.csv',\n",
    "                                    delimiter=';', engine='python', \n",
    "                                    dtype={'qid':str, 'question':str, 'descriptions':list,\n",
    "                                          'ocr_text':list, 'handwritten_text':list},\n",
    "                                    quotechar='\"', error_bad_lines=False)\n",
    "\n",
    "\n",
    "vizwiz_features_val_color = pd.read_csv('azure_features_images/data/vizwiz_val_color_recognition.csv',\n",
    "                                  delimiter=';', engine='python',\n",
    "                                  dtype={'qid':str, 'question':str, 'descriptions':list,\n",
    "                                          'tags':list, 'dominant_colors':list},\n",
    "                                  quotechar='\"', error_bad_lines=False)\n",
    "vizwiz_features_val_text = pd.read_csv('azure_features_images/data/vizwiz_val_text_recognition.csv',\n",
    "                                    delimiter=';', engine='python', \n",
    "                                    dtype={'qid':str, 'question':str, 'descriptions':list,\n",
    "                                          'ocr_text':list, 'handwritten_text':list},\n",
    "                                    quotechar='\"', error_bad_lines=False)\n",
    "\n",
    "\n",
    "vqa_features_train_color = pd.read_csv('azure_features_images/data/vqa_train_color_recognition.csv',\n",
    "                                 delimiter=';', engine='python', \n",
    "                                 dtype={'qid':str, 'question':str, 'descriptions':list,\n",
    "                                          'tags':list, 'dominant_colors':list},\n",
    "                                 quotechar='\"', error_bad_lines=False)\n",
    "vqa_features_train_text = pd.read_csv('azure_features_images/data/vqa_train_text_recognition.csv',\n",
    "                                    delimiter=';', engine='python', \n",
    "                                    dtype={'qid':str, 'question':str, 'descriptions':list,\n",
    "                                          'ocr_text':list, 'handwritten_text':list},\n",
    "                                    quotechar='\"', error_bad_lines=False)\n",
    "vqa_features_val_color = pd.read_csv('azure_features_images/data/vqa_val_color_recognition.csv',\n",
    "                               delimiter=';', engine='python',\n",
    "                               dtype={'qid':str, 'question':str, 'descriptions':list,\n",
    "                                          'tags':list, 'dominant_colors':list},\n",
    "                               quotechar='\"', error_bad_lines=False)\n",
    "vqa_features_val_text = pd.read_csv('azure_features_images/data/vqa_val_text_recognition.csv',\n",
    "                                    delimiter=';', engine='python', \n",
    "                                    dtype={'qid':str, 'question':str, 'descriptions':list,\n",
    "                                          'ocr_text':list, 'handwritten_text':list},\n",
    "                                    quotechar='\"', error_bad_lines=False)\n",
    "\n",
    "\n",
    "vizwiz_targets_train = pd.read_csv('../vizwiz_skill_typ_train.csv', dtype={'QID':str},\n",
    "                                   delimiter=',', quotechar='\"',\n",
    "                                   engine='python', error_bad_lines=False)\n",
    "vizwiz_targets_val = pd.read_csv('../vizwiz_skill_typ_val.csv', dtype={'QID':str},\n",
    "                                 delimiter=',', quotechar='\"', engine='python', error_bad_lines=False)\n",
    "vqa_targets_train = pd.read_csv('../vqa_skill_typ_train.csv', dtype={'QID':str},\n",
    "                               engine='python', quotechar='\"', error_bad_lines=False)\n",
    "vqa_targets_val = pd.read_csv('../vqa_skill_typ_val.csv', dtype={'QID':str},\n",
    "                               engine='python', quotechar='\"', error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vizwiz_features_train_text.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IMG</th>\n",
       "      <th>QSN</th>\n",
       "      <th>TXT</th>\n",
       "      <th>OBJ</th>\n",
       "      <th>COL</th>\n",
       "      <th>CNT</th>\n",
       "      <th>OTH</th>\n",
       "      <th>question</th>\n",
       "      <th>ocr_text</th>\n",
       "      <th>handwritten_text</th>\n",
       "      <th>descriptions</th>\n",
       "      <th>tags</th>\n",
       "      <th>dominant_colors</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>VizWiz_train_000000017125.jpg</th>\n",
       "      <td>VizWiz_train_000000017125.jpg</td>\n",
       "      <td>What is this?</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>What is this?</td>\n",
       "      <td>['\\\\einz', '•heart', '&amp;', 'st', 'rofk', 'du', ...</td>\n",
       "      <td>['HEINZ', '- CANADA FANCY -', 'ATO JUICE', '-C...</td>\n",
       "      <td>[{'text': 'a bottle of beer on a table', 'conf...</td>\n",
       "      <td>['indoor', 'bottle', 'table', 'sitting', 'coff...</td>\n",
       "      <td>['Grey', 'White', 'Black']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                         IMG            QSN  \\\n",
       "QID                                                                           \n",
       "VizWiz_train_000000017125.jpg  VizWiz_train_000000017125.jpg  What is this?   \n",
       "\n",
       "                              TXT OBJ COL CNT OTH       question  \\\n",
       "QID                                                                \n",
       "VizWiz_train_000000017125.jpg   1   1   0   0   0  What is this?   \n",
       "\n",
       "                                                                        ocr_text  \\\n",
       "QID                                                                                \n",
       "VizWiz_train_000000017125.jpg  ['\\\\einz', '•heart', '&', 'st', 'rofk', 'du', ...   \n",
       "\n",
       "                                                                handwritten_text  \\\n",
       "QID                                                                                \n",
       "VizWiz_train_000000017125.jpg  ['HEINZ', '- CANADA FANCY -', 'ATO JUICE', '-C...   \n",
       "\n",
       "                                                                    descriptions  \\\n",
       "QID                                                                                \n",
       "VizWiz_train_000000017125.jpg  [{'text': 'a bottle of beer on a table', 'conf...   \n",
       "\n",
       "                                                                            tags  \\\n",
       "QID                                                                                \n",
       "VizWiz_train_000000017125.jpg  ['indoor', 'bottle', 'table', 'sitting', 'coff...   \n",
       "\n",
       "                                          dominant_colors  \n",
       "QID                                                        \n",
       "VizWiz_train_000000017125.jpg  ['Grey', 'White', 'Black']  "
      ]
     },
     "execution_count": 406,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vizwiz_train.sample(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_feature_target(feature_df_text, feature_df_color, target_df):\n",
    "    feature_text = copy.deepcopy(feature_df_text)\n",
    "    feature_color = copy.deepcopy(feature_df_color)\n",
    "    target = copy.deepcopy(target_df)\n",
    "    # text features \n",
    "    feature_text.rename({'qid': 'QID'}, axis=1, inplace=True)\n",
    "    feature_text.set_index('QID', inplace=True)\n",
    "    # color features\n",
    "    feature_color.rename({'qid': 'QID'}, axis=1, inplace=True)\n",
    "    feature_color.set_index('QID', inplace=True)\n",
    "    # join features\n",
    "    features = feature_text.join(feature_color[['descriptions','tags','dominant_colors']],\n",
    "                                 on='QID',\n",
    "                                 how='outer')\n",
    "    # join features with target\n",
    "    target = target[['QID', 'IMG', 'QSN', 'TXT', 'OBJ', 'COL', 'CNT', 'OTH']]\n",
    "    target.set_index('QID', inplace=True)\n",
    "    target = target.astype(dtype=str)\n",
    "    df = target.join(features, on='QID', how='inner')\n",
    "    df['descriptions'].astype(list)\n",
    "    return df\n",
    "\n",
    "def lem(s):\n",
    "    arr = s.split(\" \")\n",
    "    lem = WordNetLemmatizer()\n",
    "    op = \"\"\n",
    "    for w in arr:\n",
    "        word = lem.lemmatize(w) + ' '\n",
    "        op += word\n",
    "    return op\n",
    "\n",
    "def preprocess_text(feature_columns):\n",
    "    \"\"\" output an nparray with single document per data point \"\"\"\n",
    "    ip = copy.deepcopy(feature_columns).values\n",
    "    op = []\n",
    "    for i in range(ip.shape[0]):\n",
    "        doc      =  \"\"\n",
    "        for j in range(ip.shape[1]):\n",
    "            # clean up chars\n",
    "            s    =  str(ip[i][j])\n",
    "            s    =  s.translate({ord(c): \"\" for c in \"!@#$%^&*()[]{};:,./<>?\\|`~-=_+'\"}).lower() + \" \"\n",
    "            if j == 1:             # clean descriptions\n",
    "                s = re.sub(r'confidence\\s+\\d+', '', s)\n",
    "                s = re.sub(r'text', '', s)\n",
    "            # lexicon normalize\n",
    "            s    = lem(s)\n",
    "            doc  += s\n",
    "        op.append(doc)\n",
    "    op = np.asarray(op)\n",
    "    return op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14257, 13) (3230, 13) 17487\n",
      "(2247, 13) (513, 13) 2760\n"
     ]
    }
   ],
   "source": [
    "vizwiz_train   = join_feature_target(vizwiz_features_train_text, \n",
    "                                   vizwiz_features_train_color, \n",
    "                                   vizwiz_targets_train)\n",
    "vizwiz_val     = join_feature_target(vizwiz_features_val_text, \n",
    "                                 vizwiz_features_val_color, \n",
    "                                 vizwiz_targets_val)\n",
    "vqa_train      = join_feature_target(vqa_features_train_text, \n",
    "                                   vqa_features_train_color, \n",
    "                                   vqa_targets_train)\n",
    "vqa_val        = join_feature_target(vizwiz_features_val_text, \n",
    "                                 vqa_features_val_color, \n",
    "                                 vqa_targets_val)\n",
    "print(vizwiz_train.shape, vqa_train.shape, vizwiz_train.shape[0] + vqa_train.shape[0])\n",
    "print(vizwiz_val.shape, vqa_val.shape, vizwiz_val.shape[0] + vqa_val.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: (17487, 13)\n",
      "Validation: (2760, 13)\n"
     ]
    }
   ],
   "source": [
    "# create X and Y\n",
    "\n",
    "train = pd.concat([vizwiz_train, vqa_train], axis=0)\n",
    "val   = pd.concat([vizwiz_val, vqa_val], axis=0)\n",
    "print(\"Training: {}\\nValidation: {}\".format(train.shape, val.shape))\n",
    "\n",
    "features_train = preprocess_text(train[['QSN','descriptions', 'tags', 'dominant_colors', \n",
    "                                        'handwritten_text', 'ocr_text']])\n",
    "txt_train      = train['TXT'].values\n",
    "col_train      = train['COL'].values\n",
    "cnt_train      = train['CNT'].values\n",
    "\n",
    "features_val   = preprocess_text(val[['QSN', 'descriptions', 'tags', 'dominant_colors',\n",
    "                                      'handwritten_text', 'ocr_text']])\n",
    "txt_val        = val['TXT'].values.astype('float32')\n",
    "col_val        = val['COL'].values.astype('float32')\n",
    "cnt_val        = val['CNT'].values.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'what kind of pudding is this   a person holding a remote control   person indoor remote sitting holding man orange control red television boy girl young bed playing food table room video game white standing shirt  black      '"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_train[random.randint(0,len(features_val))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i know this is healthy choice but what dinner is it   a close up of food on a table   indoor table food sitting green plate restaurant wooden sandwich white  black brown  healthy choice complete meall with dessert solwin healthy  complete meal healthy with dessert •hoice 9g fiber 310 calorie homestyle bury salis steak salisbury steak in sautéed onion red apple ca crisp multigrain  '"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_val[random.randint(0,len(features_val))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tokenize\n",
    "tok        = Tokenizer(num_words=VOCAB_SIZE, \n",
    "                       filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "                       lower=True,\n",
    "                       split=\" \")\n",
    "tok.fit_on_texts(features_train)\n",
    "\n",
    "# create sequences & pad\n",
    "train_seq  = tok.texts_to_sequences(features_train)\n",
    "train_seq  = sequence.pad_sequences(train_seq, maxlen=MAX_DOC_LEN)\n",
    "val_seq    = tok.texts_to_sequences(features_val)\n",
    "val_seq    = sequence.pad_sequences(val_seq, maxlen=MAX_DOC_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples each class: \n",
      "Text recognition [9119. 8368.]\n",
      "Color recognition [10926.  6561.]\n"
     ]
    }
   ],
   "source": [
    "# check class distribution\n",
    "text_recognition_labels = to_categorical(np.asarray(txt_train)).astype('float32')\n",
    "color_recognition_labels = to_categorical(np.asarray(col_train)).astype('float32')\n",
    "print('Number of samples each class: ')\n",
    "print('Text recognition', text_recognition_labels.sum(axis=0))\n",
    "print('Color recognition', color_recognition_labels.sum(axis=0))\n",
    "n_classes = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM model (skip-gram word2vec)\n",
    "config for CPU only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import requests\n",
    "import time\n",
    "import gzip\n",
    "\n",
    "os.environ['KERAS_BACKEND'] = 'cntk'\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.models import Sequential, load_model\n",
    "from keras import regularizers\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional\n",
    "from keras.callbacks import History, CSVLogger\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import nltk \n",
    "import gensim\n",
    "import logging\n",
    "\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve\n",
    "\n",
    "MAX_DOC_LEN = 40\n",
    "VOCAB_SIZE = 3000\n",
    "EMBEDDING_DIM = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# punkt sentence level tokenizer\n",
    "sent_lst = []\n",
    "for doc in features_train:\n",
    "    sentences = nltk.tokenize.sent_tokenize(doc)\n",
    "    for sent in sentences:\n",
    "        word_lst = [w for w in nltk.tokenize.word_tokenize(sent) if w.isalnum()]\n",
    "        sent_lst.append(word_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-02-28 02:08:24,304 : INFO : collecting all words and their counts\n",
      "2019-02-28 02:08:24,305 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-02-28 02:08:24,370 : INFO : PROGRESS: at sentence #10000, processed 383294 words, keeping 24880 word types\n",
      "2019-02-28 02:08:24,417 : INFO : collected 33634 word types from a corpus of 671659 raw words and 17487 sentences\n",
      "2019-02-28 02:08:24,418 : INFO : Loading a fresh vocabulary\n",
      "2019-02-28 02:08:24,440 : INFO : effective_min_count=6 retains 3819 unique words (11% of original 33634, drops 29815)\n",
      "2019-02-28 02:08:24,441 : INFO : effective_min_count=6 leaves 629542 word corpus (93% of original 671659, drops 42117)\n",
      "2019-02-28 02:08:24,450 : INFO : deleting the raw counts dictionary of 33634 items\n",
      "2019-02-28 02:08:24,452 : INFO : sample=0.001 downsamples 68 most-common words\n",
      "2019-02-28 02:08:24,452 : INFO : downsampling leaves estimated 412555 word corpus (65.5% of prior 629542)\n",
      "2019-02-28 02:08:24,461 : INFO : estimated required memory for 3819 words and 100 dimensions: 4964700 bytes\n",
      "2019-02-28 02:08:24,461 : INFO : resetting layer weights\n",
      "2019-02-28 02:08:24,504 : INFO : training model with 6 workers on 3819 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-02-28 02:08:24,995 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-02-28 02:08:25,001 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-02-28 02:08:25,002 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-02-28 02:08:25,003 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-02-28 02:08:25,003 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-02-28 02:08:25,010 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-02-28 02:08:25,010 : INFO : EPOCH - 1 : training on 671659 raw words (412964 effective words) took 0.5s, 825203 effective words/s\n",
      "2019-02-28 02:08:25,515 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-02-28 02:08:25,518 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-02-28 02:08:25,519 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-02-28 02:08:25,535 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-02-28 02:08:25,549 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-02-28 02:08:25,556 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-02-28 02:08:25,556 : INFO : EPOCH - 2 : training on 671659 raw words (412906 effective words) took 0.5s, 764881 effective words/s\n",
      "2019-02-28 02:08:26,052 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-02-28 02:08:26,061 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-02-28 02:08:26,067 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-02-28 02:08:26,069 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-02-28 02:08:26,070 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-02-28 02:08:26,078 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-02-28 02:08:26,078 : INFO : EPOCH - 3 : training on 671659 raw words (412704 effective words) took 0.5s, 798971 effective words/s\n",
      "2019-02-28 02:08:26,561 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-02-28 02:08:26,566 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-02-28 02:08:26,569 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-02-28 02:08:26,572 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-02-28 02:08:26,576 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-02-28 02:08:26,587 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-02-28 02:08:26,587 : INFO : EPOCH - 4 : training on 671659 raw words (412997 effective words) took 0.5s, 821983 effective words/s\n",
      "2019-02-28 02:08:27,070 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-02-28 02:08:27,076 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-02-28 02:08:27,078 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-02-28 02:08:27,079 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-02-28 02:08:27,087 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-02-28 02:08:27,094 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-02-28 02:08:27,094 : INFO : EPOCH - 5 : training on 671659 raw words (412250 effective words) took 0.5s, 822596 effective words/s\n",
      "2019-02-28 02:08:27,095 : INFO : training on a 3358295 raw words (2063821 effective words) took 2.6s, 796741 effective words/s\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "word2vec_model = gensim.models.Word2Vec(sentences=sent_lst,\n",
    "                                        min_count=6,\n",
    "                                        size=EMBEDDING_DIM,\n",
    "                                        sg=1,\n",
    "                                        workers=os.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 3819 word vectors\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "\n",
    "for word in word2vec_model.wv.vocab:\n",
    "    coefs = np.asarray(word2vec_model.wv[word], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "\n",
    "print('Total %s word vectors' % len(embeddings_index))\n",
    "\n",
    "# Initial word embedding\n",
    "embedding_matrix = np.zeros((VOCAB_SIZE, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in tok.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None and i < VOCAB_SIZE:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_create_train(labels, learning_rate, lstm_dim, batch_size, num_epochs, optimizer_param, regularization=1e-7):\n",
    "    \n",
    "    l2_reg = regularizers.l2(regularization)\n",
    "    \n",
    "    # init model\n",
    "    embedding_layer = Embedding(VOCAB_SIZE,\n",
    "                                EMBEDDING_DIM,\n",
    "                                input_length=MAX_DOC_LEN,\n",
    "                                trainable=True,\n",
    "                                mask_zero=False,\n",
    "                                embeddings_regularizer=l2_reg,\n",
    "                                weights=[embedding_matrix])\n",
    "    lstm_layer = LSTM(units=lstm_dim, kernel_regularizer=l2_reg)\n",
    "    dense_layer = Dense(n_classes,\n",
    "                        activation='softmax', \n",
    "                        kernel_regularizer=l2_reg)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(embedding_layer)\n",
    "    model.add(Bidirectional(lstm_layer))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(dense_layer)\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizer_param,\n",
    "                  metrics=['acc'])\n",
    "    history = History()\n",
    "    csv_logger = CSVLogger('./LSTM/text/{}_{}_{}_{}.log'.format(learning_rate, regularization, batch_size, num_epochs),\n",
    "                           separator=',',\n",
    "                           append=True)\n",
    "    t1 = time.time()\n",
    "    # model fit\n",
    "    model.fit(train_seq,\n",
    "              labels.astype('float32'),\n",
    "              batch_size=batch_size,\n",
    "              epochs=num_epochs,\n",
    "              callbacks=[history, csv_logger],\n",
    "              verbose=2)\n",
    "    t2 = time.time()\n",
    "    # save hdf5\n",
    "    model.save('./LSTM/text/{}_{}_{}_{}_model.h5'.format(learning_rate, regularization, batch_size, num_epochs))\n",
    "    np.savetxt('./LSTM/text/{}_{}_{}_{}_time.txt'.format(learning_rate, regularization, batch_size, num_epochs), \n",
    "               [regularization, (t2-t1) / 3600])\n",
    "    with open('./LSTM/text/{}_{}_{}_{}_history.txt'.format(learning_rate, regularization, batch_size, num_epochs), \"w\") as res_file:\n",
    "        res_file.write(str(history.history))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      " - 31s - loss: 0.4646 - acc: 0.7892\n",
      "Epoch 2/30\n",
      " - 31s - loss: 0.4124 - acc: 0.8254\n",
      "Epoch 3/30\n",
      " - 31s - loss: 0.3873 - acc: 0.8391\n",
      "Epoch 4/30\n",
      " - 31s - loss: 0.3728 - acc: 0.8457\n",
      "Epoch 5/30\n",
      " - 31s - loss: 0.3622 - acc: 0.8525\n",
      "Epoch 6/30\n",
      " - 31s - loss: 0.3563 - acc: 0.8542\n",
      "Epoch 7/30\n",
      " - 31s - loss: 0.3511 - acc: 0.8585\n",
      "Epoch 8/30\n",
      " - 31s - loss: 0.3448 - acc: 0.8593\n",
      "Epoch 9/30\n",
      " - 31s - loss: 0.3416 - acc: 0.8619\n",
      "Epoch 10/30\n",
      " - 31s - loss: 0.3355 - acc: 0.8674\n",
      "Epoch 11/30\n",
      " - 31s - loss: 0.3305 - acc: 0.8671\n",
      "Epoch 12/30\n",
      " - 31s - loss: 0.3310 - acc: 0.8672\n",
      "Epoch 13/30\n",
      " - 31s - loss: 0.3264 - acc: 0.8707\n",
      "Epoch 14/30\n",
      " - 31s - loss: 0.3200 - acc: 0.8730\n",
      "Epoch 15/30\n",
      " - 31s - loss: 0.3205 - acc: 0.8727\n",
      "Epoch 16/30\n",
      " - 31s - loss: 0.3173 - acc: 0.8753\n",
      "Epoch 17/30\n",
      " - 31s - loss: 0.3141 - acc: 0.8760\n",
      "Epoch 18/30\n",
      " - 31s - loss: 0.3098 - acc: 0.8788\n",
      "Epoch 19/30\n",
      " - 31s - loss: 0.3080 - acc: 0.8775\n",
      "Epoch 20/30\n",
      " - 31s - loss: 0.3061 - acc: 0.8792\n",
      "Epoch 21/30\n",
      " - 31s - loss: 0.2993 - acc: 0.8809\n",
      "Epoch 22/30\n",
      " - 31s - loss: 0.2998 - acc: 0.8821\n",
      "Epoch 23/30\n",
      " - 31s - loss: 0.2954 - acc: 0.8843\n",
      "Epoch 24/30\n",
      " - 31s - loss: 0.2935 - acc: 0.8838\n",
      "Epoch 25/30\n",
      " - 31s - loss: 0.2904 - acc: 0.8865\n",
      "Epoch 26/30\n",
      " - 31s - loss: 0.2881 - acc: 0.8876\n",
      "Epoch 27/30\n",
      " - 31s - loss: 0.2821 - acc: 0.8902\n",
      "Epoch 28/30\n",
      " - 31s - loss: 0.2780 - acc: 0.8932\n",
      "Epoch 29/30\n",
      " - 31s - loss: 0.2758 - acc: 0.8904\n",
      "Epoch 30/30\n",
      " - 31s - loss: 0.2710 - acc: 0.8934\n",
      "Learning rate: 0.2 Regularization: 1e-14 Batch size: 32 Epoch: 30\n",
      "Accuracy = 0.8496376811594203 \t AUC = 0.9012556082540055\n"
     ]
    }
   ],
   "source": [
    "L = 2e-1\n",
    "R = 1e-14\n",
    "B = 32\n",
    "E = 30\n",
    "lstm_create_train(labels=text_recognition_labels,\n",
    "                                  learning_rate=L,\n",
    "                                  lstm_dim=150, \n",
    "                                  batch_size=B,\n",
    "                                  num_epochs=E, \n",
    "                                  optimizer_param=SGD(lr=L, nesterov=True),\n",
    "                                  regularization=R)\n",
    "model = load_model('./LSTM/text/{}_{}_{}_{}_model.h5'.format(L,R,B,E))\n",
    "preds = model.predict(val_seq, verbose=0)\n",
    "print(\"Learning rate: {} Regularization: {} Batch size: {} Epoch: {}\".format(L,R,B,E))\n",
    "print((\"Accuracy = {0} \\t AUC = {1}\".format(accuracy_score(txt_val, preds.argmax(axis=1)), \n",
    "                                                            roc_auc_score(txt_val, preds[:,1]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 2e-1\n",
    "R = 0\n",
    "B = 32\n",
    "E = 30\n",
    "lstm_create_train(labels=text_recognition_labels,\n",
    "                                  learning_rate=L,\n",
    "                                  lstm_dim=250, \n",
    "                                  batch_size=B,\n",
    "                                  num_epochs=E, \n",
    "                                  optimizer_param=SGD(lr=L, nesterov=True),\n",
    "                                  regularization=R)\n",
    "model = load_model('./LSTM/text/{}_{}_{}_{}_model.h5'.format(L,R,B,E))\n",
    "preds = model.predict(val_seq, verbose=0)\n",
    "print(\"Learning rate: {} Regularization: {} Batch size: {} Epoch: {}\".format(L,R,B,E))\n",
    "print((\"Accuracy = {0} \\t AUC = {1}\".format(accuracy_score(txt_val, preds.argmax(axis=1)), \n",
    "                                                            roc_auc_score(txt_val, preds[:,1]))))\n",
    "# todo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and validation on VizWiz only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      " - 23s - loss: 0.4962 - acc: 0.7631\n",
      "Epoch 2/30\n",
      " - 23s - loss: 0.4303 - acc: 0.8098\n",
      "Epoch 3/30\n",
      " - 23s - loss: 0.4079 - acc: 0.8227\n",
      "Epoch 4/30\n",
      " - 23s - loss: 0.3894 - acc: 0.8347\n",
      "Epoch 5/30\n",
      " - 23s - loss: 0.3854 - acc: 0.8377\n",
      "Epoch 6/30\n",
      " - 23s - loss: 0.3775 - acc: 0.8409\n",
      "Epoch 7/30\n",
      " - 23s - loss: 0.3691 - acc: 0.8448\n",
      "Epoch 8/30\n",
      " - 23s - loss: 0.3630 - acc: 0.8495\n",
      "Epoch 9/30\n",
      " - 23s - loss: 0.3604 - acc: 0.8510\n",
      "Epoch 10/30\n",
      " - 23s - loss: 0.3538 - acc: 0.8542\n",
      "Epoch 11/30\n",
      " - 23s - loss: 0.3521 - acc: 0.8548\n",
      "Epoch 12/30\n",
      " - 23s - loss: 0.3485 - acc: 0.8579\n",
      "Epoch 13/30\n",
      " - 23s - loss: 0.3430 - acc: 0.8620\n",
      "Epoch 14/30\n",
      " - 23s - loss: 0.3418 - acc: 0.8604\n",
      "Epoch 15/30\n",
      " - 23s - loss: 0.3409 - acc: 0.8587\n",
      "Epoch 16/30\n",
      " - 23s - loss: 0.3366 - acc: 0.8641\n",
      "Epoch 17/30\n",
      " - 23s - loss: 0.3327 - acc: 0.8657\n",
      "Epoch 18/30\n",
      " - 23s - loss: 0.3311 - acc: 0.8687\n",
      "Epoch 19/30\n",
      " - 23s - loss: 0.3316 - acc: 0.8641\n",
      "Epoch 20/30\n",
      " - 23s - loss: 0.3307 - acc: 0.8673\n",
      "Epoch 21/30\n",
      " - 23s - loss: 0.3254 - acc: 0.8710\n",
      "Epoch 22/30\n",
      " - 23s - loss: 0.3231 - acc: 0.8688\n",
      "Epoch 23/30\n",
      " - 23s - loss: 0.3203 - acc: 0.8715\n",
      "Epoch 24/30\n",
      " - 23s - loss: 0.3228 - acc: 0.8684\n",
      "Epoch 25/30\n",
      " - 23s - loss: 0.3191 - acc: 0.8726\n",
      "Epoch 26/30\n",
      " - 23s - loss: 0.3174 - acc: 0.8726\n",
      "Epoch 27/30\n",
      " - 23s - loss: 0.3131 - acc: 0.8741\n",
      "Epoch 28/30\n",
      " - 23s - loss: 0.3146 - acc: 0.8754\n",
      "Epoch 29/30\n",
      " - 23s - loss: 0.3120 - acc: 0.8750\n",
      "Epoch 30/30\n",
      " - 23s - loss: 0.3079 - acc: 0.8788\n",
      "Learning rate: 0.1 Regularization: 0 Batch size: 32 Epoch: 30\n",
      "Accuracy = 0.8615932354250111 \t AUC = 0.9118524127205011\n"
     ]
    }
   ],
   "source": [
    "L = 1e-1\n",
    "R = 0\n",
    "B = 32\n",
    "E = 30\n",
    "lstm_create_train(labels=text_recognition_labels,\n",
    "                                  learning_rate=L,\n",
    "                                  lstm_dim=100, \n",
    "                                  batch_size=B,\n",
    "                                  num_epochs=E, \n",
    "                                  optimizer_param=SGD(lr=L, nesterov=True),\n",
    "                                  regularization=R)\n",
    "model = load_model('./LSTM/text/{}_{}_{}_{}_model.h5'.format(L,R,B,E))\n",
    "preds = model.predict(val_seq, verbose=0)\n",
    "print(\"Learning rate: {} Regularization: {} Batch size: {} Epoch: {}\".format(L,R,B,E))\n",
    "print((\"Accuracy = {0} \\t AUC = {1}\".format(accuracy_score(txt_val, preds.argmax(axis=1)), \n",
    "                                                            roc_auc_score(txt_val, preds[:,1]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
