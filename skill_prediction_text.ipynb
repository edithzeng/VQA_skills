{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skill label prediction with image-based features (text and color with descriptive tags)\n",
    "### Text recognition category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py35/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/edithzeng/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/edithzeng/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 9937585377355951665\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 249757696\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 5260878450049916427\n",
      "physical_device_desc: \"device: 0, name: Tesla K80, pci bus id: 365b:00:00.0, compute capability: 3.7\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import random\n",
    "import numpy as np\n",
    "import copy\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "kfold=KFold(n_splits=10)\n",
    "\n",
    "# for LSTM (keras with tf backend)\n",
    "import gzip\n",
    "import os\n",
    "import pickle\n",
    "import requests\n",
    "import time\n",
    "import re\n",
    "# os.environ['KERAS_BACKEND']='cntk'\n",
    "os.environ['KERAS_BACKEND']='tensorflow'\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.models import Sequential, load_model\n",
    "from keras import regularizers\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.initializers import he_normal\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional\n",
    "from keras.callbacks import History, CSVLogger\n",
    "from keras.utils import to_categorical\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "\n",
    "MAX_DOC_LEN = 40\n",
    "VOCAB_SIZE = 3000\n",
    "EMBEDDING_DIM = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipping line 2150: ';' expected after '\"'\n",
      "Skipping line 42: Expected 4 fields in line 42, saw 7\n",
      "Skipping line 60: Expected 4 fields in line 60, saw 8\n",
      "Skipping line 157: Expected 4 fields in line 157, saw 5\n",
      "Skipping line 207: Expected 4 fields in line 207, saw 6\n",
      "Skipping line 210: Expected 4 fields in line 210, saw 5\n",
      "Skipping line 275: Expected 4 fields in line 275, saw 5\n",
      "Skipping line 293: Expected 4 fields in line 293, saw 5\n",
      "Skipping line 341: Expected 4 fields in line 341, saw 7\n",
      "Skipping line 357: Expected 4 fields in line 357, saw 5\n",
      "Skipping line 376: Expected 4 fields in line 376, saw 5\n",
      "Skipping line 469: Expected 4 fields in line 469, saw 6\n",
      "Skipping line 478: Expected 4 fields in line 478, saw 5\n",
      "Skipping line 494: Expected 4 fields in line 494, saw 5\n",
      "Skipping line 496: Expected 4 fields in line 496, saw 6\n",
      "Skipping line 504: Expected 4 fields in line 504, saw 5\n",
      "Skipping line 512: Expected 4 fields in line 512, saw 5\n",
      "Skipping line 523: Expected 4 fields in line 523, saw 6\n",
      "Skipping line 550: Expected 4 fields in line 550, saw 5\n",
      "Skipping line 606: Expected 4 fields in line 606, saw 8\n",
      "Skipping line 723: Expected 4 fields in line 723, saw 5\n",
      "Skipping line 762: Expected 4 fields in line 762, saw 5\n",
      "Skipping line 764: Expected 4 fields in line 764, saw 8\n",
      "Skipping line 800: Expected 4 fields in line 800, saw 5\n",
      "Skipping line 801: Expected 4 fields in line 801, saw 6\n",
      "Skipping line 821: Expected 4 fields in line 821, saw 5\n",
      "Skipping line 845: Expected 4 fields in line 845, saw 5\n",
      "Skipping line 895: Expected 4 fields in line 895, saw 6\n",
      "Skipping line 927: Expected 4 fields in line 927, saw 5\n",
      "Skipping line 937: Expected 4 fields in line 937, saw 5\n",
      "Skipping line 952: Expected 4 fields in line 952, saw 10\n",
      "Skipping line 988: Expected 4 fields in line 988, saw 5\n",
      "Skipping line 1063: Expected 4 fields in line 1063, saw 5\n",
      "Skipping line 1081: Expected 4 fields in line 1081, saw 5\n",
      "Skipping line 1112: Expected 4 fields in line 1112, saw 5\n",
      "Skipping line 1135: Expected 4 fields in line 1135, saw 5\n",
      "Skipping line 1164: Expected 4 fields in line 1164, saw 5\n",
      "Skipping line 1169: Expected 4 fields in line 1169, saw 5\n",
      "Skipping line 1197: Expected 4 fields in line 1197, saw 5\n",
      "Skipping line 1252: Expected 4 fields in line 1252, saw 5\n",
      "Skipping line 1259: Expected 4 fields in line 1259, saw 5\n",
      "Skipping line 1275: Expected 4 fields in line 1275, saw 6\n",
      "Skipping line 1289: Expected 4 fields in line 1289, saw 5\n",
      "Skipping line 1299: Expected 4 fields in line 1299, saw 6\n",
      "Skipping line 1338: Expected 4 fields in line 1338, saw 5\n",
      "Skipping line 1344: Expected 4 fields in line 1344, saw 5\n",
      "Skipping line 1372: Expected 4 fields in line 1372, saw 5\n",
      "Skipping line 1383: Expected 4 fields in line 1383, saw 5\n",
      "Skipping line 1393: Expected 4 fields in line 1393, saw 9\n",
      "Skipping line 1410: Expected 4 fields in line 1410, saw 5\n",
      "Skipping line 1526: Expected 4 fields in line 1526, saw 5\n",
      "Skipping line 1529: Expected 4 fields in line 1529, saw 6\n",
      "Skipping line 1577: Expected 4 fields in line 1577, saw 5\n",
      "Skipping line 1596: Expected 4 fields in line 1596, saw 5\n",
      "Skipping line 1603: Expected 4 fields in line 1603, saw 5\n",
      "Skipping line 1719: Expected 4 fields in line 1719, saw 5\n",
      "Skipping line 1738: Expected 4 fields in line 1738, saw 6\n",
      "Skipping line 1790: Expected 4 fields in line 1790, saw 5\n",
      "Skipping line 1807: Expected 4 fields in line 1807, saw 5\n",
      "Skipping line 1813: Expected 4 fields in line 1813, saw 5\n",
      "Skipping line 2011: Expected 4 fields in line 2011, saw 6\n",
      "Skipping line 2099: Expected 4 fields in line 2099, saw 5\n",
      "Skipping line 2110: Expected 4 fields in line 2110, saw 8\n",
      "Skipping line 2166: Expected 4 fields in line 2166, saw 6\n",
      "Skipping line 2176: Expected 4 fields in line 2176, saw 5\n",
      "Skipping line 2327: Expected 4 fields in line 2327, saw 5\n",
      "Skipping line 2339: Expected 4 fields in line 2339, saw 5\n",
      "Skipping line 2424: Expected 4 fields in line 2424, saw 5\n",
      "Skipping line 2458: Expected 4 fields in line 2458, saw 6\n",
      "Skipping line 2492: Expected 4 fields in line 2492, saw 5\n",
      "Skipping line 2523: Expected 4 fields in line 2523, saw 5\n",
      "Skipping line 2551: Expected 4 fields in line 2551, saw 5\n",
      "Skipping line 2563: Expected 4 fields in line 2563, saw 7\n",
      "Skipping line 2594: Expected 4 fields in line 2594, saw 5\n",
      "Skipping line 2636: Expected 4 fields in line 2636, saw 5\n",
      "Skipping line 2648: Expected 4 fields in line 2648, saw 5\n",
      "Skipping line 2682: Expected 4 fields in line 2682, saw 5\n",
      "Skipping line 2724: Expected 4 fields in line 2724, saw 5\n",
      "Skipping line 2827: Expected 4 fields in line 2827, saw 5\n",
      "Skipping line 2953: Expected 4 fields in line 2953, saw 5\n",
      "Skipping line 2966: Expected 4 fields in line 2966, saw 11\n",
      "Skipping line 2987: Expected 4 fields in line 2987, saw 5\n",
      "Skipping line 2996: Expected 4 fields in line 2996, saw 5\n",
      "Skipping line 2997: Expected 4 fields in line 2997, saw 5\n",
      "Skipping line 3000: Expected 4 fields in line 3000, saw 8\n",
      "Skipping line 3040: Expected 4 fields in line 3040, saw 5\n",
      "Skipping line 3049: Expected 4 fields in line 3049, saw 18\n",
      "Skipping line 3060: Expected 4 fields in line 3060, saw 5\n",
      "Skipping line 3065: Expected 4 fields in line 3065, saw 5\n",
      "Skipping line 3077: Expected 4 fields in line 3077, saw 5\n",
      "Skipping line 3090: Expected 4 fields in line 3090, saw 5\n",
      "Skipping line 3120: Expected 4 fields in line 3120, saw 5\n",
      "Skipping line 3172: Expected 4 fields in line 3172, saw 5\n",
      "Skipping line 3174: Expected 4 fields in line 3174, saw 5\n",
      "Skipping line 3243: Expected 4 fields in line 3243, saw 5\n",
      "Skipping line 3263: Expected 4 fields in line 3263, saw 5\n",
      "Skipping line 3332: Expected 4 fields in line 3332, saw 5\n",
      "Skipping line 3352: Expected 4 fields in line 3352, saw 17\n",
      "Skipping line 3393: Expected 4 fields in line 3393, saw 5\n",
      "Skipping line 3467: Expected 4 fields in line 3467, saw 5\n",
      "Skipping line 3506: Expected 4 fields in line 3506, saw 5\n",
      "Skipping line 3514: Expected 4 fields in line 3514, saw 5\n",
      "Skipping line 3525: Expected 4 fields in line 3525, saw 5\n",
      "Skipping line 3537: Expected 4 fields in line 3537, saw 5\n",
      "Skipping line 3625: Expected 4 fields in line 3625, saw 5\n",
      "Skipping line 3697: Expected 4 fields in line 3697, saw 5\n",
      "Skipping line 3703: Expected 4 fields in line 3703, saw 6\n",
      "Skipping line 3769: Expected 4 fields in line 3769, saw 5\n",
      "Skipping line 3790: Expected 4 fields in line 3790, saw 5\n",
      "Skipping line 3807: Expected 4 fields in line 3807, saw 5\n",
      "Skipping line 3812: Expected 4 fields in line 3812, saw 5\n",
      "Skipping line 3819: Expected 4 fields in line 3819, saw 6\n",
      "Skipping line 3828: Expected 4 fields in line 3828, saw 5\n",
      "Skipping line 3835: Expected 4 fields in line 3835, saw 5\n",
      "Skipping line 3838: Expected 4 fields in line 3838, saw 6\n",
      "Skipping line 3841: Expected 4 fields in line 3841, saw 6\n",
      "Skipping line 3849: Expected 4 fields in line 3849, saw 5\n",
      "Skipping line 3876: Expected 4 fields in line 3876, saw 6\n",
      "Skipping line 3886: Expected 4 fields in line 3886, saw 5\n",
      "Skipping line 3890: Expected 4 fields in line 3890, saw 5\n",
      "Skipping line 3942: Expected 4 fields in line 3942, saw 5\n",
      "Skipping line 3996: Expected 4 fields in line 3996, saw 5\n",
      "Skipping line 4134: Expected 4 fields in line 4134, saw 5\n",
      "Skipping line 4204: Expected 4 fields in line 4204, saw 5\n",
      "Skipping line 4207: Expected 4 fields in line 4207, saw 5\n",
      "Skipping line 4225: Expected 4 fields in line 4225, saw 5\n",
      "Skipping line 4230: Expected 4 fields in line 4230, saw 9\n",
      "Skipping line 4300: Expected 4 fields in line 4300, saw 6\n",
      "Skipping line 4371: Expected 4 fields in line 4371, saw 6\n",
      "Skipping line 4375: Expected 4 fields in line 4375, saw 6\n",
      "Skipping line 4414: Expected 4 fields in line 4414, saw 18\n",
      "Skipping line 4454: Expected 4 fields in line 4454, saw 5\n",
      "Skipping line 4465: Expected 4 fields in line 4465, saw 6\n",
      "Skipping line 4561: Expected 4 fields in line 4561, saw 5\n",
      "Skipping line 4658: Expected 4 fields in line 4658, saw 5\n",
      "Skipping line 4667: Expected 4 fields in line 4667, saw 5\n",
      "Skipping line 4673: Expected 4 fields in line 4673, saw 5\n",
      "Skipping line 4674: Expected 4 fields in line 4674, saw 5\n",
      "Skipping line 4677: Expected 4 fields in line 4677, saw 5\n",
      "Skipping line 4734: Expected 4 fields in line 4734, saw 5\n",
      "Skipping line 4748: Expected 4 fields in line 4748, saw 6\n",
      "Skipping line 4764: Expected 4 fields in line 4764, saw 5\n",
      "Skipping line 4776: Expected 4 fields in line 4776, saw 5\n",
      "Skipping line 4784: Expected 4 fields in line 4784, saw 5\n",
      "Skipping line 4841: Expected 4 fields in line 4841, saw 6\n",
      "Skipping line 4859: Expected 4 fields in line 4859, saw 5\n",
      "Skipping line 4889: Expected 4 fields in line 4889, saw 5\n",
      "Skipping line 4897: Expected 4 fields in line 4897, saw 5\n",
      "Skipping line 4943: Expected 4 fields in line 4943, saw 5\n",
      "Skipping line 4955: Expected 4 fields in line 4955, saw 5\n",
      "Skipping line 5035: Expected 4 fields in line 5035, saw 6\n",
      "Skipping line 5051: Expected 4 fields in line 5051, saw 5\n",
      "Skipping line 5060: Expected 4 fields in line 5060, saw 8\n",
      "Skipping line 5089: Expected 4 fields in line 5089, saw 5\n",
      "Skipping line 5106: Expected 4 fields in line 5106, saw 6\n",
      "Skipping line 5124: Expected 4 fields in line 5124, saw 5\n",
      "Skipping line 5160: Expected 4 fields in line 5160, saw 5\n",
      "Skipping line 5170: Expected 4 fields in line 5170, saw 5\n",
      "Skipping line 5173: Expected 4 fields in line 5173, saw 5\n",
      "Skipping line 5218: Expected 4 fields in line 5218, saw 5\n",
      "Skipping line 5261: Expected 4 fields in line 5261, saw 8\n",
      "Skipping line 5268: Expected 4 fields in line 5268, saw 5\n",
      "Skipping line 5270: Expected 4 fields in line 5270, saw 6\n",
      "Skipping line 5397: Expected 4 fields in line 5397, saw 6\n",
      "Skipping line 5431: Expected 4 fields in line 5431, saw 5\n",
      "Skipping line 5432: Expected 4 fields in line 5432, saw 5\n",
      "Skipping line 5481: Expected 4 fields in line 5481, saw 5\n",
      "Skipping line 5547: Expected 4 fields in line 5547, saw 5\n",
      "Skipping line 5568: Expected 4 fields in line 5568, saw 7\n",
      "Skipping line 5598: Expected 4 fields in line 5598, saw 5\n",
      "Skipping line 5601: Expected 4 fields in line 5601, saw 5\n",
      "Skipping line 5615: Expected 4 fields in line 5615, saw 6\n",
      "Skipping line 5616: Expected 4 fields in line 5616, saw 5\n",
      "Skipping line 5653: Expected 4 fields in line 5653, saw 5\n",
      "Skipping line 5655: Expected 4 fields in line 5655, saw 5\n",
      "Skipping line 5716: Expected 4 fields in line 5716, saw 5\n",
      "Skipping line 5729: Expected 4 fields in line 5729, saw 5\n",
      "Skipping line 5753: Expected 4 fields in line 5753, saw 5\n",
      "Skipping line 5808: Expected 4 fields in line 5808, saw 5\n",
      "Skipping line 5823: Expected 4 fields in line 5823, saw 5\n",
      "Skipping line 5824: Expected 4 fields in line 5824, saw 5\n",
      "Skipping line 5861: Expected 4 fields in line 5861, saw 5\n",
      "Skipping line 5871: Expected 4 fields in line 5871, saw 5\n",
      "Skipping line 5955: Expected 4 fields in line 5955, saw 11\n",
      "Skipping line 6012: Expected 4 fields in line 6012, saw 5\n",
      "Skipping line 6107: Expected 4 fields in line 6107, saw 5\n",
      "Skipping line 6113: Expected 4 fields in line 6113, saw 5\n",
      "Skipping line 6136: Expected 4 fields in line 6136, saw 6\n",
      "Skipping line 6154: Expected 4 fields in line 6154, saw 5\n",
      "Skipping line 6160: Expected 4 fields in line 6160, saw 5\n",
      "Skipping line 6184: Expected 4 fields in line 6184, saw 5\n",
      "Skipping line 6210: Expected 4 fields in line 6210, saw 5\n",
      "Skipping line 6370: Expected 4 fields in line 6370, saw 6\n",
      "Skipping line 6371: Expected 4 fields in line 6371, saw 5\n",
      "Skipping line 6394: Expected 4 fields in line 6394, saw 5\n",
      "Skipping line 6395: Expected 4 fields in line 6395, saw 5\n",
      "Skipping line 6414: Expected 4 fields in line 6414, saw 5\n",
      "Skipping line 6455: Expected 4 fields in line 6455, saw 7\n",
      "Skipping line 6474: Expected 4 fields in line 6474, saw 5\n",
      "Skipping line 6485: Expected 4 fields in line 6485, saw 7\n",
      "Skipping line 6510: Expected 4 fields in line 6510, saw 5\n",
      "Skipping line 6549: Expected 4 fields in line 6549, saw 6\n",
      "Skipping line 6585: Expected 4 fields in line 6585, saw 5\n",
      "Skipping line 6596: Expected 4 fields in line 6596, saw 5\n",
      "Skipping line 6601: Expected 4 fields in line 6601, saw 6\n",
      "Skipping line 6621: Expected 4 fields in line 6621, saw 5\n",
      "Skipping line 6665: Expected 4 fields in line 6665, saw 5\n",
      "Skipping line 6690: Expected 4 fields in line 6690, saw 5\n",
      "Skipping line 6712: Expected 4 fields in line 6712, saw 5\n",
      "Skipping line 6753: Expected 4 fields in line 6753, saw 7\n",
      "Skipping line 6865: Expected 4 fields in line 6865, saw 5\n",
      "Skipping line 6892: Expected 4 fields in line 6892, saw 5\n",
      "Skipping line 6900: Expected 4 fields in line 6900, saw 5\n",
      "Skipping line 6903: Expected 4 fields in line 6903, saw 5\n",
      "Skipping line 6912: Expected 4 fields in line 6912, saw 6\n",
      "Skipping line 6978: Expected 4 fields in line 6978, saw 5\n",
      "Skipping line 7045: Expected 4 fields in line 7045, saw 5\n",
      "Skipping line 7074: Expected 4 fields in line 7074, saw 5\n",
      "Skipping line 7148: Expected 4 fields in line 7148, saw 5\n",
      "Skipping line 7165: Expected 4 fields in line 7165, saw 5\n",
      "Skipping line 7168: Expected 4 fields in line 7168, saw 5\n",
      "Skipping line 7225: Expected 4 fields in line 7225, saw 5\n",
      "Skipping line 7233: Expected 4 fields in line 7233, saw 5\n",
      "Skipping line 7253: Expected 4 fields in line 7253, saw 5\n",
      "Skipping line 7283: Expected 4 fields in line 7283, saw 5\n",
      "Skipping line 7284: Expected 4 fields in line 7284, saw 5\n",
      "Skipping line 7315: Expected 4 fields in line 7315, saw 8\n",
      "Skipping line 7318: Expected 4 fields in line 7318, saw 5\n",
      "Skipping line 7324: Expected 4 fields in line 7324, saw 5\n",
      "Skipping line 7327: Expected 4 fields in line 7327, saw 5\n",
      "Skipping line 7378: Expected 4 fields in line 7378, saw 5\n",
      "Skipping line 7395: Expected 4 fields in line 7395, saw 5\n",
      "Skipping line 7422: Expected 4 fields in line 7422, saw 5\n",
      "Skipping line 7441: Expected 4 fields in line 7441, saw 5\n",
      "Skipping line 7450: Expected 4 fields in line 7450, saw 5\n",
      "Skipping line 7462: Expected 4 fields in line 7462, saw 5\n",
      "Skipping line 7465: Expected 4 fields in line 7465, saw 5\n",
      "Skipping line 7513: Expected 4 fields in line 7513, saw 5\n",
      "Skipping line 7561: Expected 4 fields in line 7561, saw 9\n",
      "Skipping line 7624: Expected 4 fields in line 7624, saw 5\n",
      "Skipping line 7654: Expected 4 fields in line 7654, saw 5\n",
      "Skipping line 7684: Expected 4 fields in line 7684, saw 6\n",
      "Skipping line 7692: Expected 4 fields in line 7692, saw 5\n",
      "Skipping line 7724: Expected 4 fields in line 7724, saw 12\n",
      "Skipping line 7769: Expected 4 fields in line 7769, saw 7\n",
      "Skipping line 7782: Expected 4 fields in line 7782, saw 6\n",
      "Skipping line 7813: Expected 4 fields in line 7813, saw 5\n",
      "Skipping line 7852: Expected 4 fields in line 7852, saw 5\n",
      "Skipping line 7861: Expected 4 fields in line 7861, saw 6\n",
      "Skipping line 7872: Expected 4 fields in line 7872, saw 5\n",
      "Skipping line 7874: Expected 4 fields in line 7874, saw 6\n",
      "Skipping line 7894: Expected 4 fields in line 7894, saw 5\n",
      "Skipping line 7915: Expected 4 fields in line 7915, saw 5\n",
      "Skipping line 8100: Expected 4 fields in line 8100, saw 5\n",
      "Skipping line 8238: Expected 4 fields in line 8238, saw 5\n",
      "Skipping line 8277: Expected 4 fields in line 8277, saw 5\n",
      "Skipping line 8610: Expected 4 fields in line 8610, saw 5\n",
      "Skipping line 9179: Expected 4 fields in line 9179, saw 5\n",
      "Skipping line 10203: Expected 4 fields in line 10203, saw 5\n",
      "Skipping line 10265: Expected 4 fields in line 10265, saw 5\n",
      "Skipping line 10297: Expected 4 fields in line 10297, saw 5\n",
      "Skipping line 10875: Expected 4 fields in line 10875, saw 5\n",
      "Skipping line 11066: Expected 4 fields in line 11066, saw 5\n",
      "Skipping line 11488: Expected 4 fields in line 11488, saw 5\n",
      "Skipping line 12067: Expected 4 fields in line 12067, saw 6\n",
      "Skipping line 12481: Expected 4 fields in line 12481, saw 5\n",
      "Skipping line 13243: Expected 4 fields in line 13243, saw 5\n",
      "Skipping line 13251: Expected 4 fields in line 13251, saw 5\n",
      "Skipping line 13413: Expected 4 fields in line 13413, saw 5\n",
      "Skipping line 13475: Expected 4 fields in line 13475, saw 5\n",
      "Skipping line 13503: Expected 4 fields in line 13503, saw 5\n",
      "Skipping line 13567: Expected 4 fields in line 13567, saw 5\n",
      "Skipping line 13638: Expected 4 fields in line 13638, saw 5\n",
      "Skipping line 13640: Expected 4 fields in line 13640, saw 5\n",
      "Skipping line 13944: Expected 4 fields in line 13944, saw 6\n",
      "Skipping line 13973: Expected 4 fields in line 13973, saw 6\n",
      "Skipping line 13975: Expected 4 fields in line 13975, saw 6\n",
      "Skipping line 568: Expected 5 fields in line 568, saw 6\n",
      "Skipping line 1968: Expected 5 fields in line 1968, saw 6\n",
      "Skipping line 41: Expected 4 fields in line 41, saw 6\n",
      "Skipping line 46: Expected 4 fields in line 46, saw 7\n",
      "Skipping line 66: Expected 4 fields in line 66, saw 6\n",
      "Skipping line 193: Expected 4 fields in line 193, saw 5\n",
      "Skipping line 194: Expected 4 fields in line 194, saw 5\n",
      "Skipping line 204: Expected 4 fields in line 204, saw 5\n",
      "Skipping line 218: Expected 4 fields in line 218, saw 6\n",
      "Skipping line 298: Expected 4 fields in line 298, saw 5\n",
      "Skipping line 315: Expected 4 fields in line 315, saw 6\n",
      "Skipping line 343: Expected 4 fields in line 343, saw 5\n",
      "Skipping line 361: Expected 4 fields in line 361, saw 5\n",
      "Skipping line 366: Expected 4 fields in line 366, saw 8\n",
      "Skipping line 424: Expected 4 fields in line 424, saw 5\n",
      "Skipping line 591: Expected 4 fields in line 591, saw 6\n",
      "Skipping line 618: Expected 4 fields in line 618, saw 8\n",
      "Skipping line 620: Expected 4 fields in line 620, saw 5\n",
      "Skipping line 668: Expected 4 fields in line 668, saw 7\n",
      "Skipping line 703: Expected 4 fields in line 703, saw 6\n",
      "Skipping line 716: Expected 4 fields in line 716, saw 5\n",
      "Skipping line 809: Expected 4 fields in line 809, saw 5\n",
      "Skipping line 898: Expected 4 fields in line 898, saw 5\n",
      "Skipping line 969: Expected 4 fields in line 969, saw 5\n",
      "Skipping line 976: Expected 4 fields in line 976, saw 6\n",
      "Skipping line 1032: Expected 4 fields in line 1032, saw 5\n",
      "Skipping line 1037: Expected 4 fields in line 1037, saw 6\n",
      "Skipping line 1040: Expected 4 fields in line 1040, saw 5\n",
      "Skipping line 1061: Expected 4 fields in line 1061, saw 5\n",
      "Skipping line 1076: Expected 4 fields in line 1076, saw 5\n",
      "Skipping line 1081: Expected 4 fields in line 1081, saw 5\n",
      "Skipping line 1082: Expected 4 fields in line 1082, saw 5\n",
      "Skipping line 1134: Expected 4 fields in line 1134, saw 5\n",
      "Skipping line 1142: Expected 4 fields in line 1142, saw 5\n",
      "Skipping line 1163: Expected 4 fields in line 1163, saw 6\n",
      "Skipping line 1185: Expected 4 fields in line 1185, saw 5\n",
      "Skipping line 1272: Expected 4 fields in line 1272, saw 5\n",
      "Skipping line 1423: Expected 4 fields in line 1423, saw 5\n",
      "Skipping line 1692: Expected 4 fields in line 1692, saw 5\n",
      "Skipping line 1986: Expected 4 fields in line 1986, saw 5\n",
      "Skipping line 2010: Expected 4 fields in line 2010, saw 5\n",
      "Skipping line 2067: Expected 4 fields in line 2067, saw 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipping line 145: Expected 4 fields in line 145, saw 5\n",
      "Skipping line 175: Expected 4 fields in line 175, saw 5\n",
      "Skipping line 187: Expected 4 fields in line 187, saw 5\n",
      "Skipping line 731: Expected 4 fields in line 731, saw 5\n",
      "Skipping line 1246: Expected 4 fields in line 1246, saw 5\n",
      "Skipping line 1312: Expected 4 fields in line 1312, saw 5\n",
      "Skipping line 1439: Expected 4 fields in line 1439, saw 5\n",
      "Skipping line 1581: Expected 4 fields in line 1581, saw 5\n",
      "Skipping line 1632: Expected 4 fields in line 1632, saw 5\n",
      "Skipping line 1637: Expected 4 fields in line 1637, saw 5\n",
      "Skipping line 1638: Expected 4 fields in line 1638, saw 5\n",
      "Skipping line 1639: Expected 4 fields in line 1639, saw 5\n",
      "Skipping line 1723: Expected 4 fields in line 1723, saw 5\n",
      "Skipping line 1967: Expected 4 fields in line 1967, saw 5\n",
      "Skipping line 2026: Expected 4 fields in line 2026, saw 5\n"
     ]
    }
   ],
   "source": [
    "vizwiz_features_train_color = pd.read_csv('azure_features_images/data/vizwiz_train_color_recognition.csv',\n",
    "                                    delimiter=';', engine='python', \n",
    "                                    dtype={'qid':str, 'question':str, 'descriptions':list,\n",
    "                                          'tags':list, 'dominant_colors':list},\n",
    "                                    quotechar='\"', error_bad_lines=False)\n",
    "vizwiz_features_train_text = pd.read_csv('azure_features_images/data/vizwiz_train_text_recognition.csv',\n",
    "                                    delimiter=';', engine='python', \n",
    "                                    dtype={'qid':str, 'question':str, 'descriptions':list,\n",
    "                                          'ocr_text':list, 'handwritten_text':list},\n",
    "                                    quotechar='\"', error_bad_lines=False)\n",
    "\n",
    "\n",
    "vizwiz_features_val_color = pd.read_csv('azure_features_images/data/vizwiz_val_color_recognition.csv',\n",
    "                                  delimiter=';', engine='python',\n",
    "                                  dtype={'qid':str, 'question':str, 'descriptions':list,\n",
    "                                          'tags':list, 'dominant_colors':list},\n",
    "                                  quotechar='\"', error_bad_lines=False)\n",
    "vizwiz_features_val_text = pd.read_csv('azure_features_images/data/vizwiz_val_text_recognition.csv',\n",
    "                                    delimiter=';', engine='python', \n",
    "                                    dtype={'qid':str, 'question':str, 'descriptions':list,\n",
    "                                          'ocr_text':list, 'handwritten_text':list},\n",
    "                                    quotechar='\"', error_bad_lines=False)\n",
    "\n",
    "\n",
    "vqa_features_train_color = pd.read_csv('azure_features_images/data/vqa_train_color_recognition.csv',\n",
    "                                 delimiter=';', engine='python', \n",
    "                                 dtype={'qid':str, 'question':str, 'descriptions':list,\n",
    "                                          'tags':list, 'dominant_colors':list},\n",
    "                                 quotechar='\"', error_bad_lines=False)\n",
    "vqa_features_train_text = pd.read_csv('azure_features_images/data/vqa_train_text_recognition.csv',\n",
    "                                    delimiter=';', engine='python', \n",
    "                                    dtype={'qid':str, 'question':str, 'descriptions':list,\n",
    "                                          'ocr_text':list, 'handwritten_text':list},\n",
    "                                    quotechar='\"', error_bad_lines=False)\n",
    "vqa_features_val_color = pd.read_csv('azure_features_images/data/vqa_val_color_recognition.csv',\n",
    "                               delimiter=';', engine='python',\n",
    "                               dtype={'qid':str, 'question':str, 'descriptions':list,\n",
    "                                          'tags':list, 'dominant_colors':list},\n",
    "                               quotechar='\"', error_bad_lines=False)\n",
    "vqa_features_val_text = pd.read_csv('azure_features_images/data/vqa_val_text_recognition.csv',\n",
    "                                    delimiter=';', engine='python', \n",
    "                                    dtype={'qid':str, 'question':str, 'descriptions':list,\n",
    "                                          'ocr_text':list, 'handwritten_text':list},\n",
    "                                    quotechar='\"', error_bad_lines=False)\n",
    "\n",
    "\n",
    "vizwiz_targets_train = pd.read_csv('../vizwiz_skill_typ_train.csv', dtype={'QID':str},\n",
    "                                   delimiter=',', quotechar='\"',\n",
    "                                   engine='python', error_bad_lines=False)\n",
    "vizwiz_targets_val = pd.read_csv('../vizwiz_skill_typ_val.csv', dtype={'QID':str},\n",
    "                                 delimiter=',', quotechar='\"', engine='python', error_bad_lines=False)\n",
    "vqa_targets_train = pd.read_csv('../vqa_skill_typ_train.csv', dtype={'QID':str},\n",
    "                               engine='python', quotechar='\"', error_bad_lines=False)\n",
    "vqa_targets_val = pd.read_csv('../vqa_skill_typ_val.csv', dtype={'QID':str},\n",
    "                               engine='python', quotechar='\"', error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>question</th>\n",
       "      <th>ocr_text</th>\n",
       "      <th>handwritten_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>VizWiz_train_000000000000.jpg</td>\n",
       "      <td>What's the name of this product?</td>\n",
       "      <td>['b', 'sil', 'leaves', '0.62', 'oz', '(170)']</td>\n",
       "      <td>['NET WT O. 62 02 ( 179)']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>VizWiz_train_000000000001.jpg</td>\n",
       "      <td>Can you tell me what is in this can please?</td>\n",
       "      <td>[]</td>\n",
       "      <td>['^TAKE Three 1^']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             qid                                     question  \\\n",
       "0  VizWiz_train_000000000000.jpg             What's the name of this product?   \n",
       "1  VizWiz_train_000000000001.jpg  Can you tell me what is in this can please?   \n",
       "\n",
       "                                        ocr_text            handwritten_text  \n",
       "0  ['b', 'sil', 'leaves', '0.62', 'oz', '(170)']  ['NET WT O. 62 02 ( 179)']  \n",
       "1                                             []          ['^TAKE Three 1^']  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vizwiz_features_train_text.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vizwiz_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-222dc4e49dac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvizwiz_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'vizwiz_train' is not defined"
     ]
    }
   ],
   "source": [
    "vizwiz_train.sample(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_feature_target(feature_df_text, feature_df_color, target_df):\n",
    "    feature_text = copy.deepcopy(feature_df_text)\n",
    "    feature_color = copy.deepcopy(feature_df_color)\n",
    "    target = copy.deepcopy(target_df)\n",
    "    # text features \n",
    "    feature_text.rename({'qid': 'QID'}, axis=1, inplace=True)\n",
    "    feature_text.set_index('QID', inplace=True)\n",
    "    # color features\n",
    "    feature_color.rename({'qid': 'QID'}, axis=1, inplace=True)\n",
    "    feature_color.set_index('QID', inplace=True)\n",
    "    # join features\n",
    "    features = feature_text.join(feature_color[['descriptions','tags','dominant_colors']],\n",
    "                                 on='QID',\n",
    "                                 how='outer')\n",
    "    # join features with target\n",
    "    target = target[['QID', 'IMG', 'QSN', 'TXT', 'OBJ', 'COL', 'CNT', 'OTH']]\n",
    "    target.set_index('QID', inplace=True)\n",
    "    target = target.astype(dtype=str)\n",
    "    df = target.join(features, on='QID', how='inner')\n",
    "    df['descriptions'].astype(list)\n",
    "    return df\n",
    "\n",
    "def lem(s):\n",
    "    arr = s.split(\" \")\n",
    "    lem = WordNetLemmatizer()\n",
    "    op = \"\"\n",
    "    for w in arr:\n",
    "        word = lem.lemmatize(w) + ' '\n",
    "        op += word\n",
    "    return op\n",
    "\n",
    "def preprocess_text(feature_columns):\n",
    "    \"\"\" output an nparray with single document per data point \"\"\"\n",
    "    ip = copy.deepcopy(feature_columns).values\n",
    "    op = []\n",
    "    for i in range(ip.shape[0]):\n",
    "        doc      =  \"\"\n",
    "        for j in range(ip.shape[1]):\n",
    "            # clean up chars\n",
    "            s    =  str(ip[i][j])\n",
    "            s    =  s.translate({ord(c): \"\" for c in \"!@#$%^&*()[]{};:,./<>?\\|`~-=_+'\"}).lower() + \" \"\n",
    "            if j == 1:             # clean descriptions\n",
    "                s = re.sub(r'confidence\\s+\\d+', '', s)\n",
    "                s = re.sub(r'text', '', s)\n",
    "            # lexicon normalize\n",
    "            s    = lem(s)\n",
    "            doc  += s\n",
    "        op.append(doc)\n",
    "    op = np.asarray(op)\n",
    "    return op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14257, 13) (3230, 13) 17487\n",
      "(2247, 13) (513, 13) 2760\n"
     ]
    }
   ],
   "source": [
    "vizwiz_train   = join_feature_target(vizwiz_features_train_text, \n",
    "                                   vizwiz_features_train_color, \n",
    "                                   vizwiz_targets_train)\n",
    "vizwiz_val     = join_feature_target(vizwiz_features_val_text, \n",
    "                                 vizwiz_features_val_color, \n",
    "                                 vizwiz_targets_val)\n",
    "vqa_train      = join_feature_target(vqa_features_train_text, \n",
    "                                   vqa_features_train_color, \n",
    "                                   vqa_targets_train)\n",
    "vqa_val        = join_feature_target(vizwiz_features_val_text, \n",
    "                                 vqa_features_val_color, \n",
    "                                 vqa_targets_val)\n",
    "print(vizwiz_train.shape, vqa_train.shape, vizwiz_train.shape[0] + vqa_train.shape[0])\n",
    "print(vizwiz_val.shape, vqa_val.shape, vizwiz_val.shape[0] + vqa_val.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: (14257, 13)\n",
      "Validation: (2247, 13)\n"
     ]
    }
   ],
   "source": [
    "# VizWiz only\n",
    "train = vizwiz_train\n",
    "val = vizwiz_val\n",
    "print(\"Training: {}\\nValidation: {}\".format(train.shape, val.shape))\n",
    "\n",
    "features_train = preprocess_text(train[['QSN','descriptions', 'tags', 'dominant_colors', \n",
    "                                        'handwritten_text', 'ocr_text']])\n",
    "txt_train      = train['TXT'].values\n",
    "col_train      = train['COL'].values\n",
    "cnt_train      = train['CNT'].values\n",
    "\n",
    "features_val   = preprocess_text(val[['QSN', 'descriptions', 'tags', 'dominant_colors',\n",
    "                                      'handwritten_text', 'ocr_text']])\n",
    "txt_val        = val['TXT'].values.astype('float32')\n",
    "col_val        = val['COL'].values.astype('float32')\n",
    "cnt_val        = val['CNT'].values.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: (17487, 13)\n",
      "Validation: (2760, 13)\n"
     ]
    }
   ],
   "source": [
    "# Combining Vizwiz and VQA (create X and Y)\n",
    "\n",
    "train = pd.concat([vizwiz_train, vqa_train], axis=0)\n",
    "val   = pd.concat([vizwiz_val, vqa_val], axis=0)\n",
    "print(\"Training: {}\\nValidation: {}\".format(train.shape, val.shape))\n",
    "\n",
    "features_train = preprocess_text(train[['QSN','descriptions', 'tags', 'dominant_colors', \n",
    "                                        'handwritten_text', 'ocr_text']])\n",
    "txt_train      = train['TXT'].values\n",
    "col_train      = train['COL'].values\n",
    "cnt_train      = train['CNT'].values\n",
    "\n",
    "features_val   = preprocess_text(val[['QSN', 'descriptions', 'tags', 'dominant_colors',\n",
    "                                      'handwritten_text', 'ocr_text']])\n",
    "txt_val        = val['TXT'].values.astype('float32')\n",
    "col_val        = val['COL'].values.astype('float32')\n",
    "cnt_val        = val['CNT'].values.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'what kind of pudding is this   a person holding a remote control   person indoor remote sitting holding man orange control red television boy girl young bed playing food table room video game white standing shirt  black      '"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_train[random.randint(0,len(features_val))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i know this is healthy choice but what dinner is it   a close up of food on a table   indoor table food sitting green plate restaurant wooden sandwich white  black brown  healthy choice complete meall with dessert solwin healthy  complete meal healthy with dessert •hoice 9g fiber 310 calorie homestyle bury salis steak salisbury steak in sautéed onion red apple ca crisp multigrain  '"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_val[random.randint(0,len(features_val))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tokenize\n",
    "tok        = Tokenizer(num_words=VOCAB_SIZE, \n",
    "                       filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "                       lower=True,\n",
    "                       split=\" \")\n",
    "tok.fit_on_texts(features_train)\n",
    "\n",
    "# create sequences & pad\n",
    "train_seq  = tok.texts_to_sequences(features_train)\n",
    "train_seq  = sequence.pad_sequences(train_seq, maxlen=MAX_DOC_LEN)\n",
    "val_seq    = tok.texts_to_sequences(features_val)\n",
    "val_seq    = sequence.pad_sequences(val_seq, maxlen=MAX_DOC_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples each class: \n",
      "Text recognition [6247. 8010.]\n",
      "Color recognition [8844. 5413.]\n"
     ]
    }
   ],
   "source": [
    "# check class distribution\n",
    "text_recognition_labels = to_categorical(np.asarray(txt_train)).astype('float32')\n",
    "color_recognition_labels = to_categorical(np.asarray(col_train)).astype('float32')\n",
    "print('Number of samples each class: ')\n",
    "print('Text recognition', text_recognition_labels.sum(axis=0))\n",
    "print('Color recognition', color_recognition_labels.sum(axis=0))\n",
    "n_classes = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM model (skip-gram word2vec)\n",
    "config for CPU only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import requests\n",
    "import time\n",
    "import gzip\n",
    "\n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.models import Sequential, load_model\n",
    "from keras import regularizers\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional\n",
    "from keras.callbacks import History, CSVLogger\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import nltk \n",
    "import gensim\n",
    "import logging\n",
    "\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve\n",
    "\n",
    "MAX_DOC_LEN = 40\n",
    "VOCAB_SIZE = 3000\n",
    "EMBEDDING_DIM = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# punkt sentence level tokenizer\n",
    "sent_lst = []\n",
    "for doc in features_train:\n",
    "    sentences = nltk.tokenize.sent_tokenize(doc)\n",
    "    for sent in sentences:\n",
    "        word_lst = [w for w in nltk.tokenize.word_tokenize(sent) if w.isalnum()]\n",
    "        sent_lst.append(word_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-05 00:49:29,179 : INFO : collecting all words and their counts\n",
      "2019-03-05 00:49:29,180 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-03-05 00:49:29,245 : INFO : PROGRESS: at sentence #10000, processed 383294 words, keeping 24880 word types\n",
      "2019-03-05 00:49:29,273 : INFO : collected 31762 word types from a corpus of 541792 raw words and 14257 sentences\n",
      "2019-03-05 00:49:29,274 : INFO : Loading a fresh vocabulary\n",
      "2019-03-05 00:49:29,297 : INFO : effective_min_count=6 retains 3565 unique words (11% of original 31762, drops 28197)\n",
      "2019-03-05 00:49:29,298 : INFO : effective_min_count=6 leaves 501856 word corpus (92% of original 541792, drops 39936)\n",
      "2019-03-05 00:49:29,307 : INFO : deleting the raw counts dictionary of 31762 items\n",
      "2019-03-05 00:49:29,309 : INFO : sample=0.001 downsamples 67 most-common words\n",
      "2019-03-05 00:49:29,310 : INFO : downsampling leaves estimated 318802 word corpus (63.5% of prior 501856)\n",
      "2019-03-05 00:49:29,317 : INFO : estimated required memory for 3565 words and 100 dimensions: 4634500 bytes\n",
      "2019-03-05 00:49:29,318 : INFO : resetting layer weights\n",
      "2019-03-05 00:49:29,358 : INFO : training model with 6 workers on 3565 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-03-05 00:49:29,852 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-03-05 00:49:29,864 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-03-05 00:49:29,868 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-05 00:49:29,869 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-05 00:49:29,872 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-05 00:49:29,879 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-05 00:49:29,879 : INFO : EPOCH - 1 : training on 541792 raw words (318874 effective words) took 0.5s, 621021 effective words/s\n",
      "2019-03-05 00:49:30,257 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-03-05 00:49:30,265 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-03-05 00:49:30,269 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-05 00:49:30,272 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-05 00:49:30,273 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-05 00:49:30,287 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-05 00:49:30,287 : INFO : EPOCH - 2 : training on 541792 raw words (318344 effective words) took 0.4s, 795354 effective words/s\n",
      "2019-03-05 00:49:30,667 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-03-05 00:49:30,671 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-03-05 00:49:30,675 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-05 00:49:30,681 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-05 00:49:30,688 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-05 00:49:30,689 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-05 00:49:30,690 : INFO : EPOCH - 3 : training on 541792 raw words (318286 effective words) took 0.4s, 802742 effective words/s\n",
      "2019-03-05 00:49:31,084 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-03-05 00:49:31,094 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-03-05 00:49:31,096 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-05 00:49:31,097 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-05 00:49:31,104 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-05 00:49:31,112 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-05 00:49:31,113 : INFO : EPOCH - 4 : training on 541792 raw words (318435 effective words) took 0.4s, 763340 effective words/s\n",
      "2019-03-05 00:49:31,485 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-03-05 00:49:31,497 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-03-05 00:49:31,497 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-05 00:49:31,500 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-05 00:49:31,503 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-05 00:49:31,507 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-05 00:49:31,507 : INFO : EPOCH - 5 : training on 541792 raw words (318997 effective words) took 0.4s, 821987 effective words/s\n",
      "2019-03-05 00:49:31,508 : INFO : training on a 2708960 raw words (1592936 effective words) took 2.1s, 741253 effective words/s\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "word2vec_model = gensim.models.Word2Vec(sentences=sent_lst,\n",
    "                                        min_count=6,\n",
    "                                        size=EMBEDDING_DIM,\n",
    "                                        sg=1,\n",
    "                                        workers=os.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 3565 word vectors\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "\n",
    "for word in word2vec_model.wv.vocab:\n",
    "    coefs = np.asarray(word2vec_model.wv[word], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "\n",
    "print('Total %s word vectors' % len(embeddings_index))\n",
    "\n",
    "# Initial word embedding\n",
    "embedding_matrix = np.zeros((VOCAB_SIZE, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in tok.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None and i < VOCAB_SIZE:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_create_train(labels, learning_rate, lstm_dim, batch_size, num_epochs, optimizer_param, regularization=1e-7):\n",
    "    \n",
    "    l2_reg = regularizers.l2(regularization)\n",
    "    \n",
    "    # init model\n",
    "    embedding_layer = Embedding(VOCAB_SIZE,\n",
    "                                EMBEDDING_DIM,\n",
    "                                input_length=MAX_DOC_LEN,\n",
    "                                trainable=True,\n",
    "                                mask_zero=False,\n",
    "                                embeddings_regularizer=l2_reg,\n",
    "                                weights=[embedding_matrix])\n",
    "    lstm_layer = LSTM(units=lstm_dim, kernel_regularizer=l2_reg)\n",
    "    dense_layer = Dense(n_classes,\n",
    "                        activation='softmax', \n",
    "                        kernel_regularizer=l2_reg)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(embedding_layer)\n",
    "    model.add(Bidirectional(lstm_layer))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(dense_layer)\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizer_param,\n",
    "                  metrics=['acc'])\n",
    "    history = History()\n",
    "    csv_logger = CSVLogger('./LSTM/text/{}_{}_{}_{}.log'.format(learning_rate, regularization, batch_size, num_epochs),\n",
    "                           separator=',',\n",
    "                           append=True)\n",
    "    t1 = time.time()\n",
    "    # model fit\n",
    "    model.fit(train_seq,\n",
    "              labels.astype('float32'),\n",
    "              batch_size=batch_size,\n",
    "              epochs=num_epochs,\n",
    "              callbacks=[history, csv_logger],\n",
    "              verbose=2)\n",
    "    t2 = time.time()\n",
    "    # save hdf5\n",
    "    model.save('./LSTM/text/{}_{}_{}_{}_model.h5'.format(learning_rate, regularization, batch_size, num_epochs))\n",
    "    np.savetxt('./LSTM/text/{}_{}_{}_{}_time.txt'.format(learning_rate, regularization, batch_size, num_epochs), \n",
    "               [regularization, (t2-t1) / 3600])\n",
    "    with open('./LSTM/text/{}_{}_{}_{}_history.txt'.format(learning_rate, regularization, batch_size, num_epochs), \"w\") as res_file:\n",
    "        res_file.write(str(history.history))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Vizwiz + VQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      " - 31s - loss: 0.4646 - acc: 0.7892\n",
      "Epoch 2/30\n",
      " - 31s - loss: 0.4124 - acc: 0.8254\n",
      "Epoch 3/30\n",
      " - 31s - loss: 0.3873 - acc: 0.8391\n",
      "Epoch 4/30\n",
      " - 31s - loss: 0.3728 - acc: 0.8457\n",
      "Epoch 5/30\n",
      " - 31s - loss: 0.3622 - acc: 0.8525\n",
      "Epoch 6/30\n",
      " - 31s - loss: 0.3563 - acc: 0.8542\n",
      "Epoch 7/30\n",
      " - 31s - loss: 0.3511 - acc: 0.8585\n",
      "Epoch 8/30\n",
      " - 31s - loss: 0.3448 - acc: 0.8593\n",
      "Epoch 9/30\n",
      " - 31s - loss: 0.3416 - acc: 0.8619\n",
      "Epoch 10/30\n",
      " - 31s - loss: 0.3355 - acc: 0.8674\n",
      "Epoch 11/30\n",
      " - 31s - loss: 0.3305 - acc: 0.8671\n",
      "Epoch 12/30\n",
      " - 31s - loss: 0.3310 - acc: 0.8672\n",
      "Epoch 13/30\n",
      " - 31s - loss: 0.3264 - acc: 0.8707\n",
      "Epoch 14/30\n",
      " - 31s - loss: 0.3200 - acc: 0.8730\n",
      "Epoch 15/30\n",
      " - 31s - loss: 0.3205 - acc: 0.8727\n",
      "Epoch 16/30\n",
      " - 31s - loss: 0.3173 - acc: 0.8753\n",
      "Epoch 17/30\n",
      " - 31s - loss: 0.3141 - acc: 0.8760\n",
      "Epoch 18/30\n",
      " - 31s - loss: 0.3098 - acc: 0.8788\n",
      "Epoch 19/30\n",
      " - 31s - loss: 0.3080 - acc: 0.8775\n",
      "Epoch 20/30\n",
      " - 31s - loss: 0.3061 - acc: 0.8792\n",
      "Epoch 21/30\n",
      " - 31s - loss: 0.2993 - acc: 0.8809\n",
      "Epoch 22/30\n",
      " - 31s - loss: 0.2998 - acc: 0.8821\n",
      "Epoch 23/30\n",
      " - 31s - loss: 0.2954 - acc: 0.8843\n",
      "Epoch 24/30\n",
      " - 31s - loss: 0.2935 - acc: 0.8838\n",
      "Epoch 25/30\n",
      " - 31s - loss: 0.2904 - acc: 0.8865\n",
      "Epoch 26/30\n",
      " - 31s - loss: 0.2881 - acc: 0.8876\n",
      "Epoch 27/30\n",
      " - 31s - loss: 0.2821 - acc: 0.8902\n",
      "Epoch 28/30\n",
      " - 31s - loss: 0.2780 - acc: 0.8932\n",
      "Epoch 29/30\n",
      " - 31s - loss: 0.2758 - acc: 0.8904\n",
      "Epoch 30/30\n",
      " - 31s - loss: 0.2710 - acc: 0.8934\n",
      "Learning rate: 0.2 Regularization: 1e-14 Batch size: 32 Epoch: 30\n",
      "Accuracy = 0.8496376811594203 \t AUC = 0.9012556082540055\n"
     ]
    }
   ],
   "source": [
    "L = 2e-1\n",
    "R = 1e-14\n",
    "B = 32\n",
    "E = 30\n",
    "lstm_create_train(labels=text_recognition_labels,\n",
    "                                  learning_rate=L,\n",
    "                                  lstm_dim=150, \n",
    "                                  batch_size=B,\n",
    "                                  num_epochs=E, \n",
    "                                  optimizer_param=SGD(lr=L, nesterov=True),\n",
    "                                  regularization=R)\n",
    "model = load_model('./LSTM/text/{}_{}_{}_{}_model.h5'.format(L,R,B,E))\n",
    "preds = model.predict(val_seq, verbose=0)\n",
    "print(\"Learning rate: {} Regularization: {} Batch size: {} Epoch: {}\".format(L,R,B,E))\n",
    "print((\"Accuracy = {0} \\t AUC = {1}\".format(accuracy_score(txt_val, preds.argmax(axis=1)), \n",
    "                                                            roc_auc_score(txt_val, preds[:,1]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      " - 57s - loss: 0.4622 - acc: 0.7857\n",
      "Epoch 2/30\n",
      " - 56s - loss: 0.4101 - acc: 0.8246\n",
      "Epoch 3/30\n",
      " - 55s - loss: 0.3879 - acc: 0.8401\n",
      "Epoch 4/30\n",
      " - 55s - loss: 0.3718 - acc: 0.8455\n",
      "Epoch 5/30\n",
      " - 55s - loss: 0.3643 - acc: 0.8506\n",
      "Epoch 6/30\n",
      " - 56s - loss: 0.3537 - acc: 0.8573\n",
      "Epoch 7/30\n",
      " - 55s - loss: 0.3517 - acc: 0.8567\n",
      "Epoch 8/30\n",
      " - 55s - loss: 0.3442 - acc: 0.8612\n",
      "Epoch 9/30\n",
      " - 55s - loss: 0.3401 - acc: 0.8641\n",
      "Epoch 10/30\n",
      " - 55s - loss: 0.3363 - acc: 0.8658\n",
      "Epoch 11/30\n",
      " - 55s - loss: 0.3346 - acc: 0.8656\n",
      "Epoch 12/30\n",
      " - 55s - loss: 0.3307 - acc: 0.8669\n",
      "Epoch 13/30\n",
      " - 55s - loss: 0.3275 - acc: 0.8704\n",
      "Epoch 14/30\n",
      " - 55s - loss: 0.3216 - acc: 0.8717\n",
      "Epoch 15/30\n",
      " - 55s - loss: 0.3189 - acc: 0.8728\n",
      "Epoch 16/30\n",
      " - 55s - loss: 0.3161 - acc: 0.8744\n",
      "Epoch 17/30\n",
      " - 55s - loss: 0.3161 - acc: 0.8721\n",
      "Epoch 18/30\n",
      " - 55s - loss: 0.3116 - acc: 0.8768\n",
      "Epoch 19/30\n",
      " - 55s - loss: 0.3083 - acc: 0.8791\n",
      "Epoch 20/30\n",
      " - 55s - loss: 0.3056 - acc: 0.8802\n",
      "Epoch 21/30\n",
      " - 57s - loss: 0.3023 - acc: 0.8831\n",
      "Epoch 22/30\n",
      " - 57s - loss: 0.3002 - acc: 0.8794\n",
      "Epoch 23/30\n",
      " - 56s - loss: 0.2948 - acc: 0.8845\n",
      "Epoch 24/30\n",
      " - 56s - loss: 0.2941 - acc: 0.8859\n",
      "Epoch 25/30\n",
      " - 56s - loss: 0.2927 - acc: 0.8844\n",
      "Epoch 26/30\n",
      " - 56s - loss: 0.2875 - acc: 0.8875\n",
      "Epoch 27/30\n",
      " - 56s - loss: 0.2817 - acc: 0.8894\n",
      "Epoch 28/30\n",
      " - 56s - loss: 0.2819 - acc: 0.8893\n",
      "Epoch 29/30\n",
      " - 56s - loss: 0.2767 - acc: 0.8902\n",
      "Epoch 30/30\n",
      " - 55s - loss: 0.2716 - acc: 0.8931\n",
      "Learning rate: 0.2 Regularization: 0 Batch size: 32 Epoch: 30\n",
      "Accuracy = 0.8467391304347827 \t AUC = 0.9062736302837314\n"
     ]
    }
   ],
   "source": [
    "# todo\n",
    "L = 2e-1\n",
    "R = 0\n",
    "B = 32\n",
    "E = 30\n",
    "lstm_create_train(labels=text_recognition_labels,\n",
    "                                  learning_rate=L,\n",
    "                                  lstm_dim=200, \n",
    "                                  batch_size=B,\n",
    "                                  num_epochs=E, \n",
    "                                  optimizer_param=SGD(lr=L, nesterov=True),\n",
    "                                  regularization=R)\n",
    "model = load_model('./LSTM/text/{}_{}_{}_{}_model.h5'.format(L,R,B,E))\n",
    "preds = model.predict(val_seq, verbose=0)\n",
    "print(\"Learning rate: {} Regularization: {} Batch size: {} Epoch: {}\".format(L,R,B,E))\n",
    "print((\"Accuracy = {0} \\t AUC = {1}\".format(accuracy_score(txt_val, preds.argmax(axis=1)), \n",
    "                                                            roc_auc_score(txt_val, preds[:,1]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and validation on VizWiz only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      " - 23s - loss: 0.4962 - acc: 0.7631\n",
      "Epoch 2/30\n",
      " - 23s - loss: 0.4303 - acc: 0.8098\n",
      "Epoch 3/30\n",
      " - 23s - loss: 0.4079 - acc: 0.8227\n",
      "Epoch 4/30\n",
      " - 23s - loss: 0.3894 - acc: 0.8347\n",
      "Epoch 5/30\n",
      " - 23s - loss: 0.3854 - acc: 0.8377\n",
      "Epoch 6/30\n",
      " - 23s - loss: 0.3775 - acc: 0.8409\n",
      "Epoch 7/30\n",
      " - 23s - loss: 0.3691 - acc: 0.8448\n",
      "Epoch 8/30\n",
      " - 23s - loss: 0.3630 - acc: 0.8495\n",
      "Epoch 9/30\n",
      " - 23s - loss: 0.3604 - acc: 0.8510\n",
      "Epoch 10/30\n",
      " - 23s - loss: 0.3538 - acc: 0.8542\n",
      "Epoch 11/30\n",
      " - 23s - loss: 0.3521 - acc: 0.8548\n",
      "Epoch 12/30\n",
      " - 23s - loss: 0.3485 - acc: 0.8579\n",
      "Epoch 13/30\n",
      " - 23s - loss: 0.3430 - acc: 0.8620\n",
      "Epoch 14/30\n",
      " - 23s - loss: 0.3418 - acc: 0.8604\n",
      "Epoch 15/30\n",
      " - 23s - loss: 0.3409 - acc: 0.8587\n",
      "Epoch 16/30\n",
      " - 23s - loss: 0.3366 - acc: 0.8641\n",
      "Epoch 17/30\n",
      " - 23s - loss: 0.3327 - acc: 0.8657\n",
      "Epoch 18/30\n",
      " - 23s - loss: 0.3311 - acc: 0.8687\n",
      "Epoch 19/30\n",
      " - 23s - loss: 0.3316 - acc: 0.8641\n",
      "Epoch 20/30\n",
      " - 23s - loss: 0.3307 - acc: 0.8673\n",
      "Epoch 21/30\n",
      " - 23s - loss: 0.3254 - acc: 0.8710\n",
      "Epoch 22/30\n",
      " - 23s - loss: 0.3231 - acc: 0.8688\n",
      "Epoch 23/30\n",
      " - 23s - loss: 0.3203 - acc: 0.8715\n",
      "Epoch 24/30\n",
      " - 23s - loss: 0.3228 - acc: 0.8684\n",
      "Epoch 25/30\n",
      " - 23s - loss: 0.3191 - acc: 0.8726\n",
      "Epoch 26/30\n",
      " - 23s - loss: 0.3174 - acc: 0.8726\n",
      "Epoch 27/30\n",
      " - 23s - loss: 0.3131 - acc: 0.8741\n",
      "Epoch 28/30\n",
      " - 23s - loss: 0.3146 - acc: 0.8754\n",
      "Epoch 29/30\n",
      " - 23s - loss: 0.3120 - acc: 0.8750\n",
      "Epoch 30/30\n",
      " - 23s - loss: 0.3079 - acc: 0.8788\n",
      "Learning rate: 0.1 Regularization: 0 Batch size: 32 Epoch: 30\n",
      "Accuracy = 0.8615932354250111 \t AUC = 0.9118524127205011\n"
     ]
    }
   ],
   "source": [
    "L = 1e-1\n",
    "R = 0\n",
    "B = 32\n",
    "E = 30\n",
    "lstm_create_train(labels=text_recognition_labels,\n",
    "                                  learning_rate=L,\n",
    "                                  lstm_dim=100, \n",
    "                                  batch_size=B,\n",
    "                                  num_epochs=E, \n",
    "                                  optimizer_param=SGD(lr=L, nesterov=True),\n",
    "                                  regularization=R)\n",
    "model = load_model('./LSTM/text/{}_{}_{}_{}_model.h5'.format(L,R,B,E))\n",
    "preds = model.predict(val_seq, verbose=0)\n",
    "print(\"Learning rate: {} Regularization: {} Batch size: {} Epoch: {}\".format(L,R,B,E))\n",
    "print((\"Accuracy = {0} \\t AUC = {1}\".format(accuracy_score(txt_val, preds.argmax(axis=1)), \n",
    "                                                            roc_auc_score(txt_val, preds[:,1]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      " - 50s - loss: 0.4917 - acc: 0.7624\n",
      "Epoch 2/30\n",
      " - 44s - loss: 0.4298 - acc: 0.8063\n",
      "Epoch 3/30\n"
     ]
    }
   ],
   "source": [
    "L = 1e-1\n",
    "R = 0\n",
    "B = 32\n",
    "E = 30\n",
    "lstm_create_train(labels=text_recognition_labels,\n",
    "                                  learning_rate=L,\n",
    "                                  lstm_dim=100, \n",
    "                                  batch_size=B,\n",
    "                                  num_epochs=E, \n",
    "                                  optimizer_param=SGD(lr=L, nesterov=True),\n",
    "                                  regularization=R)\n",
    "model = load_model('./LSTM/text/{}_{}_{}_{}_model.h5'.format(L,R,B,E))\n",
    "preds = model.predict(val_seq, verbose=0)\n",
    "print(\"Learning rate: {} Regularization: {} Batch size: {} Epoch: {}\".format(L,R,B,E))\n",
    "print((\"Accuracy = {0} \\t AUC = {1}\".format(accuracy_score(txt_val, preds.argmax(axis=1)), \n",
    "                                                            roc_auc_score(txt_val, preds[:,1]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
