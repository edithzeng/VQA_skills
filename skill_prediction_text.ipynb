{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skill label prediction with image-based features (text and color with descriptive tags)\n",
    "## Text recognition only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py35/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using CNTK backend\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/edithzeng/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/edithzeng/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import random\n",
    "import numpy as np\n",
    "import copy\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "kfold=KFold(n_splits=10)\n",
    "\n",
    "# for LSTM (keras with tf backend)\n",
    "import gzip\n",
    "import os\n",
    "import pickle\n",
    "import requests\n",
    "import time\n",
    "import re\n",
    "os.environ['KERAS_BACKEND']='cntk'\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.models import Sequential, load_model\n",
    "from keras import regularizers\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.initializers import he_normal\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional\n",
    "from keras.callbacks import History, CSVLogger\n",
    "from keras.utils import to_categorical\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "MAX_DOC_LEN = 40\n",
    "VOCAB_SIZE = 3000\n",
    "EMBEDDING_DIM = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vizwiz_features_train_color = pd.read_csv('azure_features_images/data/vizwiz_train_color_recognition.csv',\n",
    "                                    delimiter=';', engine='python', \n",
    "                                    dtype={'qid':str, 'question':str, 'descriptions':list,\n",
    "                                          'tags':list, 'dominant_colors':list},\n",
    "                                    quotechar='\"', error_bad_lines=False)\n",
    "vizwiz_features_train_text = pd.read_csv('azure_features_images/data/vizwiz_train_text_recognition.csv',\n",
    "                                    delimiter=';', engine='python', \n",
    "                                    dtype={'qid':str, 'question':str, 'descriptions':list,\n",
    "                                          'ocr_text':list, 'handwritten_text':list},\n",
    "                                    quotechar='\"', error_bad_lines=False)\n",
    "\n",
    "\n",
    "vizwiz_features_val_color = pd.read_csv('azure_features_images/data/vizwiz_val_color_recognition.csv',\n",
    "                                  delimiter=';', engine='python',\n",
    "                                  dtype={'qid':str, 'question':str, 'descriptions':list,\n",
    "                                          'tags':list, 'dominant_colors':list},\n",
    "                                  quotechar='\"', error_bad_lines=False)\n",
    "vizwiz_features_val_text = pd.read_csv('azure_features_images/data/vizwiz_val_text_recognition.csv',\n",
    "                                    delimiter=';', engine='python', \n",
    "                                    dtype={'qid':str, 'question':str, 'descriptions':list,\n",
    "                                          'ocr_text':list, 'handwritten_text':list},\n",
    "                                    quotechar='\"', error_bad_lines=False)\n",
    "\n",
    "\n",
    "vqa_features_train_color = pd.read_csv('azure_features_images/data/vqa_train_color_recognition.csv',\n",
    "                                 delimiter=';', engine='python', \n",
    "                                 dtype={'qid':str, 'question':str, 'descriptions':list,\n",
    "                                          'tags':list, 'dominant_colors':list},\n",
    "                                 quotechar='\"', error_bad_lines=False)\n",
    "vqa_features_train_text = pd.read_csv('azure_features_images/data/vqa_train_text_recognition.csv',\n",
    "                                    delimiter=';', engine='python', \n",
    "                                    dtype={'qid':str, 'question':str, 'descriptions':list,\n",
    "                                          'ocr_text':list, 'handwritten_text':list},\n",
    "                                    quotechar='\"', error_bad_lines=False)\n",
    "vqa_features_val_color = pd.read_csv('azure_features_images/data/vqa_val_color_recognition.csv',\n",
    "                               delimiter=';', engine='python',\n",
    "                               dtype={'qid':str, 'question':str, 'descriptions':list,\n",
    "                                          'tags':list, 'dominant_colors':list},\n",
    "                               quotechar='\"', error_bad_lines=False)\n",
    "vqa_features_val_text = pd.read_csv('azure_features_images/data/vqa_val_text_recognition.csv',\n",
    "                                    delimiter=';', engine='python', \n",
    "                                    dtype={'qid':str, 'question':str, 'descriptions':list,\n",
    "                                          'ocr_text':list, 'handwritten_text':list},\n",
    "                                    quotechar='\"', error_bad_lines=False)\n",
    "\n",
    "\n",
    "vizwiz_targets_train = pd.read_csv('../vizwiz_skill_typ_train.csv', dtype={'QID':str},\n",
    "                                   delimiter=',', quotechar='\"',\n",
    "                                   engine='python', error_bad_lines=False)\n",
    "vizwiz_targets_val = pd.read_csv('../vizwiz_skill_typ_val.csv', dtype={'QID':str},\n",
    "                                 delimiter=',', quotechar='\"', engine='python', error_bad_lines=False)\n",
    "vqa_targets_train = pd.read_csv('../vqa_skill_typ_train.csv', dtype={'QID':str},\n",
    "                               engine='python', quotechar='\"', error_bad_lines=False)\n",
    "vqa_targets_val = pd.read_csv('../vqa_skill_typ_val.csv', dtype={'QID':str},\n",
    "                               engine='python', quotechar='\"', error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vizwiz_features_train_text.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IMG</th>\n",
       "      <th>QSN</th>\n",
       "      <th>TXT</th>\n",
       "      <th>OBJ</th>\n",
       "      <th>COL</th>\n",
       "      <th>CNT</th>\n",
       "      <th>OTH</th>\n",
       "      <th>question</th>\n",
       "      <th>ocr_text</th>\n",
       "      <th>handwritten_text</th>\n",
       "      <th>descriptions</th>\n",
       "      <th>tags</th>\n",
       "      <th>dominant_colors</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>VizWiz_train_000000017125.jpg</th>\n",
       "      <td>VizWiz_train_000000017125.jpg</td>\n",
       "      <td>What is this?</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>What is this?</td>\n",
       "      <td>['\\\\einz', 'â€¢heart', '&amp;', 'st', 'rofk', 'du', ...</td>\n",
       "      <td>['HEINZ', '- CANADA FANCY -', 'ATO JUICE', '-C...</td>\n",
       "      <td>[{'text': 'a bottle of beer on a table', 'conf...</td>\n",
       "      <td>['indoor', 'bottle', 'table', 'sitting', 'coff...</td>\n",
       "      <td>['Grey', 'White', 'Black']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                         IMG            QSN  \\\n",
       "QID                                                                           \n",
       "VizWiz_train_000000017125.jpg  VizWiz_train_000000017125.jpg  What is this?   \n",
       "\n",
       "                              TXT OBJ COL CNT OTH       question  \\\n",
       "QID                                                                \n",
       "VizWiz_train_000000017125.jpg   1   1   0   0   0  What is this?   \n",
       "\n",
       "                                                                        ocr_text  \\\n",
       "QID                                                                                \n",
       "VizWiz_train_000000017125.jpg  ['\\\\einz', 'â€¢heart', '&', 'st', 'rofk', 'du', ...   \n",
       "\n",
       "                                                                handwritten_text  \\\n",
       "QID                                                                                \n",
       "VizWiz_train_000000017125.jpg  ['HEINZ', '- CANADA FANCY -', 'ATO JUICE', '-C...   \n",
       "\n",
       "                                                                    descriptions  \\\n",
       "QID                                                                                \n",
       "VizWiz_train_000000017125.jpg  [{'text': 'a bottle of beer on a table', 'conf...   \n",
       "\n",
       "                                                                            tags  \\\n",
       "QID                                                                                \n",
       "VizWiz_train_000000017125.jpg  ['indoor', 'bottle', 'table', 'sitting', 'coff...   \n",
       "\n",
       "                                          dominant_colors  \n",
       "QID                                                        \n",
       "VizWiz_train_000000017125.jpg  ['Grey', 'White', 'Black']  "
      ]
     },
     "execution_count": 406,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vizwiz_train.sample(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_feature_target(feature_df_text, feature_df_color, target_df):\n",
    "    feature_text = copy.deepcopy(feature_df_text)\n",
    "    feature_color = copy.deepcopy(feature_df_color)\n",
    "    target = copy.deepcopy(target_df)\n",
    "    # text features \n",
    "    feature_text.rename({'qid': 'QID'}, axis=1, inplace=True)\n",
    "    feature_text.set_index('QID', inplace=True)\n",
    "    # color features\n",
    "    feature_color.rename({'qid': 'QID'}, axis=1, inplace=True)\n",
    "    feature_color.set_index('QID', inplace=True)\n",
    "    # join features\n",
    "    features = feature_text.join(feature_color[['descriptions','tags','dominant_colors']],\n",
    "                                 on='QID',\n",
    "                                 how='outer')\n",
    "    # join features with target\n",
    "    target = target[['QID', 'IMG', 'QSN', 'TXT', 'OBJ', 'COL', 'CNT', 'OTH']]\n",
    "    target.set_index('QID', inplace=True)\n",
    "    target = target.astype(dtype=str)\n",
    "    df = target.join(features, on='QID', how='inner')\n",
    "    df['descriptions'].astype(list)\n",
    "    return df\n",
    "\n",
    "def lem(s):\n",
    "    arr = s.split(\" \")\n",
    "    lem = WordNetLemmatizer()\n",
    "    op = \"\"\n",
    "    for w in arr:\n",
    "        word = lem.lemmatize(w) + ' '\n",
    "        op += word\n",
    "    return op\n",
    "\n",
    "def preprocess_text(feature_columns):\n",
    "    \"\"\" output an nparray with single document per data point \"\"\"\n",
    "    ip = copy.deepcopy(feature_columns).values\n",
    "    op = []\n",
    "    for i in range(ip.shape[0]):\n",
    "        doc      =  \"\"\n",
    "        for j in range(ip.shape[1]):\n",
    "            # clean up chars\n",
    "            s    =  str(ip[i][j])\n",
    "            s    =  s.translate({ord(c): \"\" for c in \"!@#$%^&*()[]{};:,./<>?\\|`~-=_+'\"}).lower() + \" \"\n",
    "            if j == 1:             # clean descriptions\n",
    "                s = re.sub(r'confidence\\s+\\d+', '', s)\n",
    "                s = re.sub(r'text', '', s)\n",
    "            # lexicon normalize\n",
    "            s    = lem(s)\n",
    "            doc  += s\n",
    "        op.append(doc)\n",
    "    op = np.asarray(op)\n",
    "    return op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14257, 13) (3230, 13) 17487\n",
      "(2247, 13) (513, 13) 2760\n"
     ]
    }
   ],
   "source": [
    "vizwiz_train   = join_feature_target(vizwiz_features_train_text, \n",
    "                                   vizwiz_features_train_color, \n",
    "                                   vizwiz_targets_train)\n",
    "vizwiz_val     = join_feature_target(vizwiz_features_val_text, \n",
    "                                 vizwiz_features_val_color, \n",
    "                                 vizwiz_targets_val)\n",
    "vqa_train      = join_feature_target(vqa_features_train_text, \n",
    "                                   vqa_features_train_color, \n",
    "                                   vqa_targets_train)\n",
    "vqa_val        = join_feature_target(vizwiz_features_val_text, \n",
    "                                 vqa_features_val_color, \n",
    "                                 vqa_targets_val)\n",
    "print(vizwiz_train.shape, vqa_train.shape, vizwiz_train.shape[0] + vqa_train.shape[0])\n",
    "print(vizwiz_val.shape, vqa_val.shape, vizwiz_val.shape[0] + vqa_val.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: (17487, 13)\n",
      "Validation: (2760, 13)\n"
     ]
    }
   ],
   "source": [
    "# create X and Y\n",
    "\n",
    "train = pd.concat([vizwiz_train, vqa_train], axis=0)\n",
    "val   = pd.concat([vizwiz_val, vqa_val], axis=0)\n",
    "print(\"Training: {}\\nValidation: {}\".format(train.shape, val.shape))\n",
    "\n",
    "features_train = preprocess_text(train[['QSN','descriptions', 'tags', 'dominant_colors', \n",
    "                                        'handwritten_text', 'ocr_text']])\n",
    "txt_train      = train['TXT'].values\n",
    "col_train      = train['COL'].values\n",
    "cnt_train      = train['CNT'].values\n",
    "\n",
    "features_val   = preprocess_text(val[['QSN', 'descriptions', 'tags', 'dominant_colors',\n",
    "                                      'handwritten_text', 'ocr_text']])\n",
    "txt_val        = val['TXT'].values.astype('float32')\n",
    "col_val        = val['COL'].values.astype('float32')\n",
    "cnt_val        = val['CNT'].values.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'what kind of pudding is this   a person holding a remote control   person indoor remote sitting holding man orange control red television boy girl young bed playing food table room video game white standing shirt  black      '"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_train[random.randint(0,len(features_val))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i know this is healthy choice but what dinner is it   a close up of food on a table   indoor table food sitting green plate restaurant wooden sandwich white  black brown  healthy choice complete meall with dessert solwin healthy  complete meal healthy with dessert â€¢hoice 9g fiber 310 calorie homestyle bury salis steak salisbury steak in sautÃ©ed onion red apple ca crisp multigrain  '"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_val[random.randint(0,len(features_val))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tokenize\n",
    "tok        = Tokenizer(num_words=VOCAB_SIZE, \n",
    "                       filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "                       lower=True,\n",
    "                       split=\" \")\n",
    "tok.fit_on_texts(features_train)\n",
    "\n",
    "# create sequences & pad\n",
    "train_seq  = tok.texts_to_sequences(features_train)\n",
    "train_seq  = sequence.pad_sequences(train_seq, maxlen=MAX_DOC_LEN)\n",
    "val_seq    = tok.texts_to_sequences(features_val)\n",
    "val_seq    = sequence.pad_sequences(val_seq, maxlen=MAX_DOC_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples each class: \n",
      "Text recognition [9119. 8368.]\n",
      "Color recognition [10926.  6561.]\n"
     ]
    }
   ],
   "source": [
    "# check class distribution\n",
    "text_recognition_labels = to_categorical(np.asarray(txt_train)).astype('float32')\n",
    "color_recognition_labels = to_categorical(np.asarray(col_train)).astype('float32')\n",
    "print('Number of samples each class: ')\n",
    "print('Text recognition', text_recognition_labels.sum(axis=0))\n",
    "print('Color recognition', color_recognition_labels.sum(axis=0))\n",
    "n_classes = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM model (skip-gram word2vec)\n",
    "config for CPU only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import requests\n",
    "import time\n",
    "import gzip\n",
    "\n",
    "os.environ['KERAS_BACKEND'] = 'cntk'\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.models import Sequential, load_model\n",
    "from keras import regularizers\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional\n",
    "from keras.callbacks import History, CSVLogger\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import nltk \n",
    "import gensim\n",
    "import logging\n",
    "\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve\n",
    "\n",
    "MAX_DOC_LEN = 40\n",
    "VOCAB_SIZE = 3000\n",
    "EMBEDDING_DIM = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# punkt sentence level tokenizer\n",
    "sent_lst = []\n",
    "for doc in features_train:\n",
    "    sentences = nltk.tokenize.sent_tokenize(doc)\n",
    "    for sent in sentences:\n",
    "        word_lst = [w for w in nltk.tokenize.word_tokenize(sent) if w.isalnum()]\n",
    "        sent_lst.append(word_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-02-28 02:08:24,304 : INFO : collecting all words and their counts\n",
      "2019-02-28 02:08:24,305 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-02-28 02:08:24,370 : INFO : PROGRESS: at sentence #10000, processed 383294 words, keeping 24880 word types\n",
      "2019-02-28 02:08:24,417 : INFO : collected 33634 word types from a corpus of 671659 raw words and 17487 sentences\n",
      "2019-02-28 02:08:24,418 : INFO : Loading a fresh vocabulary\n",
      "2019-02-28 02:08:24,440 : INFO : effective_min_count=6 retains 3819 unique words (11% of original 33634, drops 29815)\n",
      "2019-02-28 02:08:24,441 : INFO : effective_min_count=6 leaves 629542 word corpus (93% of original 671659, drops 42117)\n",
      "2019-02-28 02:08:24,450 : INFO : deleting the raw counts dictionary of 33634 items\n",
      "2019-02-28 02:08:24,452 : INFO : sample=0.001 downsamples 68 most-common words\n",
      "2019-02-28 02:08:24,452 : INFO : downsampling leaves estimated 412555 word corpus (65.5% of prior 629542)\n",
      "2019-02-28 02:08:24,461 : INFO : estimated required memory for 3819 words and 100 dimensions: 4964700 bytes\n",
      "2019-02-28 02:08:24,461 : INFO : resetting layer weights\n",
      "2019-02-28 02:08:24,504 : INFO : training model with 6 workers on 3819 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-02-28 02:08:24,995 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-02-28 02:08:25,001 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-02-28 02:08:25,002 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-02-28 02:08:25,003 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-02-28 02:08:25,003 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-02-28 02:08:25,010 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-02-28 02:08:25,010 : INFO : EPOCH - 1 : training on 671659 raw words (412964 effective words) took 0.5s, 825203 effective words/s\n",
      "2019-02-28 02:08:25,515 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-02-28 02:08:25,518 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-02-28 02:08:25,519 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-02-28 02:08:25,535 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-02-28 02:08:25,549 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-02-28 02:08:25,556 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-02-28 02:08:25,556 : INFO : EPOCH - 2 : training on 671659 raw words (412906 effective words) took 0.5s, 764881 effective words/s\n",
      "2019-02-28 02:08:26,052 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-02-28 02:08:26,061 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-02-28 02:08:26,067 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-02-28 02:08:26,069 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-02-28 02:08:26,070 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-02-28 02:08:26,078 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-02-28 02:08:26,078 : INFO : EPOCH - 3 : training on 671659 raw words (412704 effective words) took 0.5s, 798971 effective words/s\n",
      "2019-02-28 02:08:26,561 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-02-28 02:08:26,566 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-02-28 02:08:26,569 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-02-28 02:08:26,572 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-02-28 02:08:26,576 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-02-28 02:08:26,587 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-02-28 02:08:26,587 : INFO : EPOCH - 4 : training on 671659 raw words (412997 effective words) took 0.5s, 821983 effective words/s\n",
      "2019-02-28 02:08:27,070 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-02-28 02:08:27,076 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-02-28 02:08:27,078 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-02-28 02:08:27,079 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-02-28 02:08:27,087 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-02-28 02:08:27,094 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-02-28 02:08:27,094 : INFO : EPOCH - 5 : training on 671659 raw words (412250 effective words) took 0.5s, 822596 effective words/s\n",
      "2019-02-28 02:08:27,095 : INFO : training on a 3358295 raw words (2063821 effective words) took 2.6s, 796741 effective words/s\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "word2vec_model = gensim.models.Word2Vec(sentences=sent_lst,\n",
    "                                        min_count=6,\n",
    "                                        size=EMBEDDING_DIM,\n",
    "                                        sg=1,\n",
    "                                        workers=os.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 3819 word vectors\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "\n",
    "for word in word2vec_model.wv.vocab:\n",
    "    coefs = np.asarray(word2vec_model.wv[word], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "\n",
    "print('Total %s word vectors' % len(embeddings_index))\n",
    "\n",
    "# Initial word embedding\n",
    "embedding_matrix = np.zeros((VOCAB_SIZE, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in tok.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None and i < VOCAB_SIZE:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_create_train(labels, learning_rate, lstm_dim, batch_size, num_epochs, optimizer_param, regularization=1e-7):\n",
    "    \n",
    "    l2_reg = regularizers.l2(regularization)\n",
    "    \n",
    "    # init model\n",
    "    embedding_layer = Embedding(VOCAB_SIZE,\n",
    "                                EMBEDDING_DIM,\n",
    "                                input_length=MAX_DOC_LEN,\n",
    "                                trainable=True,\n",
    "                                mask_zero=False,\n",
    "                                embeddings_regularizer=l2_reg,\n",
    "                                weights=[embedding_matrix])\n",
    "    lstm_layer = LSTM(units=lstm_dim, kernel_regularizer=l2_reg)\n",
    "    dense_layer = Dense(n_classes,\n",
    "                        activation='softmax', \n",
    "                        kernel_regularizer=l2_reg)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(embedding_layer)\n",
    "    model.add(Bidirectional(lstm_layer))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(dense_layer)\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizer_param,\n",
    "                  metrics=['acc'])\n",
    "    history = History()\n",
    "    csv_logger = CSVLogger('./LSTM/text/{}_{}_{}_{}.log'.format(learning_rate, regularization, batch_size, num_epochs),\n",
    "                           separator=',',\n",
    "                           append=True)\n",
    "    t1 = time.time()\n",
    "    # model fit\n",
    "    model.fit(train_seq,\n",
    "              labels.astype('float32'),\n",
    "              batch_size=batch_size,\n",
    "              epochs=num_epochs,\n",
    "              callbacks=[history, csv_logger],\n",
    "              verbose=2)\n",
    "    t2 = time.time()\n",
    "    # save hdf5\n",
    "    model.save('./LSTM/text/{}_{}_{}_{}_model.h5'.format(learning_rate, regularization, batch_size, num_epochs))\n",
    "    np.savetxt('./LSTM/text/{}_{}_{}_{}_time.txt'.format(learning_rate, regularization, batch_size, num_epochs), \n",
    "               [regularization, (t2-t1) / 3600])\n",
    "    with open('./LSTM/text/{}_{}_{}_{}_history.txt'.format(learning_rate, regularization, batch_size, num_epochs), \"w\") as res_file:\n",
    "        res_file.write(str(history.history))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      " - 31s - loss: 0.4646 - acc: 0.7892\n",
      "Epoch 2/30\n",
      " - 31s - loss: 0.4124 - acc: 0.8254\n",
      "Epoch 3/30\n",
      " - 31s - loss: 0.3873 - acc: 0.8391\n",
      "Epoch 4/30\n",
      " - 31s - loss: 0.3728 - acc: 0.8457\n",
      "Epoch 5/30\n",
      " - 31s - loss: 0.3622 - acc: 0.8525\n",
      "Epoch 6/30\n",
      " - 31s - loss: 0.3563 - acc: 0.8542\n",
      "Epoch 7/30\n",
      " - 31s - loss: 0.3511 - acc: 0.8585\n",
      "Epoch 8/30\n",
      " - 31s - loss: 0.3448 - acc: 0.8593\n",
      "Epoch 9/30\n",
      " - 31s - loss: 0.3416 - acc: 0.8619\n",
      "Epoch 10/30\n",
      " - 31s - loss: 0.3355 - acc: 0.8674\n",
      "Epoch 11/30\n",
      " - 31s - loss: 0.3305 - acc: 0.8671\n",
      "Epoch 12/30\n",
      " - 31s - loss: 0.3310 - acc: 0.8672\n",
      "Epoch 13/30\n",
      " - 31s - loss: 0.3264 - acc: 0.8707\n",
      "Epoch 14/30\n",
      " - 31s - loss: 0.3200 - acc: 0.8730\n",
      "Epoch 15/30\n",
      " - 31s - loss: 0.3205 - acc: 0.8727\n",
      "Epoch 16/30\n",
      " - 31s - loss: 0.3173 - acc: 0.8753\n",
      "Epoch 17/30\n",
      " - 31s - loss: 0.3141 - acc: 0.8760\n",
      "Epoch 18/30\n",
      " - 31s - loss: 0.3098 - acc: 0.8788\n",
      "Epoch 19/30\n",
      " - 31s - loss: 0.3080 - acc: 0.8775\n",
      "Epoch 20/30\n",
      " - 31s - loss: 0.3061 - acc: 0.8792\n",
      "Epoch 21/30\n",
      " - 31s - loss: 0.2993 - acc: 0.8809\n",
      "Epoch 22/30\n",
      " - 31s - loss: 0.2998 - acc: 0.8821\n",
      "Epoch 23/30\n",
      " - 31s - loss: 0.2954 - acc: 0.8843\n",
      "Epoch 24/30\n",
      " - 31s - loss: 0.2935 - acc: 0.8838\n",
      "Epoch 25/30\n",
      " - 31s - loss: 0.2904 - acc: 0.8865\n",
      "Epoch 26/30\n",
      " - 31s - loss: 0.2881 - acc: 0.8876\n",
      "Epoch 27/30\n",
      " - 31s - loss: 0.2821 - acc: 0.8902\n",
      "Epoch 28/30\n",
      " - 31s - loss: 0.2780 - acc: 0.8932\n",
      "Epoch 29/30\n",
      " - 31s - loss: 0.2758 - acc: 0.8904\n",
      "Epoch 30/30\n",
      " - 31s - loss: 0.2710 - acc: 0.8934\n",
      "Learning rate: 0.2 Regularization: 1e-14 Batch size: 32 Epoch: 30\n",
      "Accuracy = 0.8496376811594203 \t AUC = 0.9012556082540055\n"
     ]
    }
   ],
   "source": [
    "L = 2e-1\n",
    "R = 1e-14\n",
    "B = 32\n",
    "E = 30\n",
    "lstm_create_train(labels=text_recognition_labels,\n",
    "                                  learning_rate=L,\n",
    "                                  lstm_dim=150, \n",
    "                                  batch_size=B,\n",
    "                                  num_epochs=E, \n",
    "                                  optimizer_param=SGD(lr=L, nesterov=True),\n",
    "                                  regularization=R)\n",
    "model = load_model('./LSTM/text/{}_{}_{}_{}_model.h5'.format(L,R,B,E))\n",
    "preds = model.predict(val_seq, verbose=0)\n",
    "print(\"Learning rate: {} Regularization: {} Batch size: {} Epoch: {}\".format(L,R,B,E))\n",
    "print((\"Accuracy = {0} \\t AUC = {1}\".format(accuracy_score(txt_val, preds.argmax(axis=1)), \n",
    "                                                            roc_auc_score(txt_val, preds[:,1]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 2e-1\n",
    "R = 0\n",
    "B = 32\n",
    "E = 30\n",
    "lstm_create_train(labels=text_recognition_labels,\n",
    "                                  learning_rate=L,\n",
    "                                  lstm_dim=250, \n",
    "                                  batch_size=B,\n",
    "                                  num_epochs=E, \n",
    "                                  optimizer_param=SGD(lr=L, nesterov=True),\n",
    "                                  regularization=R)\n",
    "model = load_model('./LSTM/text/{}_{}_{}_{}_model.h5'.format(L,R,B,E))\n",
    "preds = model.predict(val_seq, verbose=0)\n",
    "print(\"Learning rate: {} Regularization: {} Batch size: {} Epoch: {}\".format(L,R,B,E))\n",
    "print((\"Accuracy = {0} \\t AUC = {1}\".format(accuracy_score(txt_val, preds.argmax(axis=1)), \n",
    "                                                            roc_auc_score(txt_val, preds[:,1]))))\n",
    "# todo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and validation on VizWiz only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      " - 23s - loss: 0.4962 - acc: 0.7631\n",
      "Epoch 2/30\n",
      " - 23s - loss: 0.4303 - acc: 0.8098\n",
      "Epoch 3/30\n",
      " - 23s - loss: 0.4079 - acc: 0.8227\n",
      "Epoch 4/30\n",
      " - 23s - loss: 0.3894 - acc: 0.8347\n",
      "Epoch 5/30\n",
      " - 23s - loss: 0.3854 - acc: 0.8377\n",
      "Epoch 6/30\n",
      " - 23s - loss: 0.3775 - acc: 0.8409\n",
      "Epoch 7/30\n",
      " - 23s - loss: 0.3691 - acc: 0.8448\n",
      "Epoch 8/30\n",
      " - 23s - loss: 0.3630 - acc: 0.8495\n",
      "Epoch 9/30\n",
      " - 23s - loss: 0.3604 - acc: 0.8510\n",
      "Epoch 10/30\n",
      " - 23s - loss: 0.3538 - acc: 0.8542\n",
      "Epoch 11/30\n",
      " - 23s - loss: 0.3521 - acc: 0.8548\n",
      "Epoch 12/30\n",
      " - 23s - loss: 0.3485 - acc: 0.8579\n",
      "Epoch 13/30\n",
      " - 23s - loss: 0.3430 - acc: 0.8620\n",
      "Epoch 14/30\n",
      " - 23s - loss: 0.3418 - acc: 0.8604\n",
      "Epoch 15/30\n",
      " - 23s - loss: 0.3409 - acc: 0.8587\n",
      "Epoch 16/30\n",
      " - 23s - loss: 0.3366 - acc: 0.8641\n",
      "Epoch 17/30\n",
      " - 23s - loss: 0.3327 - acc: 0.8657\n",
      "Epoch 18/30\n",
      " - 23s - loss: 0.3311 - acc: 0.8687\n",
      "Epoch 19/30\n",
      " - 23s - loss: 0.3316 - acc: 0.8641\n",
      "Epoch 20/30\n",
      " - 23s - loss: 0.3307 - acc: 0.8673\n",
      "Epoch 21/30\n",
      " - 23s - loss: 0.3254 - acc: 0.8710\n",
      "Epoch 22/30\n",
      " - 23s - loss: 0.3231 - acc: 0.8688\n",
      "Epoch 23/30\n",
      " - 23s - loss: 0.3203 - acc: 0.8715\n",
      "Epoch 24/30\n",
      " - 23s - loss: 0.3228 - acc: 0.8684\n",
      "Epoch 25/30\n",
      " - 23s - loss: 0.3191 - acc: 0.8726\n",
      "Epoch 26/30\n",
      " - 23s - loss: 0.3174 - acc: 0.8726\n",
      "Epoch 27/30\n",
      " - 23s - loss: 0.3131 - acc: 0.8741\n",
      "Epoch 28/30\n",
      " - 23s - loss: 0.3146 - acc: 0.8754\n",
      "Epoch 29/30\n",
      " - 23s - loss: 0.3120 - acc: 0.8750\n",
      "Epoch 30/30\n",
      " - 23s - loss: 0.3079 - acc: 0.8788\n",
      "Learning rate: 0.1 Regularization: 0 Batch size: 32 Epoch: 30\n",
      "Accuracy = 0.8615932354250111 \t AUC = 0.9118524127205011\n"
     ]
    }
   ],
   "source": [
    "L = 1e-1\n",
    "R = 0\n",
    "B = 32\n",
    "E = 30\n",
    "lstm_create_train(labels=text_recognition_labels,\n",
    "                                  learning_rate=L,\n",
    "                                  lstm_dim=100, \n",
    "                                  batch_size=B,\n",
    "                                  num_epochs=E, \n",
    "                                  optimizer_param=SGD(lr=L, nesterov=True),\n",
    "                                  regularization=R)\n",
    "model = load_model('./LSTM/text/{}_{}_{}_{}_model.h5'.format(L,R,B,E))\n",
    "preds = model.predict(val_seq, verbose=0)\n",
    "print(\"Learning rate: {} Regularization: {} Batch size: {} Epoch: {}\".format(L,R,B,E))\n",
    "print((\"Accuracy = {0} \\t AUC = {1}\".format(accuracy_score(txt_val, preds.argmax(axis=1)), \n",
    "                                                            roc_auc_score(txt_val, preds[:,1]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
