{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This file contains example using tools to evaluate the performances for color recognition category for VizWiz data.\n",
    "\n",
    "Evaluation tools from https://github.com/GT-Vision-Lab/VQA.\n",
    "\n",
    "Codes in eval_util.py from Yanan Wang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from eval_util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_features = pd.read_csv(\"./azure_features_images/data/vizwiz_train_color_recognition.csv\", delimiter=\";\", engine=\"python\")\n",
    "train_features['answer'] = train_features['descriptions'].astype(str).apply(lambda x: x.lower())\n",
    "train_features['QID'] = train_features['qid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_targets = pd.read_csv(\"../vizwiz_skill_typ_train.csv\", engine=\"python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.merge(train_features, train_targets, how='left', on='QID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'basil', 'basil leaves'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(train.values[0, 15:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# current visual question answers' accuracy (agreement)\n",
    "ans_accuracy  = []\n",
    "# accuracy within each given answer type\n",
    "type_accuracy = {}\n",
    "\n",
    "mat = train.values\n",
    "\n",
    "for i in range(len(mat)):\n",
    "    resAns = mat[i,5]\n",
    "    # clean up str\n",
    "    ans_original    = re.sub('[\\n\\t\\s]', ' ', resAns).strip()\n",
    "    ans_punctuation = processPunctuation(ans_original)\n",
    "    ans_final       = str(processDigitArticle(ans_punctuation))\n",
    "    accuracy_scores = []\n",
    "    # evaluate 10 different crowd answers\n",
    "    crowd_answers = mat[i, 15:25]\n",
    "    if len(set(crowd_answers)) > 1:\n",
    "        for a in crowd_answers:\n",
    "            a = processPunctuation(a)\n",
    "    for curr in crowd_answers:\n",
    "        diff_ans = [ans for ans in crowd_answers if ans != curr]\n",
    "        # find agreement to measure accuracy\n",
    "        matches = []\n",
    "        for ans in diff_ans:\n",
    "            ans = str(ans)\n",
    "            if ans.isdigit() or ans in [\"no\", \"yes\"]:\n",
    "                ans = ans.center(len(ans)+2)\n",
    "            if ans in ans_final:\n",
    "                matches.append(ans)\n",
    "        # get VQA eval score\n",
    "        score = min(1, float(len(matches)/3))\n",
    "        accuracy_scores.append(score)\n",
    "    # record accuracy for agreement & answer type \n",
    "    ans_type = mat[i,9]\n",
    "    ans_accuracy.append(np.mean(accuracy_scores))\n",
    "    if ans_type not in type_accuracy:\n",
    "        type_accuracy[ans_type] = []\n",
    "    type_accuracy[ans_type].append(score)\n",
    "    qid = mat[i,0]\n",
    "    setEvalQA(qid, score)\n",
    "    setEvalAnsType(qid, ans_type, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "setAccuracy(ans_accuracy, type_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14257"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ans_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05637464637254214\n"
     ]
    }
   ],
   "source": [
    "# mean accuracy dropped from 7% to 5%\n",
    "print(np.mean(ans_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['unanswerable', 'yes/no', 'other', 'number'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type_accuracy.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.64\n",
      "{'unanswerable': 3.81, 'yes/no': 0.87, 'other': 6.37, 'number': 0.3}\n"
     ]
    }
   ],
   "source": [
    "print(accuracy['overall'])         \n",
    "print(accuracy['perAnswerType'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['qid', 'question', 'descriptions', 'tags', 'dominant_colors', 'answer',\n",
       "       'QID', 'IMG', 'QSN', 'ANS_TYP', 'TXT', 'OBJ', 'COL', 'CNT', 'OTH',\n",
       "       'ANS1', 'ANS2', 'ANS3', 'ANS4', 'ANS5', 'ANS6', 'ANS7', 'ANS8', 'ANS9',\n",
       "       'ANS10'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'overall': 5.64,\n",
       " 'perAnswerType': {'number': 0.3,\n",
       "  'other': 6.37,\n",
       "  'unanswerable': 3.81,\n",
       "  'yes/no': 0.87}}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
