{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/edithzeng/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/edithzeng/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import random\n",
    "import numpy as np\n",
    "import copy\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "kfold=KFold(n_splits=10)\n",
    "import nltk \n",
    "import gensim\n",
    "import logging\n",
    "\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve\n",
    "\n",
    "# for LSTM (keras with tf backend)\n",
    "import gzip\n",
    "import os\n",
    "import pickle\n",
    "import requests\n",
    "import time\n",
    "import re\n",
    "os.environ['KERAS_BACKEND']='cntk'\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.models import Sequential, load_model\n",
    "from keras import regularizers\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.initializers import he_normal\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional\n",
    "from keras.callbacks import History, CSVLogger\n",
    "from keras.utils import to_categorical\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "MAX_DOC_LEN = 40\n",
    "VOCAB_SIZE = 3000\n",
    "EMBEDDING_DIM = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vizwiz_features_train_color = pd.read_csv('azure_features_images/data/vizwiz_train_color_recognition.csv',\n",
    "                                    delimiter=';', engine='python', \n",
    "                                    dtype={'qid':str, 'question':str, 'descriptions':list,\n",
    "                                          'tags':list, 'dominant_colors':list},\n",
    "                                    quotechar='\"', error_bad_lines=False)\n",
    "vizwiz_features_train_text = pd.read_csv('azure_features_images/data/vizwiz_train_text_recognition.csv',\n",
    "                                    delimiter=';', engine='python', \n",
    "                                    dtype={'qid':str, 'question':str, 'descriptions':list,\n",
    "                                          'ocr_text':list, 'handwritten_text':list},\n",
    "                                    quotechar='\"', error_bad_lines=False)\n",
    "\n",
    "\n",
    "vizwiz_features_val_color = pd.read_csv('azure_features_images/data/vizwiz_val_color_recognition.csv',\n",
    "                                  delimiter=';', engine='python',\n",
    "                                  dtype={'qid':str, 'question':str, 'descriptions':list,\n",
    "                                          'tags':list, 'dominant_colors':list},\n",
    "                                  quotechar='\"', error_bad_lines=False)\n",
    "vizwiz_features_val_text = pd.read_csv('azure_features_images/data/vizwiz_val_text_recognition.csv',\n",
    "                                    delimiter=';', engine='python', \n",
    "                                    dtype={'qid':str, 'question':str, 'descriptions':list,\n",
    "                                          'ocr_text':list, 'handwritten_text':list},\n",
    "                                    quotechar='\"', error_bad_lines=False)\n",
    "\n",
    "\n",
    "vqa_features_train_color = pd.read_csv('azure_features_images/data/vqa_train_color_recognition.csv',\n",
    "                                 delimiter=';', engine='python', \n",
    "                                 dtype={'qid':str, 'question':str, 'descriptions':list,\n",
    "                                          'tags':list, 'dominant_colors':list},\n",
    "                                 quotechar='\"', error_bad_lines=False)\n",
    "vqa_features_train_text = pd.read_csv('azure_features_images/data/vqa_train_text_recognition.csv',\n",
    "                                    delimiter=';', engine='python', \n",
    "                                    dtype={'qid':str, 'question':str, 'descriptions':list,\n",
    "                                          'ocr_text':list, 'handwritten_text':list},\n",
    "                                    quotechar='\"', error_bad_lines=False)\n",
    "vqa_features_val_color = pd.read_csv('azure_features_images/data/vqa_val_color_recognition.csv',\n",
    "                               delimiter=';', engine='python',\n",
    "                               dtype={'qid':str, 'question':str, 'descriptions':list,\n",
    "                                          'tags':list, 'dominant_colors':list},\n",
    "                               quotechar='\"', error_bad_lines=False)\n",
    "vqa_features_val_text = pd.read_csv('azure_features_images/data/vqa_val_text_recognition.csv',\n",
    "                                    delimiter=';', engine='python', \n",
    "                                    dtype={'qid':str, 'question':str, 'descriptions':list,\n",
    "                                          'ocr_text':list, 'handwritten_text':list},\n",
    "                                    quotechar='\"', error_bad_lines=False)\n",
    "\n",
    "\n",
    "vizwiz_targets_train = pd.read_csv('../vizwiz_skill_typ_train.csv',\n",
    "                                   delimiter=',', quotechar='\"',\n",
    "                                   engine='python', error_bad_lines=False)\n",
    "vizwiz_targets_val = pd.read_csv('../vizwiz_skill_typ_val.csv',\n",
    "                                 delimiter=',', quotechar='\"', engine='python', error_bad_lines=False)\n",
    "vqa_targets_train = pd.read_csv('../vqa_skill_typ_train.csv',\n",
    "                               engine='python', quotechar='\"', error_bad_lines=False)\n",
    "vqa_targets_val = pd.read_csv('../vqa_skill_typ_val.csv',\n",
    "                               engine='python', quotechar='\"', error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_feature_target(feature_df_text, feature_df_color, target_df):\n",
    "    feature_text = copy.deepcopy(feature_df_text)\n",
    "    feature_color = copy.deepcopy(feature_df_color)\n",
    "    target = copy.deepcopy(target_df)\n",
    "    # text features \n",
    "    feature_text.rename({'qid': 'QID'}, axis=1, inplace=True)\n",
    "    feature_text.set_index('QID', inplace=True)\n",
    "    # color features\n",
    "    feature_color.rename({'qid': 'QID'}, axis=1, inplace=True)\n",
    "    feature_color.set_index('QID', inplace=True)\n",
    "    # join features\n",
    "    features = feature_text.join(feature_color[['descriptions','tags','dominant_colors']],\n",
    "                                 on='QID',\n",
    "                                 how='outer')\n",
    "    # join features with target\n",
    "    target = target[['QID', 'IMG', 'QSN', 'TXT', 'OBJ', 'COL', 'CNT', 'OTH']]\n",
    "    target.set_index('QID', inplace=True)\n",
    "    target = target.astype(dtype=str)\n",
    "    df = target.join(features, on='QID', how='inner')\n",
    "    df['descriptions'].astype(list)\n",
    "    return df\n",
    "\n",
    "def lem(s):\n",
    "    arr = s.split(\" \")\n",
    "    lem = WordNetLemmatizer()\n",
    "    op = \"\"\n",
    "    for w in arr:\n",
    "        word = lem.lemmatize(w) + ' '\n",
    "        op += word\n",
    "    return op\n",
    "\n",
    "def preprocess_text(feature_columns):\n",
    "    \"\"\" output an nparray with single document per data point \"\"\"\n",
    "    ip = copy.deepcopy(feature_columns).values\n",
    "    op = []\n",
    "    for i in range(ip.shape[0]):\n",
    "        doc      =  \"\"\n",
    "        for j in range(ip.shape[1]):\n",
    "            # clean up chars\n",
    "            s    =  str(ip[i][j])\n",
    "            s    =  s.translate({ord(c): \"\" for c in \"!@#$%^&*()[]{};:,./<>?\\|`~-=_+'\"}).lower() + \" \"\n",
    "            if j == 1:             # clean descriptions\n",
    "                s = re.sub(r'confidence\\s+\\d+', '', s)\n",
    "                s = re.sub(r'text', '', s)\n",
    "            # lexicon normalize\n",
    "            s    = lem(s)\n",
    "            doc  += (s.strip())\n",
    "        op.append(doc)\n",
    "    op = np.asarray(op)\n",
    "    return op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "vizwiz_train   = join_feature_target(vizwiz_features_train_text, \n",
    "                                   vizwiz_features_train_color, \n",
    "                                   vizwiz_targets_train)\n",
    "vizwiz_val     = join_feature_target(vizwiz_features_val_text, \n",
    "                                 vizwiz_features_val_color, \n",
    "                                 vizwiz_targets_val)\n",
    "\n",
    "# create X and Y\n",
    "features_train = preprocess_text(vizwiz_train[['QSN', \n",
    "                                       'descriptions', 'tags', 'dominant_colors', \n",
    "                                       'handwritten_text', 'ocr_text']])\n",
    "txt_train      = vizwiz_train[\"TXT\"].values.astype('float32')\n",
    "obj_train      = vizwiz_train[\"OBJ\"].values\n",
    "col_train      = vizwiz_train[\"COL\"].values.astype('float32')\n",
    "cnt_train      = vizwiz_train[\"CNT\"].values\n",
    "\n",
    "features_val   = preprocess_text(vizwiz_val[['QSN', \n",
    "                                     'descriptions', 'tags', 'dominant_colors', \n",
    "                                     'handwritten_text', 'ocr_text']])\n",
    "txt_val        = vizwiz_val[\"TXT\"].values\n",
    "obj_val        = vizwiz_val[\"OBJ\"].values\n",
    "col_val        = vizwiz_val[\"COL\"].values\n",
    "cnt_val        = vizwiz_val[\"CNT\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples each class - Vizwiz - train\n",
      "Text recognition [6247. 8010.]\n",
      "Color recognition [8844. 5413.]\n"
     ]
    }
   ],
   "source": [
    "# tokenize\n",
    "tok        = Tokenizer(num_words=VOCAB_SIZE, \n",
    "                       filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "                       lower=True,\n",
    "                       split=\" \")\n",
    "tok.fit_on_texts(features_train)\n",
    "\n",
    "# create sequences & pad\n",
    "train_seq  = tok.texts_to_sequences(features_train)\n",
    "train_seq  = sequence.pad_sequences(train_seq, maxlen=MAX_DOC_LEN)\n",
    "val_seq    = tok.texts_to_sequences(features_val)\n",
    "val_seq    = sequence.pad_sequences(val_seq, maxlen=MAX_DOC_LEN)\n",
    "\n",
    "# check class distribution\n",
    "text_recognition_labels = to_categorical(np.asarray(txt_train)).astype('float32')\n",
    "color_recognition_labels = to_categorical(np.asarray(col_train)).astype('float32')\n",
    "print('Number of samples each class - Vizwiz - train')\n",
    "print('Text recognition', text_recognition_labels.sum(axis=0))\n",
    "print('Color recognition', color_recognition_labels.sum(axis=0))\n",
    "n_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-02-26 22:43:06,884 : INFO : collecting all words and their counts\n",
      "2019-02-26 22:43:06,885 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-02-26 22:43:06,947 : INFO : PROGRESS: at sentence #10000, processed 347442 words, keeping 31941 word types\n",
      "2019-02-26 22:43:06,972 : INFO : collected 41179 word types from a corpus of 490715 raw words and 14257 sentences\n",
      "2019-02-26 22:43:06,973 : INFO : Loading a fresh vocabulary\n",
      "2019-02-26 22:43:06,997 : INFO : effective_min_count=6 retains 4193 unique words (10% of original 41179, drops 36986)\n",
      "2019-02-26 22:43:06,997 : INFO : effective_min_count=6 leaves 440027 word corpus (89% of original 490715, drops 50688)\n",
      "2019-02-26 22:43:07,009 : INFO : deleting the raw counts dictionary of 41179 items\n",
      "2019-02-26 22:43:07,011 : INFO : sample=0.001 downsamples 67 most-common words\n",
      "2019-02-26 22:43:07,011 : INFO : downsampling leaves estimated 301690 word corpus (68.6% of prior 440027)\n",
      "2019-02-26 22:43:07,020 : INFO : estimated required memory for 4193 words and 100 dimensions: 5450900 bytes\n",
      "2019-02-26 22:43:07,020 : INFO : resetting layer weights\n",
      "2019-02-26 22:43:07,072 : INFO : training model with 6 workers on 4193 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-02-26 22:43:07,428 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-02-26 22:43:07,433 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-02-26 22:43:07,436 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-02-26 22:43:07,441 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-02-26 22:43:07,448 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-02-26 22:43:07,454 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-02-26 22:43:07,454 : INFO : EPOCH - 1 : training on 490715 raw words (301526 effective words) took 0.4s, 801635 effective words/s\n",
      "2019-02-26 22:43:07,807 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-02-26 22:43:07,811 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-02-26 22:43:07,813 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-02-26 22:43:07,819 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-02-26 22:43:07,828 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-02-26 22:43:07,835 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-02-26 22:43:07,836 : INFO : EPOCH - 2 : training on 490715 raw words (302065 effective words) took 0.4s, 804444 effective words/s\n",
      "2019-02-26 22:43:08,213 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-02-26 22:43:08,217 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-02-26 22:43:08,226 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-02-26 22:43:08,229 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-02-26 22:43:08,234 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-02-26 22:43:08,237 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-02-26 22:43:08,238 : INFO : EPOCH - 3 : training on 490715 raw words (301298 effective words) took 0.4s, 761255 effective words/s\n",
      "2019-02-26 22:43:08,590 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-02-26 22:43:08,597 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-02-26 22:43:08,598 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-02-26 22:43:08,601 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-02-26 22:43:08,606 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-02-26 22:43:08,619 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-02-26 22:43:08,620 : INFO : EPOCH - 4 : training on 490715 raw words (301378 effective words) took 0.4s, 802013 effective words/s\n",
      "2019-02-26 22:43:08,973 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-02-26 22:43:08,975 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-02-26 22:43:08,979 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-02-26 22:43:08,988 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-02-26 22:43:08,989 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-02-26 22:43:08,996 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-02-26 22:43:08,997 : INFO : EPOCH - 5 : training on 490715 raw words (301618 effective words) took 0.4s, 813005 effective words/s\n",
      "2019-02-26 22:43:08,997 : INFO : training on a 2453575 raw words (1507885 effective words) took 1.9s, 783421 effective words/s\n"
     ]
    }
   ],
   "source": [
    "# punkt sentence level tokenizer\n",
    "sent_lst = []\n",
    "for doc in features_train:\n",
    "    sentences = nltk.tokenize.sent_tokenize(doc)\n",
    "    for sent in sentences:\n",
    "        word_lst = [w for w in nltk.tokenize.word_tokenize(sent) if w.isalnum()]\n",
    "        sent_lst.append(word_lst)\n",
    "        \n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "word2vec_model = gensim.models.Word2Vec(sentences=sent_lst,\n",
    "                                        min_count=6,\n",
    "                                        size=EMBEDDING_DIM,\n",
    "                                        sg=1,\n",
    "                                        workers=os.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 4193 word vectors\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "\n",
    "for word in word2vec_model.wv.vocab:\n",
    "    coefs = np.asarray(word2vec_model.wv[word], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "\n",
    "print('Total %s word vectors' % len(embeddings_index))\n",
    "\n",
    "# Initial word embedding\n",
    "embedding_matrix = np.zeros((VOCAB_SIZE, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in tok.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None and i < VOCAB_SIZE:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 4193 word vectors\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "\n",
    "for word in word2vec_model.wv.vocab:\n",
    "    coefs = np.asarray(word2vec_model.wv[word], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "\n",
    "print('Total %s word vectors' % len(embeddings_index))\n",
    "\n",
    "# Initial word embedding\n",
    "embedding_matrix = np.zeros((VOCAB_SIZE, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in tok.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None and i < VOCAB_SIZE:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_create_train(labels, learning_rate, lstm_dim, batch_size, num_epochs, optimizer_param, regularization=1e-7):\n",
    "    \n",
    "    l2_reg = regularizers.l2(regularization)\n",
    "    \n",
    "    # init model\n",
    "    initializer = he_normal(seed=42)\n",
    "    embedding_layer = Embedding(VOCAB_SIZE,\n",
    "                                EMBEDDING_DIM,\n",
    "                                input_length=MAX_DOC_LEN,\n",
    "                                trainable=True,\n",
    "                                mask_zero=False,\n",
    "                                embeddings_regularizer=l2_reg,\n",
    "                                weights=[embedding_matrix])\n",
    "    lstm_layer = LSTM(units=lstm_dim, kernel_regularizer=l2_reg)\n",
    "    dense_layer = Dense(n_classes,\n",
    "                        kernel_initializer=initializer,\n",
    "                        activation='softmax', \n",
    "                        kernel_regularizer=l2_reg)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(embedding_layer)\n",
    "    model.add(Bidirectional(lstm_layer))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(dense_layer)\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizer_param,\n",
    "                  metrics=['acc'])\n",
    "    history = History()\n",
    "    csv_logger = CSVLogger('./LSTM/color/{}_{}_{}_{}.log'.format(learning_rate, regularization, batch_size, num_epochs),\n",
    "                           separator=',',\n",
    "                           append=True)\n",
    "    t1 = time.time()\n",
    "    # model fit\n",
    "    model.fit(train_seq,\n",
    "              labels.astype('float32'),\n",
    "              batch_size=batch_size,\n",
    "              epochs=num_epochs,\n",
    "              callbacks=[history, csv_logger],\n",
    "              verbose=2)\n",
    "    t2 = time.time()\n",
    "    # save hdf5\n",
    "    model.save('./LSTM/color/{}_{}_{}_{}_model.h5'.format(learning_rate, regularization, batch_size, num_epochs))\n",
    "    np.savetxt('./LSTM/color/{}_{}_{}_{}_time.txt'.format(learning_rate, regularization, batch_size, num_epochs), \n",
    "               [regularization, (t2-t1) / 3600])\n",
    "    with open('./LSTM/color/{}_{}_{}_{}_history.txt'.format(learning_rate, regularization, batch_size, num_epochs), \"w\") as res_file:\n",
    "        res_file.write(str(history.history))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      " - 126s - loss: 535.1055 - acc: 0.6186\n",
      "Epoch 2/30\n"
     ]
    }
   ],
   "source": [
    "learning_rate  = [1e-2, 1e-4, 1e-6, 1e-8, 1e-10]\n",
    "regularization = [1, 1e-4, 1e-8, 0]\n",
    "batch_size     = [20, 50, 100, 150]\n",
    "epochs         = [30, 50, 80]\n",
    "\n",
    "# dropout = 0.5, GSD Nesterov\n",
    "\n",
    "for L in learning_rate:\n",
    "    for R in regularization:\n",
    "        for B in batch_size: \n",
    "            for E in epochs:\n",
    "                lstm_create_train(labels=color_recognition_labels,\n",
    "                                  learning_rate=L,\n",
    "                                  lstm_dim=100, \n",
    "                                  batch_size=B,\n",
    "                                  num_epochs=E, \n",
    "                                  optimizer_param=SGD(lr=L, nesterov=True),\n",
    "                                  regularization=R)\n",
    "                model = load_model('./LSTM/color/{}_{}_{}_{}_model.h5'.format(L,R,B,E))\n",
    "                preds = model.predict(val_seq, verbose=1)\n",
    "                print(\"Learning rate: {} Regularization: {} Batch size: {} Epoch: {}\".format(L,R,B,E))\n",
    "                print((\"Accuracy = {} \\t AUC = {}\".format(accuracy_score(txt_val, preds.argmax(axis=1)), \n",
    "                                                            roc_auc_score(txt_val, preds[:,1]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
