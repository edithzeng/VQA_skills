{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py35/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using CNTK backend\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/edithzeng/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/edithzeng/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py35/lib/python3.5/site-packages/keras/backend/cntk_backend.py:25: UserWarning: CNTK backend warning: GPU is not detected. CNTK's CPU version is not fully optimized,please run with GPU to get better performance.\n",
      "  'CNTK backend warning: GPU is not detected. '\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import random\n",
    "import numpy as np\n",
    "import copy\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "kfold=KFold(n_splits=10)\n",
    "import nltk \n",
    "import gensim\n",
    "import logging\n",
    "\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve\n",
    "\n",
    "# for LSTM (keras with tf backend)\n",
    "import gzip\n",
    "import os\n",
    "import pickle\n",
    "import requests\n",
    "import time\n",
    "import re\n",
    "os.environ['KERAS_BACKEND']='cntk'\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.models import Sequential, load_model\n",
    "from keras import regularizers\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.initializers import he_normal\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional\n",
    "from keras.callbacks import History, CSVLogger\n",
    "from keras.utils import to_categorical\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "MAX_DOC_LEN = 40\n",
    "VOCAB_SIZE = 3000\n",
    "EMBEDDING_DIM = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipping line 2150: ';' expected after '\"'\n",
      "Skipping line 42: Expected 4 fields in line 42, saw 7\n",
      "Skipping line 60: Expected 4 fields in line 60, saw 8\n",
      "Skipping line 157: Expected 4 fields in line 157, saw 5\n",
      "Skipping line 207: Expected 4 fields in line 207, saw 6\n",
      "Skipping line 210: Expected 4 fields in line 210, saw 5\n",
      "Skipping line 275: Expected 4 fields in line 275, saw 5\n",
      "Skipping line 293: Expected 4 fields in line 293, saw 5\n",
      "Skipping line 341: Expected 4 fields in line 341, saw 7\n",
      "Skipping line 357: Expected 4 fields in line 357, saw 5\n",
      "Skipping line 376: Expected 4 fields in line 376, saw 5\n",
      "Skipping line 469: Expected 4 fields in line 469, saw 6\n",
      "Skipping line 478: Expected 4 fields in line 478, saw 5\n",
      "Skipping line 494: Expected 4 fields in line 494, saw 5\n",
      "Skipping line 496: Expected 4 fields in line 496, saw 6\n",
      "Skipping line 504: Expected 4 fields in line 504, saw 5\n",
      "Skipping line 512: Expected 4 fields in line 512, saw 5\n",
      "Skipping line 523: Expected 4 fields in line 523, saw 6\n",
      "Skipping line 550: Expected 4 fields in line 550, saw 5\n",
      "Skipping line 606: Expected 4 fields in line 606, saw 8\n",
      "Skipping line 723: Expected 4 fields in line 723, saw 5\n",
      "Skipping line 762: Expected 4 fields in line 762, saw 5\n",
      "Skipping line 764: Expected 4 fields in line 764, saw 8\n",
      "Skipping line 800: Expected 4 fields in line 800, saw 5\n",
      "Skipping line 801: Expected 4 fields in line 801, saw 6\n",
      "Skipping line 821: Expected 4 fields in line 821, saw 5\n",
      "Skipping line 845: Expected 4 fields in line 845, saw 5\n",
      "Skipping line 895: Expected 4 fields in line 895, saw 6\n",
      "Skipping line 927: Expected 4 fields in line 927, saw 5\n",
      "Skipping line 937: Expected 4 fields in line 937, saw 5\n",
      "Skipping line 952: Expected 4 fields in line 952, saw 10\n",
      "Skipping line 988: Expected 4 fields in line 988, saw 5\n",
      "Skipping line 1063: Expected 4 fields in line 1063, saw 5\n",
      "Skipping line 1081: Expected 4 fields in line 1081, saw 5\n",
      "Skipping line 1112: Expected 4 fields in line 1112, saw 5\n",
      "Skipping line 1135: Expected 4 fields in line 1135, saw 5\n",
      "Skipping line 1164: Expected 4 fields in line 1164, saw 5\n",
      "Skipping line 1169: Expected 4 fields in line 1169, saw 5\n",
      "Skipping line 1197: Expected 4 fields in line 1197, saw 5\n",
      "Skipping line 1252: Expected 4 fields in line 1252, saw 5\n",
      "Skipping line 1259: Expected 4 fields in line 1259, saw 5\n",
      "Skipping line 1275: Expected 4 fields in line 1275, saw 6\n",
      "Skipping line 1289: Expected 4 fields in line 1289, saw 5\n",
      "Skipping line 1299: Expected 4 fields in line 1299, saw 6\n",
      "Skipping line 1338: Expected 4 fields in line 1338, saw 5\n",
      "Skipping line 1344: Expected 4 fields in line 1344, saw 5\n",
      "Skipping line 1372: Expected 4 fields in line 1372, saw 5\n",
      "Skipping line 1383: Expected 4 fields in line 1383, saw 5\n",
      "Skipping line 1393: Expected 4 fields in line 1393, saw 9\n",
      "Skipping line 1410: Expected 4 fields in line 1410, saw 5\n",
      "Skipping line 1526: Expected 4 fields in line 1526, saw 5\n",
      "Skipping line 1529: Expected 4 fields in line 1529, saw 6\n",
      "Skipping line 1577: Expected 4 fields in line 1577, saw 5\n",
      "Skipping line 1596: Expected 4 fields in line 1596, saw 5\n",
      "Skipping line 1603: Expected 4 fields in line 1603, saw 5\n",
      "Skipping line 1719: Expected 4 fields in line 1719, saw 5\n",
      "Skipping line 1738: Expected 4 fields in line 1738, saw 6\n",
      "Skipping line 1790: Expected 4 fields in line 1790, saw 5\n",
      "Skipping line 1807: Expected 4 fields in line 1807, saw 5\n",
      "Skipping line 1813: Expected 4 fields in line 1813, saw 5\n",
      "Skipping line 2011: Expected 4 fields in line 2011, saw 6\n",
      "Skipping line 2099: Expected 4 fields in line 2099, saw 5\n",
      "Skipping line 2110: Expected 4 fields in line 2110, saw 8\n",
      "Skipping line 2166: Expected 4 fields in line 2166, saw 6\n",
      "Skipping line 2176: Expected 4 fields in line 2176, saw 5\n",
      "Skipping line 2327: Expected 4 fields in line 2327, saw 5\n",
      "Skipping line 2339: Expected 4 fields in line 2339, saw 5\n",
      "Skipping line 2424: Expected 4 fields in line 2424, saw 5\n",
      "Skipping line 2458: Expected 4 fields in line 2458, saw 6\n",
      "Skipping line 2492: Expected 4 fields in line 2492, saw 5\n",
      "Skipping line 2523: Expected 4 fields in line 2523, saw 5\n",
      "Skipping line 2551: Expected 4 fields in line 2551, saw 5\n",
      "Skipping line 2563: Expected 4 fields in line 2563, saw 7\n",
      "Skipping line 2594: Expected 4 fields in line 2594, saw 5\n",
      "Skipping line 2636: Expected 4 fields in line 2636, saw 5\n",
      "Skipping line 2648: Expected 4 fields in line 2648, saw 5\n",
      "Skipping line 2682: Expected 4 fields in line 2682, saw 5\n",
      "Skipping line 2724: Expected 4 fields in line 2724, saw 5\n",
      "Skipping line 2827: Expected 4 fields in line 2827, saw 5\n",
      "Skipping line 2953: Expected 4 fields in line 2953, saw 5\n",
      "Skipping line 2966: Expected 4 fields in line 2966, saw 11\n",
      "Skipping line 2987: Expected 4 fields in line 2987, saw 5\n",
      "Skipping line 2996: Expected 4 fields in line 2996, saw 5\n",
      "Skipping line 2997: Expected 4 fields in line 2997, saw 5\n",
      "Skipping line 3000: Expected 4 fields in line 3000, saw 8\n",
      "Skipping line 3040: Expected 4 fields in line 3040, saw 5\n",
      "Skipping line 3049: Expected 4 fields in line 3049, saw 18\n",
      "Skipping line 3060: Expected 4 fields in line 3060, saw 5\n",
      "Skipping line 3065: Expected 4 fields in line 3065, saw 5\n",
      "Skipping line 3077: Expected 4 fields in line 3077, saw 5\n",
      "Skipping line 3090: Expected 4 fields in line 3090, saw 5\n",
      "Skipping line 3120: Expected 4 fields in line 3120, saw 5\n",
      "Skipping line 3172: Expected 4 fields in line 3172, saw 5\n",
      "Skipping line 3174: Expected 4 fields in line 3174, saw 5\n",
      "Skipping line 3243: Expected 4 fields in line 3243, saw 5\n",
      "Skipping line 3263: Expected 4 fields in line 3263, saw 5\n",
      "Skipping line 3332: Expected 4 fields in line 3332, saw 5\n",
      "Skipping line 3352: Expected 4 fields in line 3352, saw 17\n",
      "Skipping line 3393: Expected 4 fields in line 3393, saw 5\n",
      "Skipping line 3467: Expected 4 fields in line 3467, saw 5\n",
      "Skipping line 3506: Expected 4 fields in line 3506, saw 5\n",
      "Skipping line 3514: Expected 4 fields in line 3514, saw 5\n",
      "Skipping line 3525: Expected 4 fields in line 3525, saw 5\n",
      "Skipping line 3537: Expected 4 fields in line 3537, saw 5\n",
      "Skipping line 3625: Expected 4 fields in line 3625, saw 5\n",
      "Skipping line 3697: Expected 4 fields in line 3697, saw 5\n",
      "Skipping line 3703: Expected 4 fields in line 3703, saw 6\n",
      "Skipping line 3769: Expected 4 fields in line 3769, saw 5\n",
      "Skipping line 3790: Expected 4 fields in line 3790, saw 5\n",
      "Skipping line 3807: Expected 4 fields in line 3807, saw 5\n",
      "Skipping line 3812: Expected 4 fields in line 3812, saw 5\n",
      "Skipping line 3819: Expected 4 fields in line 3819, saw 6\n",
      "Skipping line 3828: Expected 4 fields in line 3828, saw 5\n",
      "Skipping line 3835: Expected 4 fields in line 3835, saw 5\n",
      "Skipping line 3838: Expected 4 fields in line 3838, saw 6\n",
      "Skipping line 3841: Expected 4 fields in line 3841, saw 6\n",
      "Skipping line 3849: Expected 4 fields in line 3849, saw 5\n",
      "Skipping line 3876: Expected 4 fields in line 3876, saw 6\n",
      "Skipping line 3886: Expected 4 fields in line 3886, saw 5\n",
      "Skipping line 3890: Expected 4 fields in line 3890, saw 5\n",
      "Skipping line 3942: Expected 4 fields in line 3942, saw 5\n",
      "Skipping line 3996: Expected 4 fields in line 3996, saw 5\n",
      "Skipping line 4134: Expected 4 fields in line 4134, saw 5\n",
      "Skipping line 4204: Expected 4 fields in line 4204, saw 5\n",
      "Skipping line 4207: Expected 4 fields in line 4207, saw 5\n",
      "Skipping line 4225: Expected 4 fields in line 4225, saw 5\n",
      "Skipping line 4230: Expected 4 fields in line 4230, saw 9\n",
      "Skipping line 4300: Expected 4 fields in line 4300, saw 6\n",
      "Skipping line 4371: Expected 4 fields in line 4371, saw 6\n",
      "Skipping line 4375: Expected 4 fields in line 4375, saw 6\n",
      "Skipping line 4414: Expected 4 fields in line 4414, saw 18\n",
      "Skipping line 4454: Expected 4 fields in line 4454, saw 5\n",
      "Skipping line 4465: Expected 4 fields in line 4465, saw 6\n",
      "Skipping line 4561: Expected 4 fields in line 4561, saw 5\n",
      "Skipping line 4658: Expected 4 fields in line 4658, saw 5\n",
      "Skipping line 4667: Expected 4 fields in line 4667, saw 5\n",
      "Skipping line 4673: Expected 4 fields in line 4673, saw 5\n",
      "Skipping line 4674: Expected 4 fields in line 4674, saw 5\n",
      "Skipping line 4677: Expected 4 fields in line 4677, saw 5\n",
      "Skipping line 4734: Expected 4 fields in line 4734, saw 5\n",
      "Skipping line 4748: Expected 4 fields in line 4748, saw 6\n",
      "Skipping line 4764: Expected 4 fields in line 4764, saw 5\n",
      "Skipping line 4776: Expected 4 fields in line 4776, saw 5\n",
      "Skipping line 4784: Expected 4 fields in line 4784, saw 5\n",
      "Skipping line 4841: Expected 4 fields in line 4841, saw 6\n",
      "Skipping line 4859: Expected 4 fields in line 4859, saw 5\n",
      "Skipping line 4889: Expected 4 fields in line 4889, saw 5\n",
      "Skipping line 4897: Expected 4 fields in line 4897, saw 5\n",
      "Skipping line 4943: Expected 4 fields in line 4943, saw 5\n",
      "Skipping line 4955: Expected 4 fields in line 4955, saw 5\n",
      "Skipping line 5035: Expected 4 fields in line 5035, saw 6\n",
      "Skipping line 5051: Expected 4 fields in line 5051, saw 5\n",
      "Skipping line 5060: Expected 4 fields in line 5060, saw 8\n",
      "Skipping line 5089: Expected 4 fields in line 5089, saw 5\n",
      "Skipping line 5106: Expected 4 fields in line 5106, saw 6\n",
      "Skipping line 5124: Expected 4 fields in line 5124, saw 5\n",
      "Skipping line 5160: Expected 4 fields in line 5160, saw 5\n",
      "Skipping line 5170: Expected 4 fields in line 5170, saw 5\n",
      "Skipping line 5173: Expected 4 fields in line 5173, saw 5\n",
      "Skipping line 5218: Expected 4 fields in line 5218, saw 5\n",
      "Skipping line 5261: Expected 4 fields in line 5261, saw 8\n",
      "Skipping line 5268: Expected 4 fields in line 5268, saw 5\n",
      "Skipping line 5270: Expected 4 fields in line 5270, saw 6\n",
      "Skipping line 5397: Expected 4 fields in line 5397, saw 6\n",
      "Skipping line 5431: Expected 4 fields in line 5431, saw 5\n",
      "Skipping line 5432: Expected 4 fields in line 5432, saw 5\n",
      "Skipping line 5481: Expected 4 fields in line 5481, saw 5\n",
      "Skipping line 5547: Expected 4 fields in line 5547, saw 5\n",
      "Skipping line 5568: Expected 4 fields in line 5568, saw 7\n",
      "Skipping line 5598: Expected 4 fields in line 5598, saw 5\n",
      "Skipping line 5601: Expected 4 fields in line 5601, saw 5\n",
      "Skipping line 5615: Expected 4 fields in line 5615, saw 6\n",
      "Skipping line 5616: Expected 4 fields in line 5616, saw 5\n",
      "Skipping line 5653: Expected 4 fields in line 5653, saw 5\n",
      "Skipping line 5655: Expected 4 fields in line 5655, saw 5\n",
      "Skipping line 5716: Expected 4 fields in line 5716, saw 5\n",
      "Skipping line 5729: Expected 4 fields in line 5729, saw 5\n",
      "Skipping line 5753: Expected 4 fields in line 5753, saw 5\n",
      "Skipping line 5808: Expected 4 fields in line 5808, saw 5\n",
      "Skipping line 5823: Expected 4 fields in line 5823, saw 5\n",
      "Skipping line 5824: Expected 4 fields in line 5824, saw 5\n",
      "Skipping line 5861: Expected 4 fields in line 5861, saw 5\n",
      "Skipping line 5871: Expected 4 fields in line 5871, saw 5\n",
      "Skipping line 5955: Expected 4 fields in line 5955, saw 11\n",
      "Skipping line 6012: Expected 4 fields in line 6012, saw 5\n",
      "Skipping line 6107: Expected 4 fields in line 6107, saw 5\n",
      "Skipping line 6113: Expected 4 fields in line 6113, saw 5\n",
      "Skipping line 6136: Expected 4 fields in line 6136, saw 6\n",
      "Skipping line 6154: Expected 4 fields in line 6154, saw 5\n",
      "Skipping line 6160: Expected 4 fields in line 6160, saw 5\n",
      "Skipping line 6184: Expected 4 fields in line 6184, saw 5\n",
      "Skipping line 6210: Expected 4 fields in line 6210, saw 5\n",
      "Skipping line 6370: Expected 4 fields in line 6370, saw 6\n",
      "Skipping line 6371: Expected 4 fields in line 6371, saw 5\n",
      "Skipping line 6394: Expected 4 fields in line 6394, saw 5\n",
      "Skipping line 6395: Expected 4 fields in line 6395, saw 5\n",
      "Skipping line 6414: Expected 4 fields in line 6414, saw 5\n",
      "Skipping line 6455: Expected 4 fields in line 6455, saw 7\n",
      "Skipping line 6474: Expected 4 fields in line 6474, saw 5\n",
      "Skipping line 6485: Expected 4 fields in line 6485, saw 7\n",
      "Skipping line 6510: Expected 4 fields in line 6510, saw 5\n",
      "Skipping line 6549: Expected 4 fields in line 6549, saw 6\n",
      "Skipping line 6585: Expected 4 fields in line 6585, saw 5\n",
      "Skipping line 6596: Expected 4 fields in line 6596, saw 5\n",
      "Skipping line 6601: Expected 4 fields in line 6601, saw 6\n",
      "Skipping line 6621: Expected 4 fields in line 6621, saw 5\n",
      "Skipping line 6665: Expected 4 fields in line 6665, saw 5\n",
      "Skipping line 6690: Expected 4 fields in line 6690, saw 5\n",
      "Skipping line 6712: Expected 4 fields in line 6712, saw 5\n",
      "Skipping line 6753: Expected 4 fields in line 6753, saw 7\n",
      "Skipping line 6865: Expected 4 fields in line 6865, saw 5\n",
      "Skipping line 6892: Expected 4 fields in line 6892, saw 5\n",
      "Skipping line 6900: Expected 4 fields in line 6900, saw 5\n",
      "Skipping line 6903: Expected 4 fields in line 6903, saw 5\n",
      "Skipping line 6912: Expected 4 fields in line 6912, saw 6\n",
      "Skipping line 6978: Expected 4 fields in line 6978, saw 5\n",
      "Skipping line 7045: Expected 4 fields in line 7045, saw 5\n",
      "Skipping line 7074: Expected 4 fields in line 7074, saw 5\n",
      "Skipping line 7148: Expected 4 fields in line 7148, saw 5\n",
      "Skipping line 7165: Expected 4 fields in line 7165, saw 5\n",
      "Skipping line 7168: Expected 4 fields in line 7168, saw 5\n",
      "Skipping line 7225: Expected 4 fields in line 7225, saw 5\n",
      "Skipping line 7233: Expected 4 fields in line 7233, saw 5\n",
      "Skipping line 7253: Expected 4 fields in line 7253, saw 5\n",
      "Skipping line 7283: Expected 4 fields in line 7283, saw 5\n",
      "Skipping line 7284: Expected 4 fields in line 7284, saw 5\n",
      "Skipping line 7315: Expected 4 fields in line 7315, saw 8\n",
      "Skipping line 7318: Expected 4 fields in line 7318, saw 5\n",
      "Skipping line 7324: Expected 4 fields in line 7324, saw 5\n",
      "Skipping line 7327: Expected 4 fields in line 7327, saw 5\n",
      "Skipping line 7378: Expected 4 fields in line 7378, saw 5\n",
      "Skipping line 7395: Expected 4 fields in line 7395, saw 5\n",
      "Skipping line 7422: Expected 4 fields in line 7422, saw 5\n",
      "Skipping line 7441: Expected 4 fields in line 7441, saw 5\n",
      "Skipping line 7450: Expected 4 fields in line 7450, saw 5\n",
      "Skipping line 7462: Expected 4 fields in line 7462, saw 5\n",
      "Skipping line 7465: Expected 4 fields in line 7465, saw 5\n",
      "Skipping line 7513: Expected 4 fields in line 7513, saw 5\n",
      "Skipping line 7561: Expected 4 fields in line 7561, saw 9\n",
      "Skipping line 7624: Expected 4 fields in line 7624, saw 5\n",
      "Skipping line 7654: Expected 4 fields in line 7654, saw 5\n",
      "Skipping line 7684: Expected 4 fields in line 7684, saw 6\n",
      "Skipping line 7692: Expected 4 fields in line 7692, saw 5\n",
      "Skipping line 7724: Expected 4 fields in line 7724, saw 12\n",
      "Skipping line 7769: Expected 4 fields in line 7769, saw 7\n",
      "Skipping line 7782: Expected 4 fields in line 7782, saw 6\n",
      "Skipping line 7813: Expected 4 fields in line 7813, saw 5\n",
      "Skipping line 7852: Expected 4 fields in line 7852, saw 5\n",
      "Skipping line 7861: Expected 4 fields in line 7861, saw 6\n",
      "Skipping line 7872: Expected 4 fields in line 7872, saw 5\n",
      "Skipping line 7874: Expected 4 fields in line 7874, saw 6\n",
      "Skipping line 7894: Expected 4 fields in line 7894, saw 5\n",
      "Skipping line 7915: Expected 4 fields in line 7915, saw 5\n",
      "Skipping line 8100: Expected 4 fields in line 8100, saw 5\n",
      "Skipping line 8238: Expected 4 fields in line 8238, saw 5\n",
      "Skipping line 8277: Expected 4 fields in line 8277, saw 5\n",
      "Skipping line 8610: Expected 4 fields in line 8610, saw 5\n",
      "Skipping line 9179: Expected 4 fields in line 9179, saw 5\n",
      "Skipping line 10203: Expected 4 fields in line 10203, saw 5\n",
      "Skipping line 10265: Expected 4 fields in line 10265, saw 5\n",
      "Skipping line 10297: Expected 4 fields in line 10297, saw 5\n",
      "Skipping line 10875: Expected 4 fields in line 10875, saw 5\n",
      "Skipping line 11066: Expected 4 fields in line 11066, saw 5\n",
      "Skipping line 11488: Expected 4 fields in line 11488, saw 5\n",
      "Skipping line 12067: Expected 4 fields in line 12067, saw 6\n",
      "Skipping line 12481: Expected 4 fields in line 12481, saw 5\n",
      "Skipping line 13243: Expected 4 fields in line 13243, saw 5\n",
      "Skipping line 13251: Expected 4 fields in line 13251, saw 5\n",
      "Skipping line 13413: Expected 4 fields in line 13413, saw 5\n",
      "Skipping line 13475: Expected 4 fields in line 13475, saw 5\n",
      "Skipping line 13503: Expected 4 fields in line 13503, saw 5\n",
      "Skipping line 13567: Expected 4 fields in line 13567, saw 5\n",
      "Skipping line 13638: Expected 4 fields in line 13638, saw 5\n",
      "Skipping line 13640: Expected 4 fields in line 13640, saw 5\n",
      "Skipping line 13944: Expected 4 fields in line 13944, saw 6\n",
      "Skipping line 13973: Expected 4 fields in line 13973, saw 6\n",
      "Skipping line 13975: Expected 4 fields in line 13975, saw 6\n",
      "Skipping line 568: Expected 5 fields in line 568, saw 6\n",
      "Skipping line 1968: Expected 5 fields in line 1968, saw 6\n",
      "Skipping line 41: Expected 4 fields in line 41, saw 6\n",
      "Skipping line 46: Expected 4 fields in line 46, saw 7\n",
      "Skipping line 66: Expected 4 fields in line 66, saw 6\n",
      "Skipping line 193: Expected 4 fields in line 193, saw 5\n",
      "Skipping line 194: Expected 4 fields in line 194, saw 5\n",
      "Skipping line 204: Expected 4 fields in line 204, saw 5\n",
      "Skipping line 218: Expected 4 fields in line 218, saw 6\n",
      "Skipping line 298: Expected 4 fields in line 298, saw 5\n",
      "Skipping line 315: Expected 4 fields in line 315, saw 6\n",
      "Skipping line 343: Expected 4 fields in line 343, saw 5\n",
      "Skipping line 361: Expected 4 fields in line 361, saw 5\n",
      "Skipping line 366: Expected 4 fields in line 366, saw 8\n",
      "Skipping line 424: Expected 4 fields in line 424, saw 5\n",
      "Skipping line 591: Expected 4 fields in line 591, saw 6\n",
      "Skipping line 618: Expected 4 fields in line 618, saw 8\n",
      "Skipping line 620: Expected 4 fields in line 620, saw 5\n",
      "Skipping line 668: Expected 4 fields in line 668, saw 7\n",
      "Skipping line 703: Expected 4 fields in line 703, saw 6\n",
      "Skipping line 716: Expected 4 fields in line 716, saw 5\n",
      "Skipping line 809: Expected 4 fields in line 809, saw 5\n",
      "Skipping line 898: Expected 4 fields in line 898, saw 5\n",
      "Skipping line 969: Expected 4 fields in line 969, saw 5\n",
      "Skipping line 976: Expected 4 fields in line 976, saw 6\n",
      "Skipping line 1032: Expected 4 fields in line 1032, saw 5\n",
      "Skipping line 1037: Expected 4 fields in line 1037, saw 6\n",
      "Skipping line 1040: Expected 4 fields in line 1040, saw 5\n",
      "Skipping line 1061: Expected 4 fields in line 1061, saw 5\n",
      "Skipping line 1076: Expected 4 fields in line 1076, saw 5\n",
      "Skipping line 1081: Expected 4 fields in line 1081, saw 5\n",
      "Skipping line 1082: Expected 4 fields in line 1082, saw 5\n",
      "Skipping line 1134: Expected 4 fields in line 1134, saw 5\n",
      "Skipping line 1142: Expected 4 fields in line 1142, saw 5\n",
      "Skipping line 1163: Expected 4 fields in line 1163, saw 6\n",
      "Skipping line 1185: Expected 4 fields in line 1185, saw 5\n",
      "Skipping line 1272: Expected 4 fields in line 1272, saw 5\n",
      "Skipping line 1423: Expected 4 fields in line 1423, saw 5\n",
      "Skipping line 1692: Expected 4 fields in line 1692, saw 5\n",
      "Skipping line 1986: Expected 4 fields in line 1986, saw 5\n",
      "Skipping line 2010: Expected 4 fields in line 2010, saw 5\n",
      "Skipping line 2067: Expected 4 fields in line 2067, saw 6\n",
      "Skipping line 145: Expected 4 fields in line 145, saw 5\n",
      "Skipping line 175: Expected 4 fields in line 175, saw 5\n",
      "Skipping line 187: Expected 4 fields in line 187, saw 5\n",
      "Skipping line 731: Expected 4 fields in line 731, saw 5\n",
      "Skipping line 1246: Expected 4 fields in line 1246, saw 5\n",
      "Skipping line 1312: Expected 4 fields in line 1312, saw 5\n",
      "Skipping line 1439: Expected 4 fields in line 1439, saw 5\n",
      "Skipping line 1581: Expected 4 fields in line 1581, saw 5\n",
      "Skipping line 1632: Expected 4 fields in line 1632, saw 5\n",
      "Skipping line 1637: Expected 4 fields in line 1637, saw 5\n",
      "Skipping line 1638: Expected 4 fields in line 1638, saw 5\n",
      "Skipping line 1639: Expected 4 fields in line 1639, saw 5\n",
      "Skipping line 1723: Expected 4 fields in line 1723, saw 5\n",
      "Skipping line 1967: Expected 4 fields in line 1967, saw 5\n",
      "Skipping line 2026: Expected 4 fields in line 2026, saw 5\n"
     ]
    }
   ],
   "source": [
    "vizwiz_features_train_color = pd.read_csv('azure_features_images/data/vizwiz_train_color_recognition.csv',\n",
    "                                    delimiter=';', engine='python', \n",
    "                                    dtype={'qid':str, 'question':str, 'descriptions':list,\n",
    "                                          'tags':list, 'dominant_colors':list},\n",
    "                                    quotechar='\"', error_bad_lines=False)\n",
    "vizwiz_features_train_text = pd.read_csv('azure_features_images/data/vizwiz_train_text_recognition.csv',\n",
    "                                    delimiter=';', engine='python', \n",
    "                                    dtype={'qid':str, 'question':str, 'descriptions':list,\n",
    "                                          'ocr_text':list, 'handwritten_text':list},\n",
    "                                    quotechar='\"', error_bad_lines=False)\n",
    "\n",
    "\n",
    "vizwiz_features_val_color = pd.read_csv('azure_features_images/data/vizwiz_val_color_recognition.csv',\n",
    "                                  delimiter=';', engine='python',\n",
    "                                  dtype={'qid':str, 'question':str, 'descriptions':list,\n",
    "                                          'tags':list, 'dominant_colors':list},\n",
    "                                  quotechar='\"', error_bad_lines=False)\n",
    "vizwiz_features_val_text = pd.read_csv('azure_features_images/data/vizwiz_val_text_recognition.csv',\n",
    "                                    delimiter=';', engine='python', \n",
    "                                    dtype={'qid':str, 'question':str, 'descriptions':list,\n",
    "                                          'ocr_text':list, 'handwritten_text':list},\n",
    "                                    quotechar='\"', error_bad_lines=False)\n",
    "\n",
    "\n",
    "vqa_features_train_color = pd.read_csv('azure_features_images/data/vqa_train_color_recognition.csv',\n",
    "                                 delimiter=';', engine='python', \n",
    "                                 dtype={'qid':str, 'question':str, 'descriptions':list,\n",
    "                                          'tags':list, 'dominant_colors':list},\n",
    "                                 quotechar='\"', error_bad_lines=False)\n",
    "vqa_features_train_text = pd.read_csv('azure_features_images/data/vqa_train_text_recognition.csv',\n",
    "                                    delimiter=';', engine='python', \n",
    "                                    dtype={'qid':str, 'question':str, 'descriptions':list,\n",
    "                                          'ocr_text':list, 'handwritten_text':list},\n",
    "                                    quotechar='\"', error_bad_lines=False)\n",
    "vqa_features_val_color = pd.read_csv('azure_features_images/data/vqa_val_color_recognition.csv',\n",
    "                               delimiter=';', engine='python',\n",
    "                               dtype={'qid':str, 'question':str, 'descriptions':list,\n",
    "                                          'tags':list, 'dominant_colors':list},\n",
    "                               quotechar='\"', error_bad_lines=False)\n",
    "vqa_features_val_text = pd.read_csv('azure_features_images/data/vqa_val_text_recognition.csv',\n",
    "                                    delimiter=';', engine='python', \n",
    "                                    dtype={'qid':str, 'question':str, 'descriptions':list,\n",
    "                                          'ocr_text':list, 'handwritten_text':list},\n",
    "                                    quotechar='\"', error_bad_lines=False)\n",
    "\n",
    "\n",
    "vizwiz_targets_train = pd.read_csv('../vizwiz_skill_typ_train.csv', dtype={'QID':str},\n",
    "                                   delimiter=',', quotechar='\"',\n",
    "                                   engine='python', error_bad_lines=False)\n",
    "vizwiz_targets_val = pd.read_csv('../vizwiz_skill_typ_val.csv', dtype={'QID':str},\n",
    "                                 delimiter=',', quotechar='\"', engine='python', error_bad_lines=False)\n",
    "vqa_targets_train = pd.read_csv('../vqa_skill_typ_train.csv', dtype={'QID':str},\n",
    "                               engine='python', quotechar='\"', error_bad_lines=False)\n",
    "vqa_targets_val = pd.read_csv('../vqa_skill_typ_val.csv', dtype={'QID':str},\n",
    "                               engine='python', quotechar='\"', error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_feature_target(feature_df_text, feature_df_color, target_df):\n",
    "    feature_text = copy.deepcopy(feature_df_text)\n",
    "    feature_color = copy.deepcopy(feature_df_color)\n",
    "    target = copy.deepcopy(target_df)\n",
    "    # text features \n",
    "    feature_text.rename({'qid': 'QID'}, axis=1, inplace=True)\n",
    "    feature_text.set_index('QID', inplace=True)\n",
    "    # color features\n",
    "    feature_color.rename({'qid': 'QID'}, axis=1, inplace=True)\n",
    "    feature_color.set_index('QID', inplace=True)\n",
    "    # join features\n",
    "    features = feature_text.join(feature_color[['descriptions','tags','dominant_colors']],\n",
    "                                 on='QID',\n",
    "                                 how='outer')\n",
    "    # join features with target\n",
    "    target = target[['QID', 'IMG', 'QSN', 'TXT', 'OBJ', 'COL', 'CNT', 'OTH']]\n",
    "    target.set_index('QID', inplace=True)\n",
    "    target = target.astype(dtype=str)\n",
    "    df = target.join(features, on='QID', how='inner')\n",
    "    df['descriptions'].astype(list)\n",
    "    return df\n",
    "\n",
    "def lem(s):\n",
    "    arr = s.split(\" \")\n",
    "    lem = WordNetLemmatizer()\n",
    "    op = \"\"\n",
    "    for w in arr:\n",
    "        word = lem.lemmatize(w) + ' '\n",
    "        op += word\n",
    "    return op\n",
    "\n",
    "def preprocess_text(feature_columns):\n",
    "    \"\"\" output an nparray with single document per data point \"\"\"\n",
    "    ip = copy.deepcopy(feature_columns).values\n",
    "    op = []\n",
    "    for i in range(ip.shape[0]):\n",
    "        doc      =  \"\"\n",
    "        for j in range(ip.shape[1]):\n",
    "            # clean up chars\n",
    "            s    =  str(ip[i][j])\n",
    "            s    =  s.translate({ord(c): \"\" for c in \"!@#$%^&*()[]{};:,./<>?\\|`~-=_+'\"}).lower() + \" \"\n",
    "            if j == 1:             # clean descriptions\n",
    "                s = re.sub(r'confidence\\s+\\d+', '', s)\n",
    "                s = re.sub(r'text', '', s)\n",
    "            # lexicon normalize\n",
    "            s    = lem(s)\n",
    "            doc  += (s.strip())\n",
    "        op.append(doc)\n",
    "    op = np.asarray(op)\n",
    "    return op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14257, 13) (3230, 13) 17487\n",
      "(2247, 13) (513, 13) 2760\n",
      "Training: (17487, 13)\n",
      "Validation: (2760, 13)\n"
     ]
    }
   ],
   "source": [
    "vizwiz_train   = join_feature_target(vizwiz_features_train_text, \n",
    "                                   vizwiz_features_train_color, \n",
    "                                   vizwiz_targets_train)\n",
    "vizwiz_val     = join_feature_target(vizwiz_features_val_text, \n",
    "                                 vizwiz_features_val_color, \n",
    "                                 vizwiz_targets_val)\n",
    "vqa_train      = join_feature_target(vqa_features_train_text, \n",
    "                                   vqa_features_train_color, \n",
    "                                   vqa_targets_train)\n",
    "vqa_val        = join_feature_target(vizwiz_features_val_text, \n",
    "                                 vqa_features_val_color, \n",
    "                                 vqa_targets_val)\n",
    "print(vizwiz_train.shape, vqa_train.shape, vizwiz_train.shape[0] + vqa_train.shape[0])\n",
    "print(vizwiz_val.shape, vqa_val.shape, vizwiz_val.shape[0] + vqa_val.shape[0])\n",
    "\n",
    "# create X and Y\n",
    "\n",
    "train = pd.concat([vizwiz_train, vqa_train], axis=0)\n",
    "val   = pd.concat([vizwiz_val, vqa_val], axis=0)\n",
    "print(\"Training: {}\\nValidation: {}\".format(train.shape, val.shape))\n",
    "\n",
    "features_train = preprocess_text(train[['QSN','descriptions', 'tags', 'dominant_colors', \n",
    "                                        'handwritten_text', 'ocr_text']])\n",
    "txt_train      = train['TXT'].values\n",
    "col_train      = train['COL'].values\n",
    "cnt_train      = train['CNT'].values\n",
    "\n",
    "features_val   = preprocess_text(val[['QSN', 'descriptions', 'tags', 'dominant_colors',\n",
    "                                      'handwritten_text', 'ocr_text']])\n",
    "txt_val        = val['TXT'].values.astype('float32')\n",
    "col_val        = val['COL'].values.astype('float32')\n",
    "cnt_val        = val['CNT'].values.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples each class - Vizwiz - train\n",
      "Text recognition [9119. 8368.]\n",
      "Color recognition [10926.  6561.]\n"
     ]
    }
   ],
   "source": [
    "# tokenize\n",
    "tok        = Tokenizer(num_words=VOCAB_SIZE, \n",
    "                       filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "                       lower=True,\n",
    "                       split=\" \")\n",
    "tok.fit_on_texts(features_train)\n",
    "\n",
    "# create sequences & pad\n",
    "train_seq  = tok.texts_to_sequences(features_train)\n",
    "train_seq  = sequence.pad_sequences(train_seq, maxlen=MAX_DOC_LEN)\n",
    "val_seq    = tok.texts_to_sequences(features_val)\n",
    "val_seq    = sequence.pad_sequences(val_seq, maxlen=MAX_DOC_LEN)\n",
    "\n",
    "# check class distribution\n",
    "text_recognition_labels = to_categorical(np.asarray(txt_train)).astype('float32')\n",
    "color_recognition_labels = to_categorical(np.asarray(col_train)).astype('float32')\n",
    "print('Number of samples each class - Vizwiz - train')\n",
    "print('Text recognition', text_recognition_labels.sum(axis=0))\n",
    "print('Color recognition', color_recognition_labels.sum(axis=0))\n",
    "n_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-02-28 03:29:27,308 : INFO : collecting all words and their counts\n",
      "2019-02-28 03:29:27,309 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-02-28 03:29:27,371 : INFO : PROGRESS: at sentence #10000, processed 347442 words, keeping 31941 word types\n",
      "2019-02-28 03:29:27,420 : INFO : collected 44967 word types from a corpus of 609446 raw words and 17487 sentences\n",
      "2019-02-28 03:29:27,425 : INFO : Loading a fresh vocabulary\n",
      "2019-02-28 03:29:27,460 : INFO : effective_min_count=6 retains 4665 unique words (10% of original 44967, drops 40302)\n",
      "2019-02-28 03:29:27,461 : INFO : effective_min_count=6 leaves 553935 word corpus (90% of original 609446, drops 55511)\n",
      "2019-02-28 03:29:27,473 : INFO : deleting the raw counts dictionary of 44967 items\n",
      "2019-02-28 03:29:27,476 : INFO : sample=0.001 downsamples 65 most-common words\n",
      "2019-02-28 03:29:27,477 : INFO : downsampling leaves estimated 389589 word corpus (70.3% of prior 553935)\n",
      "2019-02-28 03:29:27,488 : INFO : estimated required memory for 4665 words and 100 dimensions: 6064500 bytes\n",
      "2019-02-28 03:29:27,488 : INFO : resetting layer weights\n",
      "2019-02-28 03:29:27,551 : INFO : training model with 6 workers on 4665 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-02-28 03:29:28,284 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-02-28 03:29:28,287 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-02-28 03:29:28,296 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-02-28 03:29:28,312 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-02-28 03:29:28,313 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-02-28 03:29:28,319 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-02-28 03:29:28,320 : INFO : EPOCH - 1 : training on 609446 raw words (389469 effective words) took 0.8s, 511275 effective words/s\n",
      "2019-02-28 03:29:29,072 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-02-28 03:29:29,081 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-02-28 03:29:29,082 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-02-28 03:29:29,085 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-02-28 03:29:29,099 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-02-28 03:29:29,117 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-02-28 03:29:29,117 : INFO : EPOCH - 2 : training on 609446 raw words (389439 effective words) took 0.8s, 507067 effective words/s\n",
      "2019-02-28 03:29:29,696 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-02-28 03:29:29,701 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-02-28 03:29:29,717 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-02-28 03:29:29,722 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-02-28 03:29:29,729 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-02-28 03:29:29,732 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-02-28 03:29:29,733 : INFO : EPOCH - 3 : training on 609446 raw words (389712 effective words) took 0.6s, 647529 effective words/s\n",
      "2019-02-28 03:29:30,262 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-02-28 03:29:30,265 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-02-28 03:29:30,266 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-02-28 03:29:30,278 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-02-28 03:29:30,293 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-02-28 03:29:30,294 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-02-28 03:29:30,294 : INFO : EPOCH - 4 : training on 609446 raw words (389210 effective words) took 0.6s, 701461 effective words/s\n",
      "2019-02-28 03:29:30,845 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-02-28 03:29:30,849 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-02-28 03:29:30,853 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-02-28 03:29:30,866 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-02-28 03:29:30,870 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-02-28 03:29:30,883 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-02-28 03:29:30,883 : INFO : EPOCH - 5 : training on 609446 raw words (389944 effective words) took 0.6s, 669595 effective words/s\n",
      "2019-02-28 03:29:30,884 : INFO : training on a 3047230 raw words (1947774 effective words) took 3.3s, 584539 effective words/s\n"
     ]
    }
   ],
   "source": [
    "# punkt sentence level tokenizer\n",
    "sent_lst = []\n",
    "for doc in features_train:\n",
    "    sentences = nltk.tokenize.sent_tokenize(doc)\n",
    "    for sent in sentences:\n",
    "        word_lst = [w for w in nltk.tokenize.word_tokenize(sent) if w.isalnum()]\n",
    "        sent_lst.append(word_lst)\n",
    "        \n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "word2vec_model = gensim.models.Word2Vec(sentences=sent_lst,\n",
    "                                        min_count=6,\n",
    "                                        size=EMBEDDING_DIM,\n",
    "                                        sg=1,\n",
    "                                        workers=os.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 4665 word vectors\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "\n",
    "for word in word2vec_model.wv.vocab:\n",
    "    coefs = np.asarray(word2vec_model.wv[word], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "\n",
    "print('Total %s word vectors' % len(embeddings_index))\n",
    "\n",
    "# Initial word embedding\n",
    "embedding_matrix = np.zeros((VOCAB_SIZE, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in tok.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None and i < VOCAB_SIZE:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 4665 word vectors\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "\n",
    "for word in word2vec_model.wv.vocab:\n",
    "    coefs = np.asarray(word2vec_model.wv[word], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "\n",
    "print('Total %s word vectors' % len(embeddings_index))\n",
    "\n",
    "# Initial word embedding\n",
    "embedding_matrix = np.zeros((VOCAB_SIZE, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in tok.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None and i < VOCAB_SIZE:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_create_train(labels, learning_rate, lstm_dim, batch_size, num_epochs, optimizer_param, regularization=1e-7):\n",
    "    \n",
    "    l2_reg = regularizers.l2(regularization)\n",
    "    \n",
    "    # init model\n",
    "    embedding_layer = Embedding(VOCAB_SIZE,\n",
    "                                EMBEDDING_DIM,\n",
    "                                input_length=MAX_DOC_LEN,\n",
    "                                trainable=True,\n",
    "                                mask_zero=False,\n",
    "                                embeddings_regularizer=l2_reg,\n",
    "                                weights=[embedding_matrix])\n",
    "    lstm_layer = LSTM(units=lstm_dim, kernel_regularizer=l2_reg)\n",
    "    dense_layer = Dense(n_classes,\n",
    "                        activation='softmax', \n",
    "                        kernel_regularizer=l2_reg)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(embedding_layer)\n",
    "    model.add(Bidirectional(lstm_layer))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(dense_layer)\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizer_param,\n",
    "                  metrics=['acc'])\n",
    "    history = History()\n",
    "    csv_logger = CSVLogger('./LSTM/text/{}_{}_{}_{}.log'.format(learning_rate, regularization, batch_size, num_epochs),\n",
    "                           separator=',',\n",
    "                           append=True)\n",
    "    t1 = time.time()\n",
    "    # model fit\n",
    "    model.fit(train_seq,\n",
    "              labels.astype('float32'),\n",
    "              batch_size=batch_size,\n",
    "              epochs=num_epochs,\n",
    "              callbacks=[history, csv_logger],\n",
    "              verbose=2)\n",
    "    t2 = time.time()\n",
    "    # save hdf5\n",
    "    model.save('./LSTM/text/{}_{}_{}_{}_model.h5'.format(learning_rate, regularization, batch_size, num_epochs))\n",
    "    np.savetxt('./LSTM/text/{}_{}_{}_{}_time.txt'.format(learning_rate, regularization, batch_size, num_epochs), \n",
    "               [regularization, (t2-t1) / 3600])\n",
    "    with open('./LSTM/text/{}_{}_{}_{}_history.txt'.format(learning_rate, regularization, batch_size, num_epochs), \"w\") as res_file:\n",
    "        res_file.write(str(history.history))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      " - 154s - loss: 0.4697 - acc: 0.7841\n",
      "Epoch 2/30\n",
      " - 109s - loss: 0.4222 - acc: 0.8171\n",
      "Epoch 3/30\n",
      " - 92s - loss: 0.4024 - acc: 0.8321\n",
      "Epoch 4/30\n",
      " - 152s - loss: 0.3880 - acc: 0.8359\n",
      "Epoch 5/30\n",
      " - 153s - loss: 0.3772 - acc: 0.8414\n",
      "Epoch 6/30\n",
      " - 156s - loss: 0.3692 - acc: 0.8456\n",
      "Epoch 7/30\n",
      " - 156s - loss: 0.3638 - acc: 0.8495\n",
      "Epoch 8/30\n"
     ]
    },
    {
     "ename": "SystemError",
     "evalue": "<built-in function Trainer_train_minibatch> returned a result with an error set",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/anaconda/envs/py35/lib/python3.5/site-packages/cntk/cntk_py.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m   1760\u001b[0m     \u001b[0m__swig_setmethods__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1761\u001b[0;31m     \u001b[0m__setattr__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_swig_setattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1762\u001b[0m     \u001b[0m__swig_getmethods__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mSystemError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/anaconda/envs/py35/lib/python3.5/site-packages/cntk/cntk_py.py\u001b[0m in \u001b[0;36m__hash__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1579\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__hash__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1580\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_cntk_py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable___hash__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1581\u001b[0m     \u001b[0m__swig_destroy__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_cntk_py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete_Variable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mSystemError\u001b[0m: <built-in function Variable___hash__> returned a result with an error set",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mSystemError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/anaconda/envs/py35/lib/python3.5/site-packages/cntk/cntk_py.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m   1760\u001b[0m     \u001b[0m__swig_setmethods__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1761\u001b[0;31m     \u001b[0m__setattr__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_swig_setattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1762\u001b[0m     \u001b[0m__swig_getmethods__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py35/lib/python3.5/site-packages/cntk/cntk_py.py\u001b[0m in \u001b[0;36m_swig_setattr\u001b[0;34m(self, class_type, name, value)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_swig_setattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_swig_setattr_nondynamic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py35/lib/python3.5/site-packages/cntk/cntk_py.py\u001b[0m in \u001b[0;36m_swig_setattr_nondynamic\u001b[0;34m(self, class_type, name, value, static)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"this\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'SwigPyObject'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mSystemError\u001b[0m: <class 'type'> returned a result with an error set",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mSystemError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/anaconda/envs/py35/lib/python3.5/site-packages/cntk/cntk_py.py\u001b[0m in \u001b[0;36m__hash__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1579\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__hash__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1580\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_cntk_py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable___hash__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1581\u001b[0m     \u001b[0m__swig_destroy__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_cntk_py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete_Variable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mSystemError\u001b[0m: <built-in function Variable___hash__> returned a result with an error set",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mSystemError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-2b2bfb03a6b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m                                   \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                                   \u001b[0moptimizer_param\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnesterov\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m                                   regularization=R)\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./LSTM/text/{}_{}_{}_{}_model.h5'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mR\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-21c45d85873c>\u001b[0m in \u001b[0;36mlstm_create_train\u001b[0;34m(labels, learning_rate, lstm_dim, batch_size, num_epochs, optimizer_param, regularization)\u001b[0m\n\u001b[1;32m     36\u001b[0m               \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcsv_logger\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m               verbose=2)\n\u001b[0m\u001b[1;32m     39\u001b[0m     \u001b[0mt2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;31m# save hdf5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py35/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1035\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1037\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/anaconda/envs/py35/lib/python3.5/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py35/lib/python3.5/site-packages/keras/backend/cntk_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1967\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1968\u001b[0m             result = self.trainer.train_minibatch(\n\u001b[0;32m-> 1969\u001b[0;31m                 input_dict, self.trainer_output)\n\u001b[0m\u001b[1;32m   1970\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1971\u001b[0m             \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py35/lib/python3.5/site-packages/cntk/train/trainer.py\u001b[0m in \u001b[0;36mtrain_minibatch\u001b[0;34m(self, arguments, outputs, device, is_sweep_end)\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m                 updated = super(Trainer, self).train_minibatch(arguments, is_sweep_end,\n\u001b[0;32m--> 171\u001b[0;31m                     output_map, device)\n\u001b[0m\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutput_map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py35/lib/python3.5/site-packages/cntk/cntk_py.py\u001b[0m in \u001b[0;36mtrain_minibatch\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   3025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3026\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain_minibatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3027\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_cntk_py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrainer_train_minibatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3029\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mSystemError\u001b[0m: <built-in function Trainer_train_minibatch> returned a result with an error set"
     ]
    }
   ],
   "source": [
    "L = 1e-1\n",
    "R = 0\n",
    "B = 32\n",
    "E = 30\n",
    "lstm_create_train(labels=text_recognition_labels,\n",
    "                                  learning_rate=L,\n",
    "                                  lstm_dim=100, \n",
    "                                  batch_size=B,\n",
    "                                  num_epochs=E, \n",
    "                                  optimizer_param=SGD(lr=L, nesterov=True),\n",
    "                                  regularization=R)\n",
    "model = load_model('./LSTM/text/{}_{}_{}_{}_model.h5'.format(L,R,B,E))\n",
    "preds = model.predict(val_seq, verbose=0)\n",
    "print(\"Learning rate: {} Regularization: {} Batch size: {} Epoch: {}\".format(L,R,B,E))\n",
    "print((\"Accuracy = {0} \\t AUC = {1}\".format(accuracy_score(txt_val, preds.argmax(axis=1)), \n",
    "                                                            roc_auc_score(txt_val, preds[:,1]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
