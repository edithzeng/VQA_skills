# VQA skills

This repository holds the codes for my master's report to be completed in May 2019.

This research contributes to the understanding of the unique information needs and challenges faced by blind users with the goal to improve the status quo of visual assistive technologies.

Image-question pairings from the VizWiz datasets pose significant challenges to existing machine learning algorithms with inconsistent image quality and colloquialism. Visual question answering (VQA) tasks that involves crowdsourcing and community answering can also be better divided and assigned to the appropriate crowd workers based on their experiences, preferences and skills. An algorithm to identify the skills involved can be potentially transferred to another task utilizing machine and human computations to better assist visually impaired users. 
