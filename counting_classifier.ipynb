{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py35/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/edithzeng/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/edithzeng/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/edithzeng/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 2835451863868589003\n",
      ", name: \"/device:XLA_GPU:0\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 7219748232832260075\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 5173285521779125190\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 11281927373\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 879016515108569033\n",
      "physical_device_desc: \"device: 0, name: Tesla K80, pci bus id: b00c:00:00.0, compute capability: 3.7\"\n",
      "]\n",
      "['/job:localhost/replica:0/task:0/device:GPU:0']\n",
      "Joined features with skill labels.\n",
      "Joined features with skill labels.\n",
      "Joined features with skill labels.\n",
      "Joined features with skill labels.\n",
      "VizWiz training shape: (14257, 13)\n",
      "VQA training shape: (3230, 13)\n",
      "Total training rows: 17487\n",
      "VizWiz validation shape:(2247, 13)\n",
      "VQA validation shape:(513, 13)\n",
      "Total validation rows: 2760\n",
      "Training: (3230, 13)\n",
      "Validation: (513, 13)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "import tensorflow as tf\n",
    "from skill_label_classifier import *\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from gensim.test.utils import datapath\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "# helper function to find optimal number of PC (elbow method)\n",
    "def plot_explained_variance(X_train):\n",
    "    pca = PCA()\n",
    "    pca_full = pca.fit(X_train)\n",
    "    plt.plot(np.cumsum(pca_full.explained_variance_ratio_))\n",
    "    plt.xlabel(\"number of principal components\")\n",
    "    plt.ylabel(\"cumulative explained variance\")\n",
    "    plt.grid(color='grey',linestyle='-',alpha=0.2)\n",
    "    plt.show()\n",
    "    \n",
    "def preprocess_pca(X_train, X_test, dim, r=None):\n",
    "    pca = PCA(n_components=dim, random_state=r)\n",
    "    X_train_pca = pca.fit_transform(X_train)\n",
    "    X_test_pca = pca.transform(X_test)\n",
    "    return X_train_pca, X_test_pca\n",
    "\n",
    "\n",
    "experiment = SkillClassifier()\n",
    "experiment.import_data()\n",
    "experiment.create_df()\n",
    "\n",
    "experiment.choose_dataset('vqa')\n",
    "experiment.set_features(['QSN', 'descriptions', 'tags', 'dominant_colors','handwritten_text', 'ocr_text'])\n",
    "experiment.set_targets()\n",
    "\n",
    "features_train = experiment.features_train\n",
    "features_val   = experiment.features_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples each class: \n",
      "Text recognition - 1: 358 0: 2872\n",
      "Color recognition - 1:1148 0: 2082\n",
      "Counting - 1: 661 0: 2569\n",
      "Number of validation samples each class: \n",
      "Text recognition - 1: 63 0: 450\n",
      "Color recognition - 1:179 0: 334\n",
      "Counting - 1:109 0: 404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py35/lib/python3.5/site-packages/sklearn/utils/validation.py:590: DataConversionWarning: Data with input dtype int32 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/anaconda/envs/py35/lib/python3.5/site-packages/sklearn/utils/validation.py:590: DataConversionWarning: Data with input dtype int32 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/anaconda/envs/py35/lib/python3.5/site-packages/sklearn/utils/validation.py:590: DataConversionWarning: Data with input dtype int32 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "2019-03-30 22:43:57,424 : INFO : loading projection weights from /anaconda/envs/py35/lib/python3.5/site-packages/gensim/test/test_data/GoogleNews-vectors-negative300.bin\n"
     ]
    }
   ],
   "source": [
    "# check training class distribution\n",
    "text_recognition_y_train = np.asarray(experiment.txt_train).astype('float32')\n",
    "color_recognition_y_train = np.asarray(experiment.col_train).astype('float32')\n",
    "counting_y_train = np.asarray(experiment.cnt_train).astype('float32')\n",
    "\n",
    "print('Number of training samples each class: ')\n",
    "print('Text recognition - 1: {} 0: {}'.format(np.count_nonzero(text_recognition_y_train), \n",
    "      len(text_recognition_y_train)-np.count_nonzero(text_recognition_y_train)))\n",
    "print('Color recognition - 1:{} 0: {}'.format(np.count_nonzero(color_recognition_y_train),\n",
    "      len(color_recognition_y_train)-np.count_nonzero(color_recognition_y_train)))\n",
    "print('Counting - 1: {} 0: {}'.format(np.count_nonzero(counting_y_train), \n",
    "      len(counting_y_train)-np.count_nonzero(counting_y_train)))\n",
    "\n",
    "n_classes = 3\n",
    "\n",
    "y_train = np.column_stack((text_recognition_y_train, color_recognition_y_train, counting_y_train))\n",
    "\n",
    "# check validation class distribution\n",
    "text_recognition_y_val = np.asarray(experiment.txt_val).astype('float32')\n",
    "color_recognition_y_val = np.asarray(experiment.col_val).astype('float32')\n",
    "counting_y_val = np.asarray(experiment.cnt_val).astype('float32')\n",
    "print('Number of validation samples each class: ')\n",
    "print('Text recognition - 1: {} 0: {}'.format(np.count_nonzero(text_recognition_y_val), \n",
    "      len(text_recognition_y_val)-np.count_nonzero(text_recognition_y_val)))\n",
    "print('Color recognition - 1:{} 0: {}'.format(np.count_nonzero(color_recognition_y_val),\n",
    "     len(color_recognition_y_val)-np.count_nonzero(color_recognition_y_val)))\n",
    "print('Counting - 1:{} 0: {}'.format(np.count_nonzero(counting_y_val),\n",
    "     len(counting_y_val)-np.count_nonzero(counting_y_val)))\n",
    "\n",
    "y_val = np.column_stack((text_recognition_y_val, color_recognition_y_val, counting_y_val))\n",
    "\n",
    "# tokenize\n",
    "tok        = Tokenizer(num_words=VOCAB_SIZE, \n",
    "                       filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "                       lower=True,\n",
    "                       split=\" \")\n",
    "tok.fit_on_texts(features_train)\n",
    "\n",
    "# create sequences & pad\n",
    "train_seq  = tok.texts_to_sequences(features_train)\n",
    "train_seq  = sequence.pad_sequences(train_seq, maxlen=MAX_DOC_LEN)\n",
    "val_seq    = tok.texts_to_sequences(features_val)\n",
    "val_seq    = sequence.pad_sequences(val_seq, maxlen=MAX_DOC_LEN)\n",
    "\n",
    "# standardize training and testing features\n",
    "sc = StandardScaler()\n",
    "train_seq = sc.fit_transform(train_seq)\n",
    "val_seq = sc.transform(val_seq)\n",
    "\n",
    "# Set validation data tuple\n",
    "val_data = (val_seq, y_val)\n",
    "\n",
    "# punkt sentence level tokenizer\n",
    "sent_lst = [] \n",
    "for doc in features_train:\n",
    "    sentences = nltk.tokenize.sent_tokenize(doc)\n",
    "    for sent in sentences:\n",
    "        word_lst = [w for w in nltk.tokenize.word_tokenize(sent) if w.isalnum()]\n",
    "        sent_lst.append(word_lst)\n",
    "\n",
    "EMBEDDING_DIM = 300\n",
    "googlenews_corpus = '/anaconda/envs/py35/lib/python3.5/site-packages/gensim/test/test_data/GoogleNews-vectors-negative300.bin'\n",
    "        \n",
    "# load pre-trained word2vec on GoogleNews (https://code.google.com/archive/p/word2vec/)\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "word2vec_model = gensim.models.KeyedVectors.load_word2vec_format(datapath(googlenews_corpus),\n",
    "                                                                 binary=True)\n",
    "embeddings_index = {}\n",
    "for word in word2vec_model.wv.vocab:\n",
    "    coefs = np.asarray(word2vec_model.wv[word], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "print('Total %s word vectors' % len(embeddings_index))\n",
    "\n",
    "# Initial word embedding\n",
    "embedding_matrix = np.zeros((VOCAB_SIZE, EMBEDDING_DIM))\n",
    "for word, i in tok.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None and i < VOCAB_SIZE:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# PCA for dimensionality reduction\n",
    "N_COMPONENTS = 40\n",
    "train_seq, val_seq = preprocess_pca(train_seq, val_seq, dim=N_COMPONENTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 1e-1\n",
    "R = 0\n",
    "B = 64\n",
    "E = 1000\n",
    "model, history = lstm_create_train(train_seq, embedding_matrix,\n",
    "                 train_labels=y_train,\n",
    "                 val_data=val_data,\n",
    "                 learning_rate=L,\n",
    "                 lstm_dim=100,\n",
    "                 batch_size=B,\n",
    "                 num_epochs=E,\n",
    "                 optimizer_param=SGD(lr=L, nesterov=True),\n",
    "                 regularization=R, n_classes=3)\n",
    "preds = model.predict(val_seq, verbose=0)\n",
    "print(\"accuracy:\", accuracy_score(y_val, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
